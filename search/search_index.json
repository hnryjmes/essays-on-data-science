{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Essays on Data Science In which I put together my thoughts on the practice of data science. This is a curated and edited collection of my blog posts, as well as essays specially written for the broader Python community. Support this project If you find this collection of essays useful, please star the repository on GitHub ! If you enjoyed this essay collection and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Home"},{"location":"#essays-on-data-science","text":"In which I put together my thoughts on the practice of data science. This is a curated and edited collection of my blog posts, as well as essays specially written for the broader Python community.","title":"Essays on Data Science"},{"location":"#support-this-project","text":"If you find this collection of essays useful, please star the repository on GitHub ! If you enjoyed this essay collection and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Support this project"},{"location":"supporters/","text":"A big thank you... ...to my Patreon supporters! Eddie Janowicz Carol Willing Hector Munoz Mridul Seth Kapil Jain Brian Gue Brice Paris Your support keeps me caffeinated, so I can continue to make educational material for the data science and Python communities!","title":"Supporters"},{"location":"supporters/#a-big-thank-you","text":"...to my Patreon supporters! Eddie Janowicz Carol Willing Hector Munoz Mridul Seth Kapil Jain Brian Gue Brice Paris Your support keeps me caffeinated, so I can continue to make educational material for the data science and Python communities!","title":"A big thank you..."},{"location":"computing/recursion/","text":"Recursion Recursion is an incredibly useful concept to know. To be clear, it is distinct from looping, but is related. I think it's helpful for data scientists to have recursion as a programming trick in their back pocket. In this essay, let's take an introductory look at recursion, and where it can come in handy. What recursion looks like Recursion happens when we have a function that calls itself by default or else returns a result when some stopping criteria is reached. A classic example of recursion is in finding the root of a tree from a given node. Here, we essentially want to follow every node's predecessor until we reach a node that has no predecessor. In code form, this looks something like this: 1 2 3 4 5 6 def find_root ( G , n ): predecessor = G . predecessor ( n ) if G . predecessor ( n ): return find_root ( G , predecessor ) else : return n Generally, we first compute something on the basis of the inputs (line 2). This is usually some form of finding a new substitute input on which we can check a condition (lines 4 and 6). Under one condition, we return the function call with a new input, and under another condition, we return the desired output. Why you would use recursion Recursion is essentially a neat way to write a loop concisely, and can be useful, say, under circumstances where we do not know the exact number of loop iterations needed before we encounter the stopping condition. While I do find recursion useful in certain applied settings, I will also clarify that I don't use recursion on a daily basis. As such, I recommend this as a back-pocket trick that one should have, but won't necessarily use all the time. Where recursion shows up in a real-life situation I can speak to one situation at work where I was benchmarking some deep neural network models, and also testing hyperparameters on a grid. There, I used YAML files to keep track of parameters and experiments, and in order to keep things concise, I implemented a very lightweight YAML inheritance scheme, where I would have a master \"template\" experiment, but use child YAML files that inherited from the \"master\" template in which certain parts of the experiment parameters were changed. (An example might be one where the master template specified the use of the Adam optimizer with a particular learning rate, while the child templates simply modified the learning rate.) As the experiments got deeper and varied more parameters, things became more tree-like, and so I had to navigate the parameter tree from the child templates up till the root template, which by definition had no parents. After finding the root template, I could then travel back down from the root template, iteratively updating the parameters until I reached the child template of interest. The more general scenario to look out for is in graph traversal problems. If your problem can be cast in terms of a graph data structure that you need to program your computer to take a walk over, then that is a prime candidate for trying your hand at recursion. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Recursion"},{"location":"computing/recursion/#recursion","text":"Recursion is an incredibly useful concept to know. To be clear, it is distinct from looping, but is related. I think it's helpful for data scientists to have recursion as a programming trick in their back pocket. In this essay, let's take an introductory look at recursion, and where it can come in handy.","title":"Recursion"},{"location":"computing/recursion/#what-recursion-looks-like","text":"Recursion happens when we have a function that calls itself by default or else returns a result when some stopping criteria is reached. A classic example of recursion is in finding the root of a tree from a given node. Here, we essentially want to follow every node's predecessor until we reach a node that has no predecessor. In code form, this looks something like this: 1 2 3 4 5 6 def find_root ( G , n ): predecessor = G . predecessor ( n ) if G . predecessor ( n ): return find_root ( G , predecessor ) else : return n Generally, we first compute something on the basis of the inputs (line 2). This is usually some form of finding a new substitute input on which we can check a condition (lines 4 and 6). Under one condition, we return the function call with a new input, and under another condition, we return the desired output.","title":"What recursion looks like"},{"location":"computing/recursion/#why-you-would-use-recursion","text":"Recursion is essentially a neat way to write a loop concisely, and can be useful, say, under circumstances where we do not know the exact number of loop iterations needed before we encounter the stopping condition. While I do find recursion useful in certain applied settings, I will also clarify that I don't use recursion on a daily basis. As such, I recommend this as a back-pocket trick that one should have, but won't necessarily use all the time.","title":"Why you would use recursion"},{"location":"computing/recursion/#where-recursion-shows-up-in-a-real-life-situation","text":"I can speak to one situation at work where I was benchmarking some deep neural network models, and also testing hyperparameters on a grid. There, I used YAML files to keep track of parameters and experiments, and in order to keep things concise, I implemented a very lightweight YAML inheritance scheme, where I would have a master \"template\" experiment, but use child YAML files that inherited from the \"master\" template in which certain parts of the experiment parameters were changed. (An example might be one where the master template specified the use of the Adam optimizer with a particular learning rate, while the child templates simply modified the learning rate.) As the experiments got deeper and varied more parameters, things became more tree-like, and so I had to navigate the parameter tree from the child templates up till the root template, which by definition had no parents. After finding the root template, I could then travel back down from the root template, iteratively updating the parameters until I reached the child template of interest. The more general scenario to look out for is in graph traversal problems. If your problem can be cast in terms of a graph data structure that you need to program your computer to take a walk over, then that is a prime candidate for trying your hand at recursion.","title":"Where recursion shows up in a real-life situation"},{"location":"computing/recursion/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"machine-learning/computational-bayesian-stats/","text":"An Introduction to Probability and Computational Bayesian Statistics In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here? Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference. Probability Distributions We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\". Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution. Base Object Implementation Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as: class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma Probability Density Function Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability density function\", or the \"PDF\". Using this, we may then extend extend this object with a method called .pdf(x) , that returns a number giving the number of credibility points assigned to the value of x passed in. import numpy as np class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma def pdf ( self , x ): return ( 1 / np . sqrt ( 2 * self . sigma ** 2 * np . pi ) * np . exp ( - ( x - self . mu ) ** 2 / 2 * self . sigma ** 2 )) If we pass in a number x from the number line, we will get back another number that tells us the number of credibility points given to that value x , under the state of the normal distribution instantiated. We'll call this P(x) P(x) . To simplify the implementation used here, we are going to borrow some machinery already available to us in the Python scientific computing ecosystem, particularly from the SciPy stats module, which gives us reference implementations of probability distributions. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) Log Probability A common task in Bayesian inference is computing the likelihood of data. Let's assume that the data {X_1, X_2, ... X_i} {X_1, X_2, ... X_i} generated are independent and identically distributed, (the famous i.i.d. term comes from this). This means, then, that the joint probability of the data that was generated is equivalent to the product of the individual probabilities of each datum: P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) (We have to know the rules of probability to know this result; it is a topic for a different essay.) If you remember the notation above, each P(X_i) P(X_i) is an evaluation of X_i X_i on the distribution's probability density function. It being a probability value means it is bound between 0 and 1. However, multiplying many probabilities together usually will result in issues with underflow computationally, so in evaluating likelihoods, we usually stick with log-likelihoods instead. By the usual rules of math, then: \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) To our normal distribution class, we can now add in another class method that computes the sum of log likelihoods evaluated at a bunch of i.i.d. data points. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) def logpdf ( self , x ): return self . dist . logpdf ( x ) Random Variables Definition Informally, a \"random variable\" is nothing more than a variable whose quantity is non-deterministic (hence random) but whose probability of taking on a certain value can be described by a probability distribution. According to the Wikipedia definition of a random variable : A random variable has a probability distribution, which specifies the probability of its values. As such, it may be tempting to conceive of a random variable as an object that has a probability distribution attribute attached to it. Realizations of a Random Variable On the other hand, it can also be convenient to invert that relationship, and claim that a probability distribution can generate realizations of a random variable. The latter is exactly how SciPy distributions are implemented: from scipy.stats import norm # Normal distribution can generate realizations of an RV # The following returns a NumPy array of 10 draws # from a standard normal distribution. norm ( loc = 0 , scale = 1 ) . rvs ( 10 ) Realizations of a Random Variable A \"realization\" of a random variable is nothing more than generating a random number whose probability of being generated is defined by the random variable's probability density function. Because the generation of realizations of a random variable is equivalent to sampling from a probability distribution, we can extend our probability distribution definition to include a .sample(n) method: from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) # ... def sample ( self , n ): return self . dist . rvs ( n ) Now, if we draw 10 realizations of a normally distributed random variable, and the drawing of each realization has no dependence of any kind on the previous draw, then we can claim that each draw is independent and identically distributed . This is where the fabled \" iid \" term in undergraduate statistics classes comes from. Data Generating Process Now that we have covered what probability distributions are, we can now move on to other concepts that are important in Bayesian statistical modelling. Realizations of a random variable, or draws from its probability distribution, are how a Bayesian assumes data are generated. Describing how data are generated using probability distributions, or in other words, writing down the \"data generating process\", is a core activity in Bayesian statistical modelling. Viewed this way, data values generated by a random process depend on the underlying random variable's probability distribution. In other words, the random variable realizations are known, given the probability distribution used to model it. Keep this idea in mind: it is going to be important shortly. Bayes' Rule Now that we've covered probability distributions, we can move on to Bayes' rule. You probably have seen the following equation: P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} Bayes' rule states nothing more than the fact that the conditional probability of B given A is equal to the conditional probability of A given B times the probability of B divided by the probability of A. When doing Bayesian statistical inference, we commonly take a related but distinct interpretation: P(H|D) = \\frac{P(D|H)P(H)}{P(D)} P(H|D) = \\frac{P(D|H)P(H)}{P(D)} It may look weird, but didn't we say before that data are realizations from a random variable? Why are we now treating data as a random variable? Here, we are doing not-so-intuitive but technically correct step of treating the data D D as being part of this probabilistic model (hence it \"looks\" like a random variable), alongside our model parameters H H . There's a lot of measure theory that goes into this interpretation, which at this point I have not yet mastered, and so will wave my hands in great arcs and propose that this interpretation be accepted for now and move on. Data are random variables? Notes from a chat with Colin gave me a lot to chew on, as usual: The answer is in how you define \"event\" as \"an element of a sigma algebra\". intuitively, an \"event\" is just an abstraction, so one event might be \"the coin is heads\", or in another context the event might be \"the parameters are [0.2, 0.1, 0.2]\". And so analogously, \"the data were configured as [0, 5, 2, 3]\". Notice also that the events are different if the data being ordered vs unordered are different! This was a logical leap that I had been asked about before, but did not previously have the knowledge to respond to. Thanks to Colin, I now do. With the data + hypothesis interpretation of Bayes' rule in hand, the next question arises: What math happens when we calculate posterior densities? Translating Bayes' Math to Python Defining Posterior Log-Likelihood To understand this, let's look at the simplest complex example that I could think of: Estimating the \\mu \\mu and \\sigma \\sigma parameters of a normal distribution conditioned on observing data points y y . If we assume a data generating process that looks like the following (with no probability distributions specified yet): graph TD; \u03bc((\u03bc)) --> y(y); \u03c3((\u03c3)) --> y(y); We can write out the following probabilistic model (now explicitly specifying probability distributions): \\mu \\sim Normal(0, 10) \\mu \\sim Normal(0, 10) \\sigma \\sim Exponential(1) \\sigma \\sim Exponential(1) y \\sim Normal(\\mu, \\sigma) y \\sim Normal(\\mu, \\sigma) Let's now map the symbols onto Bayes' rule. H H are the parameters, which are \\mu \\mu and \\sigma \\sigma here. D D is the data that I will observe P(H|D) P(H|D) is the posterior, which we would like to compute. P(D|H) P(D|H) is the likelihood, and is given by y y 's probability distribution Normal(\\mu, \\sigma) Normal(\\mu, \\sigma) , or in probability notation, P(y|\\mu, \\sigma) P(y|\\mu, \\sigma) . P(H) P(H) is the the prior, and is given by P(\\mu, \\sigma) P(\\mu, \\sigma) . P(D) P(D) is a hard quantity to calculate, so we sort of cheat and don't use it, and merely claim that the posterior is proportional to likelihood times prior. If we look at the probability symbols again, we should notice that P(\\mu, \\sigma) P(\\mu, \\sigma) is the joint distribution between \\mu \\mu and \\sigma \\sigma . However, from observing the graphical diagram, we'll notice that \\mu \\mu and \\sigma \\sigma have no bearing on one another: we do not need to know \\mu \\mu to know the value of \\sigma \\sigma , and vice versa. Hence, they are independent of one another, and so by the rules of probability, P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) Now, by simply moving symbols around: P(H|D) = P(D|H)P(H) P(H|D) = P(D|H)P(H) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) This translates directly into Python code! def model_prob ( mu , sigma , y ): # Probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_prob = normal_prior . pdf ( mu ) # Probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_prob = sigma_prior . pdf ( sigma ) # Likelihood of data given mu and sigma likelihood = Normal ( mu , sigma ) likelihood_prob = likelihood . pdf ( y ) . prod () # Joint likelihood return mu_prob * sigma_prob * likelihood_prob If you remember, multiplying so many probability distributions together can give us underflow issues when computing, so it is common to take the log of both sides. \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) This also translates directly into Python code! def model_log_prob ( mu , sigma , y ): # log-probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_log_prob = normal_prior . logpdf ( mu ) # log-probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_log_prob = sigma_prior . logpdf ( sigma ) # log-likelihood given priors and data likelihood = Normal ( mu , sigma ) likelihood_log_prob = likelihood . logpdf ( y ) . sum () # Joint log-likelihood return mu_log_prob + sigma_log_prob + likelihood_log_prob Computing the Posterior with Sampling To identify what the values of \\mu \\mu and \\sigma \\sigma should take on given the data and priors, we can turn to sampling to help us. I am intentionally skipping over integrals which are used to compute expectations, which is what sampling is replacing. Metropolis-Hastings Sampling An easy-to-understand sampler that we can start with is the Metropolis-Hastings sampler. I first learned it in a grad-level computational biology class, but I expect most statistics undergrads should have a good working knowledge of the algorithm. For the rest of us, check out the note below on how the algorithm works. The Metropolis-Hastings Algorithm Shamelessly copied (and modified) from the Wikipedia article : For each parameter p p , do the following. Initialize an arbitrary point for the parameter (this is p_t p_t , or p p at step t t ). Define a probability density P(p_t) P(p_t) , for which we will draw new values of the parameters. Here, we will use P(p) = Normal(p_{t-1}, 1) P(p) = Normal(p_{t-1}, 1) . For each iteration: Generate candidate new candidate p_t p_t drawn from P(p_t) P(p_t) . Calculate the likelihood of the data under the previous parameter value(s) p_{t-1} p_{t-1} : L(p_{t-1}) L(p_{t-1}) Calculate the likelihood of the data under the proposed parameter value(s) p_t p_t : L(p_t) L(p_t) Calculate acceptance ratio r = \\frac{L(p_t)}{L(p_{t-1})} r = \\frac{L(p_t)}{L(p_{t-1})} . Generate a new random number on the unit interval: s \\sim U(0, 1) s \\sim U(0, 1) . Compare s s to r r . If s \\leq r s \\leq r , accept p_t p_t . If s \\gt r s \\gt r , reject p_t p_t and continue sampling again with p_{t-1} p_{t-1} . In the algorithm described in the note above, our parameters p p are actually (\\mu, \\sigma) (\\mu, \\sigma) . This means that we have to propose two numbers and sample two numbers in each loop of the sampler. To make things simple for us, let's use the normal distribution centered on 0 0 but with scale 0.1 0.1 to propose values for each. We can implement the algorithm in Python code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # Metropolis-Hastings Sampling mu_prev = np . random . normal () sigma_prev = np . random . normal () # Keep a history of the parameter values and ratio. mu_history = dict () sigma_history = dict () ratio_history = dict () for i in range ( 1000 ): mu_history [ i ] = mu_prev sigma_history [ i ] = sigma_prev mu_t = np . random . normal ( mu_prev , 0.1 ) sigma_t = np . random . normal ( sigma_prev , 0.1 ) # Compute joint log likelihood LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # Calculate the difference in log-likelihoods # (or a.k.a. ratio of likelihoods) diff_log_like = LL_t - LL_prev if diff_log_like > 0 : ratio = 1 else : # We need to exponentiate to get the correct ratio, # since all of our calculations were in log-space ratio = np . exp ( diff_log_like ) # Defensive programming check if np . isinf ( ratio ) or np . isnan ( ratio ): raise ValueError ( f \"LL_t: { LL_t } , LL_prev: { LL_prev } \" ) # Ratio comparison step ratio_history [ i ] = ratio p = np . random . uniform ( 0 , 1 ) if ratio >= p : mu_prev = mu_t sigma_prev = sigma_t Because of a desire for convenience, we chose to use a single normal distribution to sample all values. However, that distribution choice is going to bite us during sampling, because the values that we could possibly sample for the \\sigma \\sigma parameter can take on negatives, but when a negative \\sigma \\sigma is passed into the normally-distributed likelihood, we are going to get computation errors! This is because the scale parameter of a normal distribution can only be positive, and cannot be negative or zero. (If it were zero, there would be no randomness.) Transformations as a Hack The key problem here is that the support of the Exponential distribution is bound to be positive real numbers only. That said, we can get around this problem simply by sampling amongst the unbounded real number space (-\\inf, +\\inf) (-\\inf, +\\inf) , and then transforming the number by a math function to be in the bounded space. One way we can transform numbers from an unbounded space to a positive-bounded space is to use the exponential transform: y = e^x y = e^x For any given value x x , y y will be guaranteed to be positive. Knowing this, we can modify our sampling code, specifically, what was before: # Initialize in unconstrained space sigma_prev_unbounded = np . random . normal ( 0 , 1 ) # ... for i in range ( 1000 ): # ... # Propose in unconstrained space sigma_t_unbounded = np . random . normal ( sigma_prev_unbounded , 0.1 ) # Transform the sampled values to the constrained space sigma_prev = np . exp ( sigma_prev_unbounded ) sigma_t = np . exp ( sigma_t_unbounded ) # ... # Pass the transformed values into the log-likelihood calculation LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # ... And voila ! If you notice, the key trick here was to sample in unbounded space , but evalute log-likelihood in bounded space . We call the \"unbounded\" space the transformed space, while the \"bounded\" space is the original or untransformed space. We have implemented the necessary components to compute posterior distributions on parameters! Samples from Posterior If we simulate 1000 data points from a Normal(3, 1) Normal(3, 1) distribution, and pass them into the model log probability function defined above, then after running the sampler, we get a chain of values that the sampler has picked out as maximizing the joint likelihood of the data and the model. This, by the way, is essentially the simplest version of Markov Chain Monte Carlo sampling that exists in modern computational Bayesian statistics. Let's examine the trace from one run: Notice how it takes about 200 steps before the trace becomes stationary , that is it becomes a flat trend-line. If we prune the trace to just the values after the 200th iteration, we get the following trace: The samples drawn are an approximation to the expected values of \\mu \\mu and \\sigma \\sigma given the data and priors specified. Random Variables and Sampling A piece of wisdom directly quoted from my friend Colin Carroll , who is also a PyMC developer: Random variables are measures , and measures are only really defined under an integral sign. Sampling is usually defined as the act of generating data according to a certain measure. This is confusing, because we invert this relationship when we do computational statistics: we generate the data, and use that to approximate an integral or expectation. Topics We Skipped Over We intentionally skipped over a number of topics. One of them was why we used a normal distribution with scale of 0.1 to propose a different value, rather than a different scale. As it turns out the, scale parameter is a tunable hyperparameter, and in PyMC3 we do perform tuning as well. If you want to learn more about how tuning happens, Colin has a great essay on that too. We also skipped over API design, as that is a topic I will be exploring in a separate essay. It will also serve as a tour through the PyMC3 API as I understand it. An Anchoring Thought Framework for Learning Computational Bayes Having gone through this exercise has been extremely helpful in deciphering what goes on behind-the-scenes in PyMC3 (and the in-development PyMC4, which is built on top of TensorFlow probability). From digging through everything from scratch, my thought framework to think about Bayesian modelling has been updated (pun intended) to the following. Firstly, we can view a Bayesian model from the axis of prior, likelihood, posterior . Bayes' rule provides us the equation \"glue\" that links those three components together. Secondly, when doing computational Bayesian statistics, we should be able to modularly separate sampling from model definition . Sampling is computing the posterior distribution of parameters given the model and data. Model definition , by contrast, is all about providing the model structure as well as a function that calculates the joint log likelihood of the model and data. In fact, based on the exercise above, any \"sampler\" is only concerned with the model log probability (though some also require the local gradient of the log probability w.r.t. the parameters to find where to climb next), and should only be required to accept a model log probability function and a proposed set of initial parameter values, and return a chain of sampled values. Finally, I hope the \"simplest complex example\" of estimating \\mu \\mu and \\sigma \\sigma of a normal distribution helps further your understanding of the math behind Bayesian statistics. All in all, I hope this essay helps your learning, as writing it did for me! Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"An Introduction to Probability and Computational Bayesian Statistics"},{"location":"machine-learning/computational-bayesian-stats/#an-introduction-to-probability-and-computational-bayesian-statistics","text":"In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here? Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference.","title":"An Introduction to Probability and Computational Bayesian Statistics"},{"location":"machine-learning/computational-bayesian-stats/#probability-distributions","text":"We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\". Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution.","title":"Probability Distributions"},{"location":"machine-learning/computational-bayesian-stats/#base-object-implementation","text":"Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as: class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma","title":"Base Object Implementation"},{"location":"machine-learning/computational-bayesian-stats/#probability-density-function","text":"Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability density function\", or the \"PDF\". Using this, we may then extend extend this object with a method called .pdf(x) , that returns a number giving the number of credibility points assigned to the value of x passed in. import numpy as np class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma def pdf ( self , x ): return ( 1 / np . sqrt ( 2 * self . sigma ** 2 * np . pi ) * np . exp ( - ( x - self . mu ) ** 2 / 2 * self . sigma ** 2 )) If we pass in a number x from the number line, we will get back another number that tells us the number of credibility points given to that value x , under the state of the normal distribution instantiated. We'll call this P(x) P(x) . To simplify the implementation used here, we are going to borrow some machinery already available to us in the Python scientific computing ecosystem, particularly from the SciPy stats module, which gives us reference implementations of probability distributions. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x )","title":"Probability Density Function"},{"location":"machine-learning/computational-bayesian-stats/#log-probability","text":"A common task in Bayesian inference is computing the likelihood of data. Let's assume that the data {X_1, X_2, ... X_i} {X_1, X_2, ... X_i} generated are independent and identically distributed, (the famous i.i.d. term comes from this). This means, then, that the joint probability of the data that was generated is equivalent to the product of the individual probabilities of each datum: P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i) (We have to know the rules of probability to know this result; it is a topic for a different essay.) If you remember the notation above, each P(X_i) P(X_i) is an evaluation of X_i X_i on the distribution's probability density function. It being a probability value means it is bound between 0 and 1. However, multiplying many probabilities together usually will result in issues with underflow computationally, so in evaluating likelihoods, we usually stick with log-likelihoods instead. By the usual rules of math, then: \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) \\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i) To our normal distribution class, we can now add in another class method that computes the sum of log likelihoods evaluated at a bunch of i.i.d. data points. from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) def pdf ( self , x ): # Now, our PDF class method is simplified to be just a wrapper. return self . dist . pdf ( x ) def logpdf ( self , x ): return self . dist . logpdf ( x )","title":"Log Probability"},{"location":"machine-learning/computational-bayesian-stats/#random-variables","text":"","title":"Random Variables"},{"location":"machine-learning/computational-bayesian-stats/#definition","text":"Informally, a \"random variable\" is nothing more than a variable whose quantity is non-deterministic (hence random) but whose probability of taking on a certain value can be described by a probability distribution. According to the Wikipedia definition of a random variable : A random variable has a probability distribution, which specifies the probability of its values. As such, it may be tempting to conceive of a random variable as an object that has a probability distribution attribute attached to it.","title":"Definition"},{"location":"machine-learning/computational-bayesian-stats/#realizations-of-a-random-variable","text":"On the other hand, it can also be convenient to invert that relationship, and claim that a probability distribution can generate realizations of a random variable. The latter is exactly how SciPy distributions are implemented: from scipy.stats import norm # Normal distribution can generate realizations of an RV # The following returns a NumPy array of 10 draws # from a standard normal distribution. norm ( loc = 0 , scale = 1 ) . rvs ( 10 ) Realizations of a Random Variable A \"realization\" of a random variable is nothing more than generating a random number whose probability of being generated is defined by the random variable's probability density function. Because the generation of realizations of a random variable is equivalent to sampling from a probability distribution, we can extend our probability distribution definition to include a .sample(n) method: from scipy.stats import norm class Normal : def __init__ ( self , mu , sigma ): self . mu = mu self . sigma = sigma # We instantiate the distribution object here. self . dist = norm ( loc = mu , scale = sigma ) # ... def sample ( self , n ): return self . dist . rvs ( n ) Now, if we draw 10 realizations of a normally distributed random variable, and the drawing of each realization has no dependence of any kind on the previous draw, then we can claim that each draw is independent and identically distributed . This is where the fabled \" iid \" term in undergraduate statistics classes comes from.","title":"Realizations of a Random Variable"},{"location":"machine-learning/computational-bayesian-stats/#data-generating-process","text":"Now that we have covered what probability distributions are, we can now move on to other concepts that are important in Bayesian statistical modelling. Realizations of a random variable, or draws from its probability distribution, are how a Bayesian assumes data are generated. Describing how data are generated using probability distributions, or in other words, writing down the \"data generating process\", is a core activity in Bayesian statistical modelling. Viewed this way, data values generated by a random process depend on the underlying random variable's probability distribution. In other words, the random variable realizations are known, given the probability distribution used to model it. Keep this idea in mind: it is going to be important shortly.","title":"Data Generating Process"},{"location":"machine-learning/computational-bayesian-stats/#bayes-rule","text":"Now that we've covered probability distributions, we can move on to Bayes' rule. You probably have seen the following equation: P(B|A) = \\frac{P(A|B)P(B)}{P(A)} P(B|A) = \\frac{P(A|B)P(B)}{P(A)} Bayes' rule states nothing more than the fact that the conditional probability of B given A is equal to the conditional probability of A given B times the probability of B divided by the probability of A. When doing Bayesian statistical inference, we commonly take a related but distinct interpretation: P(H|D) = \\frac{P(D|H)P(H)}{P(D)} P(H|D) = \\frac{P(D|H)P(H)}{P(D)} It may look weird, but didn't we say before that data are realizations from a random variable? Why are we now treating data as a random variable? Here, we are doing not-so-intuitive but technically correct step of treating the data D D as being part of this probabilistic model (hence it \"looks\" like a random variable), alongside our model parameters H H . There's a lot of measure theory that goes into this interpretation, which at this point I have not yet mastered, and so will wave my hands in great arcs and propose that this interpretation be accepted for now and move on. Data are random variables? Notes from a chat with Colin gave me a lot to chew on, as usual: The answer is in how you define \"event\" as \"an element of a sigma algebra\". intuitively, an \"event\" is just an abstraction, so one event might be \"the coin is heads\", or in another context the event might be \"the parameters are [0.2, 0.1, 0.2]\". And so analogously, \"the data were configured as [0, 5, 2, 3]\". Notice also that the events are different if the data being ordered vs unordered are different! This was a logical leap that I had been asked about before, but did not previously have the knowledge to respond to. Thanks to Colin, I now do. With the data + hypothesis interpretation of Bayes' rule in hand, the next question arises: What math happens when we calculate posterior densities?","title":"Bayes' Rule"},{"location":"machine-learning/computational-bayesian-stats/#translating-bayes-math-to-python","text":"","title":"Translating Bayes' Math to Python"},{"location":"machine-learning/computational-bayesian-stats/#defining-posterior-log-likelihood","text":"To understand this, let's look at the simplest complex example that I could think of: Estimating the \\mu \\mu and \\sigma \\sigma parameters of a normal distribution conditioned on observing data points y y . If we assume a data generating process that looks like the following (with no probability distributions specified yet): graph TD; \u03bc((\u03bc)) --> y(y); \u03c3((\u03c3)) --> y(y); We can write out the following probabilistic model (now explicitly specifying probability distributions): \\mu \\sim Normal(0, 10) \\mu \\sim Normal(0, 10) \\sigma \\sim Exponential(1) \\sigma \\sim Exponential(1) y \\sim Normal(\\mu, \\sigma) y \\sim Normal(\\mu, \\sigma) Let's now map the symbols onto Bayes' rule. H H are the parameters, which are \\mu \\mu and \\sigma \\sigma here. D D is the data that I will observe P(H|D) P(H|D) is the posterior, which we would like to compute. P(D|H) P(D|H) is the likelihood, and is given by y y 's probability distribution Normal(\\mu, \\sigma) Normal(\\mu, \\sigma) , or in probability notation, P(y|\\mu, \\sigma) P(y|\\mu, \\sigma) . P(H) P(H) is the the prior, and is given by P(\\mu, \\sigma) P(\\mu, \\sigma) . P(D) P(D) is a hard quantity to calculate, so we sort of cheat and don't use it, and merely claim that the posterior is proportional to likelihood times prior. If we look at the probability symbols again, we should notice that P(\\mu, \\sigma) P(\\mu, \\sigma) is the joint distribution between \\mu \\mu and \\sigma \\sigma . However, from observing the graphical diagram, we'll notice that \\mu \\mu and \\sigma \\sigma have no bearing on one another: we do not need to know \\mu \\mu to know the value of \\sigma \\sigma , and vice versa. Hence, they are independent of one another, and so by the rules of probability, P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H) Now, by simply moving symbols around: P(H|D) = P(D|H)P(H) P(H|D) = P(D|H)P(H) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu,\\sigma)P(\\mu, \\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma) This translates directly into Python code! def model_prob ( mu , sigma , y ): # Probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_prob = normal_prior . pdf ( mu ) # Probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_prob = sigma_prior . pdf ( sigma ) # Likelihood of data given mu and sigma likelihood = Normal ( mu , sigma ) likelihood_prob = likelihood . pdf ( y ) . prod () # Joint likelihood return mu_prob * sigma_prob * likelihood_prob If you remember, multiplying so many probability distributions together can give us underflow issues when computing, so it is common to take the log of both sides. \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) \\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma)) This also translates directly into Python code! def model_log_prob ( mu , sigma , y ): # log-probability of mu under prior. normal_prior = Normal ( 0 , 10 ) mu_log_prob = normal_prior . logpdf ( mu ) # log-probability of sigma under prior. sigma_prior = Exponential ( 1 ) sigma_log_prob = sigma_prior . logpdf ( sigma ) # log-likelihood given priors and data likelihood = Normal ( mu , sigma ) likelihood_log_prob = likelihood . logpdf ( y ) . sum () # Joint log-likelihood return mu_log_prob + sigma_log_prob + likelihood_log_prob","title":"Defining Posterior Log-Likelihood"},{"location":"machine-learning/computational-bayesian-stats/#computing-the-posterior-with-sampling","text":"To identify what the values of \\mu \\mu and \\sigma \\sigma should take on given the data and priors, we can turn to sampling to help us. I am intentionally skipping over integrals which are used to compute expectations, which is what sampling is replacing.","title":"Computing the Posterior with Sampling"},{"location":"machine-learning/computational-bayesian-stats/#metropolis-hastings-sampling","text":"An easy-to-understand sampler that we can start with is the Metropolis-Hastings sampler. I first learned it in a grad-level computational biology class, but I expect most statistics undergrads should have a good working knowledge of the algorithm. For the rest of us, check out the note below on how the algorithm works. The Metropolis-Hastings Algorithm Shamelessly copied (and modified) from the Wikipedia article : For each parameter p p , do the following. Initialize an arbitrary point for the parameter (this is p_t p_t , or p p at step t t ). Define a probability density P(p_t) P(p_t) , for which we will draw new values of the parameters. Here, we will use P(p) = Normal(p_{t-1}, 1) P(p) = Normal(p_{t-1}, 1) . For each iteration: Generate candidate new candidate p_t p_t drawn from P(p_t) P(p_t) . Calculate the likelihood of the data under the previous parameter value(s) p_{t-1} p_{t-1} : L(p_{t-1}) L(p_{t-1}) Calculate the likelihood of the data under the proposed parameter value(s) p_t p_t : L(p_t) L(p_t) Calculate acceptance ratio r = \\frac{L(p_t)}{L(p_{t-1})} r = \\frac{L(p_t)}{L(p_{t-1})} . Generate a new random number on the unit interval: s \\sim U(0, 1) s \\sim U(0, 1) . Compare s s to r r . If s \\leq r s \\leq r , accept p_t p_t . If s \\gt r s \\gt r , reject p_t p_t and continue sampling again with p_{t-1} p_{t-1} . In the algorithm described in the note above, our parameters p p are actually (\\mu, \\sigma) (\\mu, \\sigma) . This means that we have to propose two numbers and sample two numbers in each loop of the sampler. To make things simple for us, let's use the normal distribution centered on 0 0 but with scale 0.1 0.1 to propose values for each. We can implement the algorithm in Python code: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # Metropolis-Hastings Sampling mu_prev = np . random . normal () sigma_prev = np . random . normal () # Keep a history of the parameter values and ratio. mu_history = dict () sigma_history = dict () ratio_history = dict () for i in range ( 1000 ): mu_history [ i ] = mu_prev sigma_history [ i ] = sigma_prev mu_t = np . random . normal ( mu_prev , 0.1 ) sigma_t = np . random . normal ( sigma_prev , 0.1 ) # Compute joint log likelihood LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # Calculate the difference in log-likelihoods # (or a.k.a. ratio of likelihoods) diff_log_like = LL_t - LL_prev if diff_log_like > 0 : ratio = 1 else : # We need to exponentiate to get the correct ratio, # since all of our calculations were in log-space ratio = np . exp ( diff_log_like ) # Defensive programming check if np . isinf ( ratio ) or np . isnan ( ratio ): raise ValueError ( f \"LL_t: { LL_t } , LL_prev: { LL_prev } \" ) # Ratio comparison step ratio_history [ i ] = ratio p = np . random . uniform ( 0 , 1 ) if ratio >= p : mu_prev = mu_t sigma_prev = sigma_t Because of a desire for convenience, we chose to use a single normal distribution to sample all values. However, that distribution choice is going to bite us during sampling, because the values that we could possibly sample for the \\sigma \\sigma parameter can take on negatives, but when a negative \\sigma \\sigma is passed into the normally-distributed likelihood, we are going to get computation errors! This is because the scale parameter of a normal distribution can only be positive, and cannot be negative or zero. (If it were zero, there would be no randomness.)","title":"Metropolis-Hastings Sampling"},{"location":"machine-learning/computational-bayesian-stats/#transformations-as-a-hack","text":"The key problem here is that the support of the Exponential distribution is bound to be positive real numbers only. That said, we can get around this problem simply by sampling amongst the unbounded real number space (-\\inf, +\\inf) (-\\inf, +\\inf) , and then transforming the number by a math function to be in the bounded space. One way we can transform numbers from an unbounded space to a positive-bounded space is to use the exponential transform: y = e^x y = e^x For any given value x x , y y will be guaranteed to be positive. Knowing this, we can modify our sampling code, specifically, what was before: # Initialize in unconstrained space sigma_prev_unbounded = np . random . normal ( 0 , 1 ) # ... for i in range ( 1000 ): # ... # Propose in unconstrained space sigma_t_unbounded = np . random . normal ( sigma_prev_unbounded , 0.1 ) # Transform the sampled values to the constrained space sigma_prev = np . exp ( sigma_prev_unbounded ) sigma_t = np . exp ( sigma_t_unbounded ) # ... # Pass the transformed values into the log-likelihood calculation LL_t = model_log_prob ( mu_t , sigma_t , y ) LL_prev = model_log_prob ( mu_prev , sigma_prev , y ) # ... And voila ! If you notice, the key trick here was to sample in unbounded space , but evalute log-likelihood in bounded space . We call the \"unbounded\" space the transformed space, while the \"bounded\" space is the original or untransformed space. We have implemented the necessary components to compute posterior distributions on parameters!","title":"Transformations as a Hack"},{"location":"machine-learning/computational-bayesian-stats/#samples-from-posterior","text":"If we simulate 1000 data points from a Normal(3, 1) Normal(3, 1) distribution, and pass them into the model log probability function defined above, then after running the sampler, we get a chain of values that the sampler has picked out as maximizing the joint likelihood of the data and the model. This, by the way, is essentially the simplest version of Markov Chain Monte Carlo sampling that exists in modern computational Bayesian statistics. Let's examine the trace from one run: Notice how it takes about 200 steps before the trace becomes stationary , that is it becomes a flat trend-line. If we prune the trace to just the values after the 200th iteration, we get the following trace: The samples drawn are an approximation to the expected values of \\mu \\mu and \\sigma \\sigma given the data and priors specified. Random Variables and Sampling A piece of wisdom directly quoted from my friend Colin Carroll , who is also a PyMC developer: Random variables are measures , and measures are only really defined under an integral sign. Sampling is usually defined as the act of generating data according to a certain measure. This is confusing, because we invert this relationship when we do computational statistics: we generate the data, and use that to approximate an integral or expectation.","title":"Samples from Posterior"},{"location":"machine-learning/computational-bayesian-stats/#topics-we-skipped-over","text":"We intentionally skipped over a number of topics. One of them was why we used a normal distribution with scale of 0.1 to propose a different value, rather than a different scale. As it turns out the, scale parameter is a tunable hyperparameter, and in PyMC3 we do perform tuning as well. If you want to learn more about how tuning happens, Colin has a great essay on that too. We also skipped over API design, as that is a topic I will be exploring in a separate essay. It will also serve as a tour through the PyMC3 API as I understand it.","title":"Topics We Skipped Over"},{"location":"machine-learning/computational-bayesian-stats/#an-anchoring-thought-framework-for-learning-computational-bayes","text":"Having gone through this exercise has been extremely helpful in deciphering what goes on behind-the-scenes in PyMC3 (and the in-development PyMC4, which is built on top of TensorFlow probability). From digging through everything from scratch, my thought framework to think about Bayesian modelling has been updated (pun intended) to the following. Firstly, we can view a Bayesian model from the axis of prior, likelihood, posterior . Bayes' rule provides us the equation \"glue\" that links those three components together. Secondly, when doing computational Bayesian statistics, we should be able to modularly separate sampling from model definition . Sampling is computing the posterior distribution of parameters given the model and data. Model definition , by contrast, is all about providing the model structure as well as a function that calculates the joint log likelihood of the model and data. In fact, based on the exercise above, any \"sampler\" is only concerned with the model log probability (though some also require the local gradient of the log probability w.r.t. the parameters to find where to climb next), and should only be required to accept a model log probability function and a proposed set of initial parameter values, and return a chain of sampled values. Finally, I hope the \"simplest complex example\" of estimating \\mu \\mu and \\sigma \\sigma of a normal distribution helps further your understanding of the math behind Bayesian statistics. All in all, I hope this essay helps your learning, as writing it did for me!","title":"An Anchoring Thought Framework for Learning Computational Bayes"},{"location":"machine-learning/computational-bayesian-stats/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"machine-learning/generating-markov-chains-dirichlet/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Dirichlet Processes and Hidden Markov Model Transition Matrices How do we construct transition matrices that prioritize re-entry into a constrained set of states? Especially if we don't have perfect knowledge of how many true states there are? From Matt Johnson's thesis , I learned exactly how. Some of us might be used to thinking about transition matrices that have strong diagonals. That's all good for providing stability in a sequence of transitions. But if the goal is to provide a model where the constrained set of states is given priority over the other states, then what we really need is a transition matrix where the first K columns of the transition matrix are given priority over the others. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' from jax import random , vmap , jit , grad , lax from jax.scipy import stats import jax.numpy as np import matplotlib.pyplot as plt from functools import partial import seaborn as sns To generate a transition matrix with this desired property, we can turn to the GEM distribution . The GEM distribution is one way to generate a random vector from a Dirichlet distribution (which is the generalization of a Beta distribution). You can think of it as stick-breaking, basically. We take a stick of unit length 1, and break it in two according to a draw from a Beta distribution. We record the length of the left part of the stick, and then break the right stick into two according to a draw from a Beta distribution. We then record the new length of the left side and break the right one again and again, ad infinitum or until we have reached a predefined (but finite) number of breaks. The vector of recorded lengths becomes a \"weighting\" vector. One thing to keep in mind: this weighting vector doesn't necessarily sum to 1, so in order to use the weighting vector in a transition matrix, we do have to normalize it to sum to 1, or we append the remainder of the stick to the end to get it to sum to 1. Enough said, let's dig in and try simulating this process. Firstly, we generate a vector of i.i.d. draws from a Beta distribution with parameters \\alpha = 1 \\alpha = 1 and \\beta = 1 \\beta = 1 . key = random . PRNGKey ( 45 ) # for reproducibility beta_draws = random . beta ( key , a = 1 , b = 1 , shape = ( 10 ,)) plt . plot ( beta_draws , marker = \"o\" ) sns . despine () WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Now, we take this and begin our stick-breaking process. Because it is effectively a for-loop in which each loop iteration uses carryover from the previous loop iteration, I have written it taking advantage of JAX's lax.scan function. def stick_breaking_weights ( beta_draws ): \"\"\"Return weights from a stick breaking process. :param beta_draws: i.i.d draws from a Beta distribution. This should be a row vector. \"\"\" def weighting ( occupied_probability , beta_i ): \"\"\" :param occupied_probability: The cumulative occupied probability taken up. :param beta_i: Current value of beta to consider. \"\"\" weight = ( 1 - occupied_probability ) * beta_i return occupied_probability + weight , weight occupied_probability , weights = lax . scan ( weighting , np . array ( 0. ), beta_draws ) weights = weights / np . sum ( weights ) return occupied_probability , weights occupied_prob , weights = stick_breaking_weights ( beta_draws ) plt . plot ( weights , marker = \"o\" ) plt . ylim ( - 0.1 , 1.1 ) sns . despine () Really cool! We now have a vector of weights, normalized to a probability distribution. It's worth at this point exploring the effect of varying the b b parameter in the Beta distribution: fig , axes = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 20 , 4 ), sharey = True ) bvals = np . array ([ 1 , 3 , 5 , 10 , 20 ]) beta_draws = vmap ( partial ( random . beta , key , 1 , shape = ( 10 ,)))( bvals ) occupied_probs , weights = vmap ( stick_breaking_weights )( beta_draws ) for ax , weight , bval in zip ( axes , weights , bvals ): ax . plot ( weight , marker = \"o\" ) ax . set_title ( f \"b = { bval } \" ) sns . despine () As should be visible, when we increase the b value, we get a less concentrated and flatter distribution compared to when we use a smaller b value. Thus, b acts as a \"concentration\" parameter. Smaller values means probability mass is concentrated on a smaller number of slots, while larger values means probability mass is diffused across a larger number of slots. How does this relate to transition matrices in hidden Markov models? Well, a potentially desirable property is that we wish to express is that most of the states tend to move into a certain smaller number of states, thereby concentrating the number of occupied states into a smaller set. This is equivalent to concentrating the transition matrix to a subset of columns . Let's see how we can generate this kind of transition matrix. Firstly, since transition matrices are square, let's start with a 15x15 transition matrix, i.e. one with 15 states. N_STATES = 15 CONCENTRATION = 2 beta_draws = random . beta ( key , a = 1 , b = CONCENTRATION , shape = ( N_STATES , N_STATES )) To visualize these i.i.d. Beta-distributed draws, let's use a heatmap. sns . heatmap ( beta_draws ); <AxesSubplot:> Keep in mind, this is not the transition matrix just yet. It is the precursor to one! Next up, on a row-wise basis, we convert each row to a weighting vector, thereby getting back a transition matrix: def compute_transition_matrix ( beta_draws ): _ , transition_matrix = vmap ( stick_breaking_weights )( beta_draws ) return _ , transition_matrix _ , transition_matrix = compute_transition_matrix ( beta_draws ) And visualizing the transition_matrix... sns . heatmap ( transition_matrix ); <AxesSubplot:> Voil\u00e0! We have a transition matrix that has most of the probability mass concentrated in just a few states. Let's calculate the equilibrium distribution: def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] eq_distribution = equilibrium_distribution ( transition_matrix ) plt . plot ( eq_distribution , marker = \"o\" ) sns . despine () As should be visible, we spend the majority of time in just a few states, and not too many more. At this point, it's worth exploring how the \"concentration\" parameter affects the transition matrix, and hence the equilibrium distribution. CONCENTRATIONS = np . array ([ 1 , 3 , 5 , 10 , 20 ]) N_DIMS = 30 beta_draws = vmap ( partial ( random . beta , key , 1 , shape = ( N_DIMS , N_DIMS )))( CONCENTRATIONS ) beta_draws . shape (5, 30, 30) _ , transition_matrices = vmap ( compute_transition_matrix )( beta_draws ) eq_distributions = vmap ( equilibrium_distribution )( transition_matrices ) fig , axes = plt . subplots ( nrows = 2 , ncols = 5 , figsize = ( 20 , 8 )) for ax , conc , tmat in zip ( axes [ 0 , :], CONCENTRATIONS , transition_matrices ): sns . heatmap ( tmat , ax = ax ) ax . set_title ( f \"concentration = { conc } \" ) for ax , conc , eq_dist in zip ( axes [ 1 , :], CONCENTRATIONS , eq_distributions ): ax . plot ( eq_dist , marker = \"o\" ) ax . set_title ( f \"concentration = { conc } \" ) ax . set_ylim ( - 0.1 , 1.1 ) sns . despine () As you can see, when the value of b goes up, the more diffuse the transition matrix, and the more evenly spread-out the equilibrium states will be. Let's generate Markov sequences now Now that we know how to generate transition matrices, let's step back and try to see whether the generated Markovian sequences from these transition matrices make sense, i.e. whether they display the desired properties that we seek or not. from jax import random from jax.scipy.special import logit Firstly, let's try writing the function that generates a Markov sequence given a transition matrix. from typing import List from jax import jit def markov_sequence ( key , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. Strategy: leverage categorical distribution. We need to vmap over split PRNGKeys, which will give us the desired number of draws. \"\"\" p_eq = equilibrium_distribution ( p_transition ) logit_p_eq = logit ( p_eq ) initial_state = random . categorical ( key , logits = logit_p_eq , shape = ( 1 ,)) def draw_state ( prev_state , key ): logits = logit ( p_transition [ prev_state ]) state = random . categorical ( key , logits = logits , shape = ( 1 ,)) return state , state keys = random . split ( key , sequence_length ) final_state , states = lax . scan ( draw_state , initial_state , keys ) return final_state , np . squeeze ( states ) markov_sequence = jit ( markov_sequence , static_argnums = ( 2 ,)) final , sequence = markov_sequence ( key , transition_matrices [ 0 ], 100 ) final , sequence (DeviceArray([0], dtype=int32), DeviceArray([0, 1, 2, 1, 0, 0, 3, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0, 0, 1, 0, 4, 1, 0, 7, 0, 1, 0, 1, 2, 0, 2, 4, 0, 0, 1, 2, 1, 2, 1, 0, 1, 4, 5, 0, 1, 0, 5, 0, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 3, 0, 1, 1, 1, 4, 1, 4, 1, 2, 1, 0, 1, 2, 1, 0, 3, 0, 1, 5, 0, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0], dtype=int32)) Now, I think we can generate a bunch of Markov chain sequences using vmap . def markov_seq_vmappable ( key , transition_matrix ): sequence_length = 500 return markov_sequence ( key , transition_matrix , sequence_length ) _ , sequences = vmap ( markov_seq_vmappable )( random . split ( key , 5 ), transition_matrices ) Let's plot them out! import seaborn as sns fig , axes = plt . subplots ( figsize = ( 16 , 20 ), nrows = 5 , ncols = 1 ) for ax , seq in zip ( axes , sequences ): ax . plot ( range ( len ( seq )), seq , marker = \"o\" ) sns . despine () As should be visible, as we increase the concentration parameter (really, I think this should be renamed as a \"diffusion\" parameter), the number of states that we would typically occupy increases. At smaller values of the concentration parameter, the number of states we would typically occupy decreases. Generating Markov sequences concentrated on a few stable states We've thus far generated transition matrices that are biased towards a few states, but they do tend to be jumpy, as the above scatterplots show. In other words, we have not yet generated matrices that allow for stability inside a state . Stability inside a state is generated from strong diagonals. We can engineer this in the generation of the matrix by leveraging (once again) the Beta distribution. Specifically, the generative story here is that each row of the transition matrix is generated by stick-breaking, but the diagonals are replaced by a Beta draw that is biased towards high probabilities. Let's see this in action for one transition matrix. def diagonal_draws ( key , bias_factor , shape ): return random . beta ( key , a = bias_factor , b = 1 , shape = shape ) dd = diagonal_draws ( key , bias_factor = 50 , shape = ( N_DIMS ,)) dom_diag_transition_matrix = transition_matrices [ 0 ] + np . diagflat ( dd ) def normalize_prob_vect ( v ): return v / np . sum ( v ) def normalize_transition_matrix ( transition_matrix ): return vmap ( normalize_prob_vect )( transition_matrix ) dom_diag_transition_matrix = normalize_transition_matrix ( dom_diag_transition_matrix ) sns . heatmap ( dom_diag_transition_matrix ); <AxesSubplot:> Now given this transition matrix, let's generate new sequences: _ , seq = markov_sequence ( key , dom_diag_transition_matrix , 500 ) plt . plot ( seq , marker = \"o\" ) sns . despine () As is visible from the plot above, we get sequences that tend to stay inside a state, and when they do venture out to unfavoured states (e.g. state 5), they quickly return back to a favoured state. Now, let's see what kind of sequences we get when we use the same dominant diagonal with different concentration parameters. Firstly, we generate a bunch of dominant diagonal matrices: keys = random . split ( key , 5 ) diagonals = vmap ( partial ( diagonal_draws , bias_factor = 50 , shape = ( N_DIMS ,)))( keys ) def create_dominant_diagonal ( p_transition , diagonal ): p_transition = p_transition + np . diagflat ( diagonal ) return normalize_transition_matrix ( p_transition ) dom_diag_transition_matrices = vmap ( create_dominant_diagonal )( transition_matrices , diagonals ) fig , axes = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 20 , 3 )) for ax , mat in zip ( axes , dom_diag_transition_matrices ): sns . heatmap ( mat , ax = ax ) Now, we generate a bunch of sequences. _ , sequences = vmap ( markov_seq_vmappable )( random . split ( key , 5 ), dom_diag_transition_matrices ) fig , axes = plt . subplots ( figsize = ( 16 , 20 ), nrows = 5 , ncols = 1 ) for ax , seq , conc in zip ( axes , sequences , CONCENTRATIONS ): ax . plot ( range ( len ( seq )), seq , marker = \"o\" ) ax . set_title ( f \"concentration = { conc } \" ) sns . despine () As should be visible from here, we now generate sequences that have a much higher propensity to stay within their own state, rather than jump around. Additionally, when there are more states \"available\" (i.e. concentration runs higher), we also see them stay within their own state rather than jump back down to the favoured states. Inference of the right \"concentration\" of states Given the transition matrix, can we infer the concentration parameter that best describes it? This is what we're going to try out here. We start with a vanilla transition matrix generated from a concentration = 1 setting, with no dominant diagonals. This is the easier setting to begin with: sns . heatmap ( transition_matrices [ 0 ]); <AxesSubplot:> Each row of the transition matrix is generated by running a stick breaking process forward from Beta distributed draws. We can run the process backwards to get back our Beta-distributed matrix. Because there's division involved, I have opted to operate in logarithmic space instead, to avoid over/under-flow issues. from jax import lax def beta_draw_from_weights ( weights ): def beta_from_w ( accounted_probability , weights_i ): \"\"\" :param accounted_probability: The cumulative probability acounted for. :param weights_i: Current value of weights to consider. \"\"\" denominator = 1 - accounted_probability log_denominator = np . log ( denominator ) log_beta_i = np . log ( weights_i ) - log_denominator newly_accounted_probability = accounted_probability + weights_i return newly_accounted_probability , np . exp ( log_beta_i ) final , betas = lax . scan ( beta_from_w , np . array ( 0. ), weights ) return final , betas And now, to sanity-check that it works: beta_draw = random . beta ( key , a = 1 , b = 3 , shape = ( 15 ,)) _ , weights = stick_breaking_weights ( beta_draw ) _ , beta_draw_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draw , label = \"original\" ) plt . plot ( beta_draw_hat , label = \"recovered\" ) plt . legend () sns . despine () Up till the last few values, we are basically able to recover the beta distributed draw that generated the matrix. The fundamental problem we're facing here is that when we are faced with a probability vector, we're still missing the \"last stick\" which would give us an accurate estimate of the originals. As such, only the first few are really accurate, and the accuracy of beta draw recovery goes down as we go across the vector. Let's now apply the function to every row in the transition matrix. def recover_beta ( transition_matrix ): return vmap ( beta_draw_from_weights )( transition_matrix ) _ , recovered_betas = vmap ( recover_beta )( transition_matrices ) fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 10 , 4 )) idx = 4 sns . heatmap ( recovered_betas [ idx ], ax = axes [ 0 ]) sns . heatmap ( beta_draws [ idx ], ax = axes [ 1 ]) <AxesSubplot:> Matches what we saw above - we're doing an almost-OK job here. Now, we can evaluate the logpdf of the matrix. Because each entry in the recovered betas matrix is an i.i.d. draw from the Beta distribution, and because row-wise the first 3-5 elements are accurately estimatable backwards from the weights, we will estimate the concentration parameter using only the first three columns of betas from the matrix. Test-driving the syntax below... np . sum ( stats . beta . logpdf ( recovered_betas [ 4 , :, : 3 ], a = 1 , b = 9 )) DeviceArray(153.58743, dtype=float32) Looks good! Let's now define a function for the logpdf. def transition_matrix_logpdf ( transition_matrix , concentration , num_cols = 2 ): _ , beta_recovered = recover_beta ( transition_matrix ) logp = stats . beta . logpdf ( beta_recovered [:, : num_cols ], a = 1 , b = concentration ) return np . sum ( logp ) transition_matrix_logpdf ( transition_matrices [ 1 ], 5 ) DeviceArray(20.504543, dtype=float32) Just to see if this is a gradient optimizable problem, let's plot a range of concentration values, and see what happens. log_conc_range = np . linspace ( - 3 , 3 , 10000 ) conc_range = np . exp ( log_conc_range ) def loglike_range ( transition_matrix , log_conc_range ): conc_range = np . exp ( log_conc_range ) ll = vmap ( partial ( transition_matrix_logpdf , transition_matrix ))( conc_range ) return ll ll = loglike_range ( transition_matrices [ 0 ], log_conc_range ) lls = vmap ( partial ( loglike_range , log_conc_range = log_conc_range ))( transition_matrices ) fig , axes = plt . subplots ( nrows = 1 , ncols = len ( CONCENTRATIONS ), figsize = ( 20 , 4 )) for ll , conc , ax in zip ( lls , CONCENTRATIONS , axes ): ax . plot ( conc_range , ll ) ax . set_title ( f \"concentration = { conc } \" ) sns . despine () Not bad! Visually, it appears that we do pretty good in recovering the maximum likelihood value for most of the entries, but for concentration = 20, it's more difficult to do so. Let's confirm by extracting the concentration at which we have maximum log-likelihood. maxes = vmap ( np . argmax )( lls ) mle_estimates = np . take ( conc_range , maxes ) mle_estimates DeviceArray([ 0.93080336, 3.1544516 , 5.2786283 , 10.013461 , 16.251398 ], dtype=float32) Great! Doing this brute-force is all nice and good, but one of the points of JAX is that we get to do gradient descent easily. So now, let's try to perform gradient-based optimization :). We start by first defining a loss as a function of the log of the concentration. (We use the log so that when we do gradient optimization, we can be in an unbounded space.) We also define the gradient of the loss function. def loglike_loss ( log_concentration , transition_matrix ): concentration = np . exp ( log_concentration ) ll = transition_matrix_logpdf ( transition_matrix , concentration ) return - ll loglike_loss = jit ( loglike_loss ) dloglike_loss = grad ( loglike_loss ) Next up, we do the loop. Instead of writing an explicit for-loop, we are going to some JAX trickery here: We write the loop taking advantage of lax.scan , which allows us to leverage the previous state to get back parameters to optimize. We also vmap our training loop over all 5 matrices, starting with the same log concentration starting point. This allows us to essentially train five models at one shot. (I could have pmap -ed it, but I really only have one CPU and one GPU on my computer.) Since the result is a vmapped, we have a collection of final states and historical states (which we can post-process post-hoc). Hence, we can vmap get_params over final states to get back the vector of final states (from a constant initial state). from jax.experimental.optimizers import adam init , update , get_params = adam ( 0.05 ) log_conc_start = random . normal ( key ) def step ( prev_state , i , data , dloss ): \"\"\"One step in the training loop.\"\"\" params = get_params ( prev_state ) g = dloss ( params , data ) state = update ( i , g , prev_state ) return state , state def train ( transition_matrix , dloss , params , n_steps = 200 ): \"\"\"The training loop for one transition matrix.\"\"\" stepfunc = partial ( step , data = transition_matrix , dloss = dloss ) stepfunc = jit ( stepfunc ) state = init ( params ) final_state , states = lax . scan ( stepfunc , state , np . arange ( n_steps )) return final_state , states trainfunc = partial ( train , params = log_conc_start , dloss = dloglike_loss ) trainfunc = jit ( trainfunc ) # Train across all transition matrices! final_states , states_history = vmap ( trainfunc )( transition_matrices ) np . exp ( vmap ( get_params )( final_states )) DeviceArray([ 0.93007433, 3.1553512 , 5.277723 , 10.022267 , 16.264425 ], dtype=float32) We can also get the history by vmap -ing get_params over all_states . log_concentration_history = vmap ( get_params )( states_history ) for concentration , history in zip ( CONCENTRATIONS , log_concentration_history ): plt . plot ( np . exp ( history ), label = concentration ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"Concentration Value\" ) plt . legend () sns . despine () Now, if you're wondering how I knew 200 steps would be sufficient for convergence a priori , I didn't :). I had originally tried 1000 steps before staring at the concentration curves, at which point I then knew 200 was sufficient. So... no magic there. Summary This was a bit of a whirlwind tour of a notebook, in that there were many concepts and ideas tied together into one \"project\". With respect to HMMs, it's super important to have a proper mental model of each of its components. In the case of expressing the idea that we have a restricted number of states, we may engineer this into the model by taking advantage of column-wise heaviness in a restricted subset of states. This is mathematically most naturally incorporated by composint together row-wise Dirichlet-distributed probability arrays. We can also mathematically engineer consistency in states by taking advantage of high Beta-distributed values. Compose the two together, and we get a transition matrix that favours entry into a small number of states with stability in there. Beyond that lesson, we saw the power of composable programs using JAX. vmap , jit , lax.scan , and more from the JAX toolkit gives us the ability to write performant programs that sort of \"just make sense\", once you know what their idioms are. Specifically: vmap is a vanilla for-loop, processing an elementary function over an axis of an array. lax.scan is a carry-over for-loop, processing the result of a previous iteration, with accumulation of history as well. jit gives you just-in-time compilation of a function. grad gives you gradients of any arbitrary function written in a JAX-compatible, pure functional fashion. In particular, getting used to the lax.scan idioms has been a literal door-opener. We can now write really performant loops that use results from previous iterations, such as a gradient descent training loop or an MCMC sampler. Using lax.scan , we wrote: A Markov chain state generator A generator of Dirichlet-distributed weights from i.i.d. Beta distribution draws A reverse generator/estimator of Beta distribution draws from Dirichlet-distributed weights (with some inaccuracies of course, due to a lack of information). A fully-compiled gradient descent training loop that ran extremely fast. And using vmap , we were able to do all sorts of vanilla loops, but the one I want to highlight is our ability to vmap the compiled training loop across multiple transition matrices. The fact that this actually works never has left me in awe. Props to the JAX team here! Opens the door to vmap -ing training loops across random starting points (i.e. a split PRNGKey, much like what we did in the HMM state generator). The trade-off is that we don't get nice progress bars, of course, which require that we break out of the compiled loop to show the current state. But the compilation speedups provided the opportunity to build our compiled tensor program end-to-end. We could verify first that things ran correctly on a moderately-sized number of iterations, before finally estimating how long we would need to go until convergence and letting the program run on its own. I'm sure this little change isn't too hard to adapt to, but will give you access to a whole new world of differential tensor programming that is just cool !","title":"Dirichlet Processes and Hidden Markov Model Transition Matrices"},{"location":"machine-learning/generating-markov-chains-dirichlet/#dirichlet-processes-and-hidden-markov-model-transition-matrices","text":"How do we construct transition matrices that prioritize re-entry into a constrained set of states? Especially if we don't have perfect knowledge of how many true states there are? From Matt Johnson's thesis , I learned exactly how. Some of us might be used to thinking about transition matrices that have strong diagonals. That's all good for providing stability in a sequence of transitions. But if the goal is to provide a model where the constrained set of states is given priority over the other states, then what we really need is a transition matrix where the first K columns of the transition matrix are given priority over the others. % load_ext autoreload % autoreload 2 % matplotlib inline % config InlineBackend . figure_format = 'retina' from jax import random , vmap , jit , grad , lax from jax.scipy import stats import jax.numpy as np import matplotlib.pyplot as plt from functools import partial import seaborn as sns To generate a transition matrix with this desired property, we can turn to the GEM distribution . The GEM distribution is one way to generate a random vector from a Dirichlet distribution (which is the generalization of a Beta distribution). You can think of it as stick-breaking, basically. We take a stick of unit length 1, and break it in two according to a draw from a Beta distribution. We record the length of the left part of the stick, and then break the right stick into two according to a draw from a Beta distribution. We then record the new length of the left side and break the right one again and again, ad infinitum or until we have reached a predefined (but finite) number of breaks. The vector of recorded lengths becomes a \"weighting\" vector. One thing to keep in mind: this weighting vector doesn't necessarily sum to 1, so in order to use the weighting vector in a transition matrix, we do have to normalize it to sum to 1, or we append the remainder of the stick to the end to get it to sum to 1. Enough said, let's dig in and try simulating this process. Firstly, we generate a vector of i.i.d. draws from a Beta distribution with parameters \\alpha = 1 \\alpha = 1 and \\beta = 1 \\beta = 1 . key = random . PRNGKey ( 45 ) # for reproducibility beta_draws = random . beta ( key , a = 1 , b = 1 , shape = ( 10 ,)) plt . plot ( beta_draws , marker = \"o\" ) sns . despine () WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.) Now, we take this and begin our stick-breaking process. Because it is effectively a for-loop in which each loop iteration uses carryover from the previous loop iteration, I have written it taking advantage of JAX's lax.scan function. def stick_breaking_weights ( beta_draws ): \"\"\"Return weights from a stick breaking process. :param beta_draws: i.i.d draws from a Beta distribution. This should be a row vector. \"\"\" def weighting ( occupied_probability , beta_i ): \"\"\" :param occupied_probability: The cumulative occupied probability taken up. :param beta_i: Current value of beta to consider. \"\"\" weight = ( 1 - occupied_probability ) * beta_i return occupied_probability + weight , weight occupied_probability , weights = lax . scan ( weighting , np . array ( 0. ), beta_draws ) weights = weights / np . sum ( weights ) return occupied_probability , weights occupied_prob , weights = stick_breaking_weights ( beta_draws ) plt . plot ( weights , marker = \"o\" ) plt . ylim ( - 0.1 , 1.1 ) sns . despine () Really cool! We now have a vector of weights, normalized to a probability distribution. It's worth at this point exploring the effect of varying the b b parameter in the Beta distribution: fig , axes = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 20 , 4 ), sharey = True ) bvals = np . array ([ 1 , 3 , 5 , 10 , 20 ]) beta_draws = vmap ( partial ( random . beta , key , 1 , shape = ( 10 ,)))( bvals ) occupied_probs , weights = vmap ( stick_breaking_weights )( beta_draws ) for ax , weight , bval in zip ( axes , weights , bvals ): ax . plot ( weight , marker = \"o\" ) ax . set_title ( f \"b = { bval } \" ) sns . despine () As should be visible, when we increase the b value, we get a less concentrated and flatter distribution compared to when we use a smaller b value. Thus, b acts as a \"concentration\" parameter. Smaller values means probability mass is concentrated on a smaller number of slots, while larger values means probability mass is diffused across a larger number of slots. How does this relate to transition matrices in hidden Markov models? Well, a potentially desirable property is that we wish to express is that most of the states tend to move into a certain smaller number of states, thereby concentrating the number of occupied states into a smaller set. This is equivalent to concentrating the transition matrix to a subset of columns . Let's see how we can generate this kind of transition matrix. Firstly, since transition matrices are square, let's start with a 15x15 transition matrix, i.e. one with 15 states. N_STATES = 15 CONCENTRATION = 2 beta_draws = random . beta ( key , a = 1 , b = CONCENTRATION , shape = ( N_STATES , N_STATES )) To visualize these i.i.d. Beta-distributed draws, let's use a heatmap. sns . heatmap ( beta_draws ); <AxesSubplot:> Keep in mind, this is not the transition matrix just yet. It is the precursor to one! Next up, on a row-wise basis, we convert each row to a weighting vector, thereby getting back a transition matrix: def compute_transition_matrix ( beta_draws ): _ , transition_matrix = vmap ( stick_breaking_weights )( beta_draws ) return _ , transition_matrix _ , transition_matrix = compute_transition_matrix ( beta_draws ) And visualizing the transition_matrix... sns . heatmap ( transition_matrix ); <AxesSubplot:> Voil\u00e0! We have a transition matrix that has most of the probability mass concentrated in just a few states. Let's calculate the equilibrium distribution: def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] eq_distribution = equilibrium_distribution ( transition_matrix ) plt . plot ( eq_distribution , marker = \"o\" ) sns . despine () As should be visible, we spend the majority of time in just a few states, and not too many more. At this point, it's worth exploring how the \"concentration\" parameter affects the transition matrix, and hence the equilibrium distribution. CONCENTRATIONS = np . array ([ 1 , 3 , 5 , 10 , 20 ]) N_DIMS = 30 beta_draws = vmap ( partial ( random . beta , key , 1 , shape = ( N_DIMS , N_DIMS )))( CONCENTRATIONS ) beta_draws . shape (5, 30, 30) _ , transition_matrices = vmap ( compute_transition_matrix )( beta_draws ) eq_distributions = vmap ( equilibrium_distribution )( transition_matrices ) fig , axes = plt . subplots ( nrows = 2 , ncols = 5 , figsize = ( 20 , 8 )) for ax , conc , tmat in zip ( axes [ 0 , :], CONCENTRATIONS , transition_matrices ): sns . heatmap ( tmat , ax = ax ) ax . set_title ( f \"concentration = { conc } \" ) for ax , conc , eq_dist in zip ( axes [ 1 , :], CONCENTRATIONS , eq_distributions ): ax . plot ( eq_dist , marker = \"o\" ) ax . set_title ( f \"concentration = { conc } \" ) ax . set_ylim ( - 0.1 , 1.1 ) sns . despine () As you can see, when the value of b goes up, the more diffuse the transition matrix, and the more evenly spread-out the equilibrium states will be.","title":"Dirichlet Processes and Hidden Markov Model Transition Matrices"},{"location":"machine-learning/generating-markov-chains-dirichlet/#lets-generate-markov-sequences-now","text":"Now that we know how to generate transition matrices, let's step back and try to see whether the generated Markovian sequences from these transition matrices make sense, i.e. whether they display the desired properties that we seek or not. from jax import random from jax.scipy.special import logit Firstly, let's try writing the function that generates a Markov sequence given a transition matrix. from typing import List from jax import jit def markov_sequence ( key , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. Strategy: leverage categorical distribution. We need to vmap over split PRNGKeys, which will give us the desired number of draws. \"\"\" p_eq = equilibrium_distribution ( p_transition ) logit_p_eq = logit ( p_eq ) initial_state = random . categorical ( key , logits = logit_p_eq , shape = ( 1 ,)) def draw_state ( prev_state , key ): logits = logit ( p_transition [ prev_state ]) state = random . categorical ( key , logits = logits , shape = ( 1 ,)) return state , state keys = random . split ( key , sequence_length ) final_state , states = lax . scan ( draw_state , initial_state , keys ) return final_state , np . squeeze ( states ) markov_sequence = jit ( markov_sequence , static_argnums = ( 2 ,)) final , sequence = markov_sequence ( key , transition_matrices [ 0 ], 100 ) final , sequence (DeviceArray([0], dtype=int32), DeviceArray([0, 1, 2, 1, 0, 0, 3, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 2, 0, 0, 1, 0, 4, 1, 0, 7, 0, 1, 0, 1, 2, 0, 2, 4, 0, 0, 1, 2, 1, 2, 1, 0, 1, 4, 5, 0, 1, 0, 5, 0, 1, 0, 1, 0, 1, 0, 1, 2, 1, 1, 2, 0, 3, 0, 1, 1, 1, 4, 1, 4, 1, 2, 1, 0, 1, 2, 1, 0, 3, 0, 1, 5, 0, 1, 2, 1, 0, 1, 0, 1, 0, 3, 0, 1, 0, 1, 0], dtype=int32)) Now, I think we can generate a bunch of Markov chain sequences using vmap . def markov_seq_vmappable ( key , transition_matrix ): sequence_length = 500 return markov_sequence ( key , transition_matrix , sequence_length ) _ , sequences = vmap ( markov_seq_vmappable )( random . split ( key , 5 ), transition_matrices ) Let's plot them out! import seaborn as sns fig , axes = plt . subplots ( figsize = ( 16 , 20 ), nrows = 5 , ncols = 1 ) for ax , seq in zip ( axes , sequences ): ax . plot ( range ( len ( seq )), seq , marker = \"o\" ) sns . despine () As should be visible, as we increase the concentration parameter (really, I think this should be renamed as a \"diffusion\" parameter), the number of states that we would typically occupy increases. At smaller values of the concentration parameter, the number of states we would typically occupy decreases.","title":"Let's generate Markov sequences now"},{"location":"machine-learning/generating-markov-chains-dirichlet/#generating-markov-sequences-concentrated-on-a-few-stable-states","text":"We've thus far generated transition matrices that are biased towards a few states, but they do tend to be jumpy, as the above scatterplots show. In other words, we have not yet generated matrices that allow for stability inside a state . Stability inside a state is generated from strong diagonals. We can engineer this in the generation of the matrix by leveraging (once again) the Beta distribution. Specifically, the generative story here is that each row of the transition matrix is generated by stick-breaking, but the diagonals are replaced by a Beta draw that is biased towards high probabilities. Let's see this in action for one transition matrix. def diagonal_draws ( key , bias_factor , shape ): return random . beta ( key , a = bias_factor , b = 1 , shape = shape ) dd = diagonal_draws ( key , bias_factor = 50 , shape = ( N_DIMS ,)) dom_diag_transition_matrix = transition_matrices [ 0 ] + np . diagflat ( dd ) def normalize_prob_vect ( v ): return v / np . sum ( v ) def normalize_transition_matrix ( transition_matrix ): return vmap ( normalize_prob_vect )( transition_matrix ) dom_diag_transition_matrix = normalize_transition_matrix ( dom_diag_transition_matrix ) sns . heatmap ( dom_diag_transition_matrix ); <AxesSubplot:> Now given this transition matrix, let's generate new sequences: _ , seq = markov_sequence ( key , dom_diag_transition_matrix , 500 ) plt . plot ( seq , marker = \"o\" ) sns . despine () As is visible from the plot above, we get sequences that tend to stay inside a state, and when they do venture out to unfavoured states (e.g. state 5), they quickly return back to a favoured state. Now, let's see what kind of sequences we get when we use the same dominant diagonal with different concentration parameters. Firstly, we generate a bunch of dominant diagonal matrices: keys = random . split ( key , 5 ) diagonals = vmap ( partial ( diagonal_draws , bias_factor = 50 , shape = ( N_DIMS ,)))( keys ) def create_dominant_diagonal ( p_transition , diagonal ): p_transition = p_transition + np . diagflat ( diagonal ) return normalize_transition_matrix ( p_transition ) dom_diag_transition_matrices = vmap ( create_dominant_diagonal )( transition_matrices , diagonals ) fig , axes = plt . subplots ( nrows = 1 , ncols = 5 , figsize = ( 20 , 3 )) for ax , mat in zip ( axes , dom_diag_transition_matrices ): sns . heatmap ( mat , ax = ax ) Now, we generate a bunch of sequences. _ , sequences = vmap ( markov_seq_vmappable )( random . split ( key , 5 ), dom_diag_transition_matrices ) fig , axes = plt . subplots ( figsize = ( 16 , 20 ), nrows = 5 , ncols = 1 ) for ax , seq , conc in zip ( axes , sequences , CONCENTRATIONS ): ax . plot ( range ( len ( seq )), seq , marker = \"o\" ) ax . set_title ( f \"concentration = { conc } \" ) sns . despine () As should be visible from here, we now generate sequences that have a much higher propensity to stay within their own state, rather than jump around. Additionally, when there are more states \"available\" (i.e. concentration runs higher), we also see them stay within their own state rather than jump back down to the favoured states.","title":"Generating Markov sequences concentrated on a few stable states"},{"location":"machine-learning/generating-markov-chains-dirichlet/#inference-of-the-right-concentration-of-states","text":"Given the transition matrix, can we infer the concentration parameter that best describes it? This is what we're going to try out here. We start with a vanilla transition matrix generated from a concentration = 1 setting, with no dominant diagonals. This is the easier setting to begin with: sns . heatmap ( transition_matrices [ 0 ]); <AxesSubplot:> Each row of the transition matrix is generated by running a stick breaking process forward from Beta distributed draws. We can run the process backwards to get back our Beta-distributed matrix. Because there's division involved, I have opted to operate in logarithmic space instead, to avoid over/under-flow issues. from jax import lax def beta_draw_from_weights ( weights ): def beta_from_w ( accounted_probability , weights_i ): \"\"\" :param accounted_probability: The cumulative probability acounted for. :param weights_i: Current value of weights to consider. \"\"\" denominator = 1 - accounted_probability log_denominator = np . log ( denominator ) log_beta_i = np . log ( weights_i ) - log_denominator newly_accounted_probability = accounted_probability + weights_i return newly_accounted_probability , np . exp ( log_beta_i ) final , betas = lax . scan ( beta_from_w , np . array ( 0. ), weights ) return final , betas And now, to sanity-check that it works: beta_draw = random . beta ( key , a = 1 , b = 3 , shape = ( 15 ,)) _ , weights = stick_breaking_weights ( beta_draw ) _ , beta_draw_hat = beta_draw_from_weights ( weights ) plt . plot ( beta_draw , label = \"original\" ) plt . plot ( beta_draw_hat , label = \"recovered\" ) plt . legend () sns . despine () Up till the last few values, we are basically able to recover the beta distributed draw that generated the matrix. The fundamental problem we're facing here is that when we are faced with a probability vector, we're still missing the \"last stick\" which would give us an accurate estimate of the originals. As such, only the first few are really accurate, and the accuracy of beta draw recovery goes down as we go across the vector. Let's now apply the function to every row in the transition matrix. def recover_beta ( transition_matrix ): return vmap ( beta_draw_from_weights )( transition_matrix ) _ , recovered_betas = vmap ( recover_beta )( transition_matrices ) fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 10 , 4 )) idx = 4 sns . heatmap ( recovered_betas [ idx ], ax = axes [ 0 ]) sns . heatmap ( beta_draws [ idx ], ax = axes [ 1 ]) <AxesSubplot:> Matches what we saw above - we're doing an almost-OK job here. Now, we can evaluate the logpdf of the matrix. Because each entry in the recovered betas matrix is an i.i.d. draw from the Beta distribution, and because row-wise the first 3-5 elements are accurately estimatable backwards from the weights, we will estimate the concentration parameter using only the first three columns of betas from the matrix. Test-driving the syntax below... np . sum ( stats . beta . logpdf ( recovered_betas [ 4 , :, : 3 ], a = 1 , b = 9 )) DeviceArray(153.58743, dtype=float32) Looks good! Let's now define a function for the logpdf. def transition_matrix_logpdf ( transition_matrix , concentration , num_cols = 2 ): _ , beta_recovered = recover_beta ( transition_matrix ) logp = stats . beta . logpdf ( beta_recovered [:, : num_cols ], a = 1 , b = concentration ) return np . sum ( logp ) transition_matrix_logpdf ( transition_matrices [ 1 ], 5 ) DeviceArray(20.504543, dtype=float32) Just to see if this is a gradient optimizable problem, let's plot a range of concentration values, and see what happens. log_conc_range = np . linspace ( - 3 , 3 , 10000 ) conc_range = np . exp ( log_conc_range ) def loglike_range ( transition_matrix , log_conc_range ): conc_range = np . exp ( log_conc_range ) ll = vmap ( partial ( transition_matrix_logpdf , transition_matrix ))( conc_range ) return ll ll = loglike_range ( transition_matrices [ 0 ], log_conc_range ) lls = vmap ( partial ( loglike_range , log_conc_range = log_conc_range ))( transition_matrices ) fig , axes = plt . subplots ( nrows = 1 , ncols = len ( CONCENTRATIONS ), figsize = ( 20 , 4 )) for ll , conc , ax in zip ( lls , CONCENTRATIONS , axes ): ax . plot ( conc_range , ll ) ax . set_title ( f \"concentration = { conc } \" ) sns . despine () Not bad! Visually, it appears that we do pretty good in recovering the maximum likelihood value for most of the entries, but for concentration = 20, it's more difficult to do so. Let's confirm by extracting the concentration at which we have maximum log-likelihood. maxes = vmap ( np . argmax )( lls ) mle_estimates = np . take ( conc_range , maxes ) mle_estimates DeviceArray([ 0.93080336, 3.1544516 , 5.2786283 , 10.013461 , 16.251398 ], dtype=float32) Great! Doing this brute-force is all nice and good, but one of the points of JAX is that we get to do gradient descent easily. So now, let's try to perform gradient-based optimization :). We start by first defining a loss as a function of the log of the concentration. (We use the log so that when we do gradient optimization, we can be in an unbounded space.) We also define the gradient of the loss function. def loglike_loss ( log_concentration , transition_matrix ): concentration = np . exp ( log_concentration ) ll = transition_matrix_logpdf ( transition_matrix , concentration ) return - ll loglike_loss = jit ( loglike_loss ) dloglike_loss = grad ( loglike_loss ) Next up, we do the loop. Instead of writing an explicit for-loop, we are going to some JAX trickery here: We write the loop taking advantage of lax.scan , which allows us to leverage the previous state to get back parameters to optimize. We also vmap our training loop over all 5 matrices, starting with the same log concentration starting point. This allows us to essentially train five models at one shot. (I could have pmap -ed it, but I really only have one CPU and one GPU on my computer.) Since the result is a vmapped, we have a collection of final states and historical states (which we can post-process post-hoc). Hence, we can vmap get_params over final states to get back the vector of final states (from a constant initial state). from jax.experimental.optimizers import adam init , update , get_params = adam ( 0.05 ) log_conc_start = random . normal ( key ) def step ( prev_state , i , data , dloss ): \"\"\"One step in the training loop.\"\"\" params = get_params ( prev_state ) g = dloss ( params , data ) state = update ( i , g , prev_state ) return state , state def train ( transition_matrix , dloss , params , n_steps = 200 ): \"\"\"The training loop for one transition matrix.\"\"\" stepfunc = partial ( step , data = transition_matrix , dloss = dloss ) stepfunc = jit ( stepfunc ) state = init ( params ) final_state , states = lax . scan ( stepfunc , state , np . arange ( n_steps )) return final_state , states trainfunc = partial ( train , params = log_conc_start , dloss = dloglike_loss ) trainfunc = jit ( trainfunc ) # Train across all transition matrices! final_states , states_history = vmap ( trainfunc )( transition_matrices ) np . exp ( vmap ( get_params )( final_states )) DeviceArray([ 0.93007433, 3.1553512 , 5.277723 , 10.022267 , 16.264425 ], dtype=float32) We can also get the history by vmap -ing get_params over all_states . log_concentration_history = vmap ( get_params )( states_history ) for concentration , history in zip ( CONCENTRATIONS , log_concentration_history ): plt . plot ( np . exp ( history ), label = concentration ) plt . xlabel ( \"Iteration\" ) plt . ylabel ( \"Concentration Value\" ) plt . legend () sns . despine () Now, if you're wondering how I knew 200 steps would be sufficient for convergence a priori , I didn't :). I had originally tried 1000 steps before staring at the concentration curves, at which point I then knew 200 was sufficient. So... no magic there.","title":"Inference of the right \"concentration\" of states"},{"location":"machine-learning/generating-markov-chains-dirichlet/#summary","text":"This was a bit of a whirlwind tour of a notebook, in that there were many concepts and ideas tied together into one \"project\". With respect to HMMs, it's super important to have a proper mental model of each of its components. In the case of expressing the idea that we have a restricted number of states, we may engineer this into the model by taking advantage of column-wise heaviness in a restricted subset of states. This is mathematically most naturally incorporated by composint together row-wise Dirichlet-distributed probability arrays. We can also mathematically engineer consistency in states by taking advantage of high Beta-distributed values. Compose the two together, and we get a transition matrix that favours entry into a small number of states with stability in there. Beyond that lesson, we saw the power of composable programs using JAX. vmap , jit , lax.scan , and more from the JAX toolkit gives us the ability to write performant programs that sort of \"just make sense\", once you know what their idioms are. Specifically: vmap is a vanilla for-loop, processing an elementary function over an axis of an array. lax.scan is a carry-over for-loop, processing the result of a previous iteration, with accumulation of history as well. jit gives you just-in-time compilation of a function. grad gives you gradients of any arbitrary function written in a JAX-compatible, pure functional fashion. In particular, getting used to the lax.scan idioms has been a literal door-opener. We can now write really performant loops that use results from previous iterations, such as a gradient descent training loop or an MCMC sampler. Using lax.scan , we wrote: A Markov chain state generator A generator of Dirichlet-distributed weights from i.i.d. Beta distribution draws A reverse generator/estimator of Beta distribution draws from Dirichlet-distributed weights (with some inaccuracies of course, due to a lack of information). A fully-compiled gradient descent training loop that ran extremely fast. And using vmap , we were able to do all sorts of vanilla loops, but the one I want to highlight is our ability to vmap the compiled training loop across multiple transition matrices. The fact that this actually works never has left me in awe. Props to the JAX team here! Opens the door to vmap -ing training loops across random starting points (i.e. a split PRNGKey, much like what we did in the HMM state generator). The trade-off is that we don't get nice progress bars, of course, which require that we break out of the compiled loop to show the current state. But the compilation speedups provided the opportunity to build our compiled tensor program end-to-end. We could verify first that things ran correctly on a moderately-sized number of iterations, before finally estimating how long we would need to go until convergence and letting the program run on its own. I'm sure this little change isn't too hard to adapt to, but will give you access to a whole new world of differential tensor programming that is just cool !","title":"Summary"},{"location":"machine-learning/markov-models/","text":"% load_ext autoreload % autoreload 2 % config InlineBackend . figure_format = 'retina' import pymc3 as pm Markov Models From The Bottom Up, with Python Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were the in thing for processing time series and biological data. Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well. You'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too. Markov Models: What they are, with mostly plain English and some math The simplest Markov models assume that we have a system that contains a finite set of states, and that the system transitions between these states with some probability at each time step t t , thus generating a sequence of states over time. Let's call these states S S , where S = \\{s_1, s_2, ..., s_n\\} S = \\{s_1, s_2, ..., s_n\\} To keep things simple, let's start with three states: S = \\{s_1, s_2, s_3\\} S = \\{s_1, s_2, s_3\\} A Markov model generates a sequence of states, with one possible realization being: \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} And generically, we represent it as a sequence of states x_t, x_{t+1}... x_{t+n} x_t, x_{t+1}... x_{t+n} . (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following: Initializing a Markov chain Every Markov chain needs to be initialized. To do so, we need an initial state probability vector , which tells us what the distribution of initial states will be. Let's call the vector p_S p_S , where the subscript S S indicates that it is for the \"states\". p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} Semantically, they allocate the probabilities of starting the sequence at a given state. For example, we might assume a discrete uniform distribution, which in Python would look like: import numpy as np p_init = np . array ([ 1 / 3. , 1 / 3. , 1 / 3. ]) Alternatively, we might assume a fixed starting point, which can be expressed as the p_S p_S array: p_init = np . array ([ 0 , 1 , 0 ]) Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion: # State 0: 0.1 probability # State 1: 0.8 probability # State 2: 0.1 probability p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Finally, we might assume that the system was long-running before we started observing the sequence of states, and as such the initial state was drawn as one realization of some equilibrated distribution of states. Keep this idea in your head, as we'll need it later. For now, just to keep things concrete, let's specify an initial distribution as a non-uniform probability vector. import numpy as np p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Modelling transitions between states To know how a system transitions between states, we now need a transition matrix . The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.) By convention, transition matrix rows correspond to the state at time t t , while columns correspond to state at time t+1 t+1 . Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated. Let's call the transition matrix P_{transition} P_{transition} . The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: transition transition doesn't refer to time but simply indicates that it is for transitioning states, P P is used because it is a probability matrix. P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} Using the transition matrix, we can express that the system likes to stay in the state that it enters into, by assigning larger probability mass to the diagonals. Alternatively, we can express that the system likes to transition out of states that it enters into, by assigning larger probability mass to the off-diagonal. Alrighty, enough with that now, let's initialize a transition matrix below. p_transition = np . array ( [[ 0.90 , 0.05 , 0.05 ], [ 0.01 , 0.90 , 0.09 ], [ 0.07 , 0.03 , 0.9 ]] ) p_transition array([[0.9 , 0.05, 0.05], [0.01, 0.9 , 0.09], [0.07, 0.03, 0.9 ]]) And just to confirm with you that each row sums to one: assert p_transition [ 0 , :] . sum () == 1 assert p_transition [ 1 , :] . sum () == 1 assert p_transition [ 2 , :] . sum () == 1 Equilibrium or Stationary Distribution Now, do you remember how above we talked about the Markov chain being in some \"equilibrated\" state? Well, the stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time. An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same, as the equilibrium distribution only depends on the transition matrix. Here's how to think about the equilibrium: if you were to imagine instantiating a thousand Markov chains using the initial distribution p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} 10% would start out in state 1 80% would start out in state 2 10% would start out in state 3 However, if you ran each of the systems to a large number of time steps (say, 1 million time steps, to exaggerate the point) then how the states were distributed initially wouldn't matter, as how they transition from time step to time step begins to dominate. We could simulate this explicitly in Python, but as it turns out, there is a mathematical shortcut that involves simple dot products. Let's check it out. Assume we have an initial state and a transition matrix. We're going to reuse p_init from above, but use a different p_transition to make the equilibrium distribution values distinct. This will make it easier for us to plot later. p_transition_example = np . array ( [[ 0.6 , 0.2 , 0.2 ], [ 0.05 , 0.9 , 0.05 ], [ 0.1 , 0.2 , 0.7 ]] ) To simulate the distribution of states in the next time step, we take the initial distribution p_init and matrix multiply it against the transition matrix. p_next = p_init @ p_transition_example p_next array([0.11, 0.76, 0.13]) We can do it again to simulate the distribution of states in the next time step after: p_next = p_next @ p_transition_example p_next array([0.117, 0.732, 0.151]) Let's now write a for-loop to automate the process. p_state_t = [ p_init ] for i in range ( 200 ): # 200 time steps sorta, kinda, approximates infinite time :) p_state_t . append ( p_state_t [ - 1 ] @ p_transition_example ) To make it easier for you to see what we've generated, let's make the p_state_t list into a pandas DataFrame. import pandas as pd state_distributions = pd . DataFrame ( p_state_t ) state_distributions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0.100000 0.800000 0.10000 1 0.110000 0.760000 0.13000 2 0.117000 0.732000 0.15100 3 0.121900 0.712400 0.16570 4 0.125330 0.698680 0.17599 ... ... ... ... 196 0.133333 0.666667 0.20000 197 0.133333 0.666667 0.20000 198 0.133333 0.666667 0.20000 199 0.133333 0.666667 0.20000 200 0.133333 0.666667 0.20000 201 rows \u00d7 3 columns Now, let's plot what the distributions look like. import matplotlib.pyplot as plt state_distributions . plot (); If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be: the equilibrium state distribution, which is the fraction of time the Markov chain is in that state over infinite time, will always be the same as long as the transition matrix stays the same. print ( p_state_t [ - 1 ]) [0.13333333 0.66666667 0.2 ] As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to this blog post from which I modified the code to fit the variable naming here.) def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) b = np . transpose ( np . array ([ 0 ] * n_states + [ 1 ])) p_eq = np . linalg . solve ( a = np . transpose ( A ) . dot ( A ), b = np . transpose ( A ) . dot ( b ) ) return p_eq # alternative def equilibrium_distribution ( p_transition ): \"\"\"This implementation comes from Colin Carroll, who kindly reviewed the notebook\"\"\" n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] print ( equilibrium_distribution ( p_transition_example )) [0.13333333 0.66666667 0.2 ] Generating a Markov Sequence Generating a Markov sequence means we \"forward\" simulate the chain by: (1) Optionally drawing an initial state from p_S p_S (let's call that s_{t} s_{t} ). This is done by drawing from a multinomial distribution: s_t \\sim Multinomial(1, p_S) s_t \\sim Multinomial(1, p_S) If we assume (and keep in mind that we don't have to) that the system was equilibrated before we started observing its state sequence, then the initial state distribution is equivalent to the equilibrium distribution. All this means that we don't necessarily have to specify the initial distribution explicitly. (2) Drawing the next state by indexing into the transition matrix p_T p_T , and drawing a new state based on the Multinomial distribution: s_{t+1} \\sim Multinomial(1, p_{T_i}) s_{t+1} \\sim Multinomial(1, p_{T_i}) where i i is the index of the state. I previously wrote about what probability distributions are , leveraging the SciPy probability distributions library. We're going to use that extensively here, as opposed to NumPy's random module, so that we can practice getting familiar with probability distributions as objects. In Python code: from scipy.stats import multinomial from typing import List def markov_sequence ( p_init : np . array , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. \"\"\" if p_init is None : p_init = equilibrium_distribution ( p_transition ) initial_state = list ( multinomial . rvs ( 1 , p_init )) . index ( 1 ) states = [ initial_state ] for _ in range ( sequence_length - 1 ): p_tr = p_transition [ states [ - 1 ]] new_state = list ( multinomial . rvs ( 1 , p_tr )) . index ( 1 ) states . append ( new_state ) return states With this function in hand, let's generate a sequence of length 1000. import seaborn as sns states = markov_sequence ( p_init , p_transition , sequence_length = 1000 ) fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( states ) plt . xlabel ( \"time step\" ) plt . ylabel ( \"state\" ) plt . yticks ([ 0 , 1 , 2 ]) sns . despine () As is pretty evident from the transition probabilities, once this Markov chain enters a state, it tends to maintain its current state rather than transitioning between states. If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes. If a \"Markov sequence\" feels abstract at this point, one example to help you anchor your understanding would be human motion. The three states can be \"stationary\", \"walking\", and \"running\". We transition between the three states with some probability throughout the day, moving from \"stationary\" (sitting at my desk) to \"walking\" (to get water) to \"stationary\" (because I'm pouring water), to \"walking\" (out the door) to finally \"running\" (for exercise). Emissions: When Markov chains not only produce \"states\", but also observable data So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states. A more common scenario, however, is that the states are latent , i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these Hidden Markov Models . That all sounds abstract, so let's try to make it more concrete. Gaussian Emissions: When Markov chains emit Gaussian-distributed data. With a three state model, we might say that the emissions are Gaussian distributed, but the location ( \\mu \\mu ) and scale ( \\sigma \\sigma ) vary based on which state we are in. In the simplest case: State 1 gives us data y_1 \\sim N(\\mu=1, \\sigma=0.2) y_1 \\sim N(\\mu=1, \\sigma=0.2) State 2 gives us data y_2 \\sim N(\\mu=0, \\sigma=0.5) y_2 \\sim N(\\mu=0, \\sigma=0.5) State 3 gives us data y_3 \\sim N(\\mu=-1, \\sigma=0.1) y_3 \\sim N(\\mu=-1, \\sigma=0.1) In terms of a graphical model, it would look something like this: Turns out, we can model this in Python code too! from scipy.stats import norm def gaussian_emissions ( states : List [ int ], mus : List [ float ], sigmas : List [ float ]) -> List [ float ]: emissions = [] for state in states : loc = mus [ state ] scale = sigmas [ state ] e = norm . rvs ( loc = loc , scale = scale ) emissions . append ( e ) return emissions Let's see what the emissions look like. gaussian_ems = gaussian_emissions ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ]) def plot_emissions ( states , emissions ): fig , axes = plt . subplots ( figsize = ( 16 , 8 ), nrows = 2 , ncols = 1 , sharex = True ) axes [ 0 ] . plot ( states ) axes [ 0 ] . set_title ( \"States\" ) axes [ 1 ] . plot ( emissions ) axes [ 1 ] . set_title ( \"Emissions\" ) sns . despine (); plot_emissions ( states , gaussian_ems ) Emission Distributions can be any valid distribution! Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions! Let's try Poisson emissions. Here, then, the poisson rate \\lambda \\lambda is given one per state. In our example below: State 1 gives us data y_1 \\sim Pois(\\lambda=1) y_1 \\sim Pois(\\lambda=1) State 2 gives us data y_2 \\sim Pois(\\lambda=10) y_2 \\sim Pois(\\lambda=10) State 3 gives us data y_3 \\sim Pois(\\lambda=50) y_3 \\sim Pois(\\lambda=50) from scipy.stats import poisson def poisson_emissions ( states : List [ int ], lam : List [ float ]) -> List [ int ]: emissions = [] for state in states : rate = lam [ state ] e = poisson . rvs ( rate ) emissions . append ( e ) return emissions Once again, let's observe the emissions: poisson_ems = poisson_emissions ( states , lam = [ 1 , 10 , 50 ]) plot_emissions ( states , poisson_ems ) Hope the point is made: Take your favourite distribution and use it as the emission distribution, as long as it can serve as a useful model for the data that you observe! Autoregressive Emissions Autoregressive emissions make things even more interesting and flexible! They show up, for example, when we're trying to model \"motion states\" of people or animals: that's because people and animals don't abruptly change from one state to another, but gradually transition in. The \"autoregressive\" component thus helps us model that the emission value does not only depend on the current state, but also on previous state(s), which is what motion data, for example, might look like. How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission. In terms of a generic graphical model, it is represented as follows: Heteroskedastic Autoregressive Emissions Here's a \"simple complex\" example, where the location \\mu_t \\mu_t of the emission distribution at time t t depends on y_{t-1} y_{t-1} , and the scale \\sigma \\sigma depends only on the current state s_t s_t . A place where this model might be useful is when we believe that noise is the only thing that depends on state, while the location follows a random walk. (Stock markets might be an applicable place for this.) In probabilistic notation: y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) Here, k k is a multiplicative autoregressive coefficient that scales how the previous emission affects the location \\mu \\mu of the current emission. We might also assume that the initial location \\mu=0 \\mu=0 . Because the scale \\sigma \\sigma varies with state, the emissions are called heteroskedastic , which means \"of non-constant variance\". In the example below: State 1 gives us \\sigma=0.5 \\sigma=0.5 (kind of small variance). State 2 gives us \\sigma=0.1 \\sigma=0.1 (smaller variance). State 3 gives us \\sigma=0.01 \\sigma=0.01 (very small varaince). In Python code, we would model it this way: def ar_gaussian_heteroskedastic_emissions ( states : List [ int ], k : float , sigmas : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc , scale = sigmas [ state ]) emissions . append ( e ) prev_loc = e return emissions ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Keep in mind, here, that given the way that we've defined the autoregressive heteroskedastic Gaussian HMM , it is the variance around the heteroskedastic autoregressive emissions that gives us information about the state, not the location. (To see this, notice how every time the system enters into state 2, the chain stops bouncing around much.) Contrast that against vanilla Gaussian emissions that are non-autoregressive: plot_emissions ( states , gaussian_ems ) How does the autoregressive coefficient k k affect the Markov chain emissions? As should be visible, the structure of autoregressiveness can really change how things look! What happens as k k changes? ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Interesting stuff! As k \\rightarrow 0 k \\rightarrow 0 , we approach a Gaussian centered exactly on zero, where only the variance of the observations, rather than the collective average location of the observations, give us information about the state. Homoskedastic Autoregressive Emissions What if we wanted instead the variance to remain the same, but desired instead that the emission location \\mu \\mu gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure! y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) In Python code: def ar_gaussian_homoskedastic_emissions ( states : List [ int ], k : float , mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc + mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 1 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) The variance is too small relative to the scale of the data, so it looks like smooth lines. If we change k k , however, we get interesting effects. ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 0.8 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) Notice how we get \"smoother\" transitions into each state. It's less jumpy. As mentioned earlier, this is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.) Non-Autoregressive Homoskedastic Emissions With non-autoregressive homoskedastic Gaussian emissions, the mean \\mu \\mu depends only on the hidden state at time t t , and not on the previous hidden state or the previous emission value. In equations: y_t \\sim N(\\mu=f(x_t), \\sigma) y_t \\sim N(\\mu=f(x_t), \\sigma) , where f(x_t) f(x_t) could be a simple mapping: If x_t = 1 x_t = 1 , \\mu = -10 \\mu = -10 , If x_t = 2 x_t = 2 , \\mu = 0 \\mu = 0 , If x_t = 3 x_t = 3 , \\mu = 10 \\mu = 10 . What we can see here is that the mean gives us information about the state, but the scale doesn't. def gaussian_homoskedastic_emissions ( states : List [ int ], mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions hom_ems = gaussian_homoskedastic_emissions ( states , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , hom_ems ) As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above. The Framework There's the plain old Markov Model , in which we might generate a sequence of states S S , which are generated from some initial distribution and transition matrix. p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} Graphically: Then there's the \"Hidden\" Markov Model , in which we don't observe the states but rather the emissions generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some distribution e.g., the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best. Usually, we would pick a parametric distribution both because of modelling convenience and because we think it would help us interpret our data. y_t|s_t \\sim Dist(\\theta_{t}) y_t|s_t \\sim Dist(\\theta_{t}) Where \\theta_t \\theta_t refers to the parameters for the generic distribution Dist Dist that are indexed by the state s_t s_t . (Think back to \"state 1 gives me N(-10, 1) N(-10, 1) , while state 2 gives me N(0, 1) N(0, 1) \", etc...) Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions. Graphically: Here are some special cases of the general framework. Firstly, the parameters of the emission distribution can be held constant (i.e. simple random walks). This is equivalent to when k=1 k=1 and neither \\mu \\mu nor \\sigma \\sigma depend on current state. In this case, we get back the Gaussian random walk, where y_t \\sim N(k y_{t-1}, \\sigma) y_t \\sim N(k y_{t-1}, \\sigma) ! Secondly, the distribution parameters can depend on the solely on the current state. In this case, you get back basic HMMs! If you make the variance of the likelihood distribution vary based on state, you get heteroskedastic HMMs; conversely, if you keep the variance constant, then you have homoskedastic HMMs. Moving on, there's the \"Autoregressive\" Hidden Markov Models , in which the emissions generated from the states have a dependence on the previous states' emissions (and hence, indirectly, on the previous state). Here, we have the ultimate amount of flexibility to model our processes. y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) Graphically: To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the t-1 t-1 comes from). However, arbitrary numbers of time lags are possible too! And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the Dist Dist distribution. Bonus point: your data don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the f(y_{t-1}, \\theta_t) f(y_{t-1}, \\theta_t) in a fashion that handles y y that are multidimensional, you're golden! Moreover, you can also write the function f f to be any function you like. The function f f doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay. Bayesian Inference on Markov Models Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light. If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track. Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities P(s_t | s_{t-1}) P(s_t | s_{t-1}) are given by a Multinomial distribution. The emissions are given by an arbitrary continuous (or discrete) distribution, depending on what you believe to be the likelihood distribution for the observed data. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe! (Why we use log-likelihoods instead of likelihoods is clarified here .) Markov Chain Log-Likelihood Calculation Let's examine how we would calculate the log likelihood of state data given the parameters. This will lead us to the Markov chain log-likelihood. The likelihood of a given Markov chain states is: the probability of the first state given some assumed initial distribution, times the probability of the second state given the first state, times the probability of the third state given the second state, and so on... until the end. In math notation, given the states S = \\{s_1, s_2, s_3, ..., s_n\\} S = \\{s_1, s_2, s_3, ..., s_n\\} , this becomes: L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... More explicitly, P(s_1) P(s_1) is nothing more than the probability of observing that state s_1 s_1 given an assumed initial (or equilibrium) distribution: s1 = [ 0 , 1 , 0 ] # assume we start in state 1 of {0, 1, 2} p_eq = equilibrium_distribution ( p_transition ) prob_s1 = p_eq [ s1 . index ( 1 )] prob_s1 0.27896995708154565 Then, P(s_2) P(s_2) is nothing more than the probability of observing that state s_2 s_2 given the transition matrix entry for state s_1 s_1 . # assume we enter into state 2 of {0, 1, 2} s2 = [ 0 , 0 , 1 ] transition_entry = p_transition [ s1 . index ( 1 )] prob_s2 = transition_entry [ s2 . index ( 1 )] prob_s2 0.09 Their joint likelihood is given then by prob_s1 times prob_s2 . prob_s1 * prob_s2 0.025107296137339107 And because we operate in log space to avoid underflow, we do joint log-likelihoods instead: np . log ( prob_s1 ) + np . log ( prob_s2 ) -3.6845967923219334 Let's generalize this in a math function. Since P(s_t|s_{t-1}) P(s_t|s_{t-1}) is a multinomial distribution , then if we are given the log-likelihood of \\{s_1, s_2, s_3, ..., s_n\\} \\{s_1, s_2, s_3, ..., s_n\\} , we can calculate the log-likelihood over \\{s_2,... s_n\\} \\{s_2,... s_n\\} , which is given by the sum of the log probabilities: def state_logp ( states , p_transition ): logp = 0 # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1] states_oh = np . eye ( len ( p_transition )) for curr_state , next_state in zip ( states [: - 1 ], states [ 1 :]): p_tr = p_transition [ curr_state ] logp += multinomial ( n = 1 , p = p_tr ) . logpmf ( states_oh [ next_state ]) return logp state_logp ( states , p_transition ) -418.65677519562405 We will also write a vectorized version of state_logp . def state_logp_vect ( states , p_transition ): states_oh = np . eye ( len ( p_transition )) p_tr = p_transition [ states [: - 1 ]] obs = states_oh [ states [ 1 :]] return np . sum ( multinomial ( n = 1 , p = p_tr ) . logpmf ( obs )) state_logp_vect ( states , p_transition ) -418.6567751956279 Now, there is a problem here: we also need the log likelihood of the first state. Remember that if we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy! def initial_logp ( states , p_transition ): initial_state = states [ 0 ] states_oh = np . eye ( len ( p_transition )) eq_p = equilibrium_distribution ( p_transition ) return ( multinomial ( n = 1 , p = eq_p ) . logpmf ( states_oh [ initial_state ] . squeeze ()) ) initial_logp ( states , p_transition ) array(-1.16057901) Taken together, we get the following Markov chain log-likelihood: def markov_state_logp ( states , p_transition ): return ( state_logp_vect ( states , p_transition ) + initial_logp ( states , p_transition ) ) markov_state_logp ( states , p_transition ) -419.81735420804523 Markov Chain with Gaussian Emissions Log-Likelihood Calculation Now that we know how to calculate the log-likelihood for the Markov chain sequence of states, we can move on to the log-likelihood calculation for the emissions. Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood. For the benefit of those who need it written out explicitly, here's the for-loop version: def gaussian_logp ( states , mus , sigmas , emissions ): logp = 0 for ( emission , state ) in zip ( emissions , states ): logp += norm ( mus [ state ], sigmas [ state ]) . logpdf ( emission ) return logp gaussian_logp ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ], emissions = gaussian_ems ) 250.57996114495296 And we'll also make a vectorized version of it: def gaussian_logp_vect ( states , mus , sigmas , emissions ): mu = mus [ states ] sigma = sigmas [ states ] return np . sum ( norm ( mu , sigma ) . logpdf ( emissions )) gaussian_logp_vect ( states , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) 250.5799611449528 The joint log likelihood of the emissions and states are then given by their summation. def gaussian_emission_hmm_logp ( states , p_transition , mus , sigmas , emissions ): return markov_state_logp ( states , p_transition ) + gaussian_logp_vect ( states , mus , sigmas , emissions ) gaussian_emission_hmm_logp ( states , p_transition , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) -169.23739306309244 If you're in a Binder or local Jupyter session, go ahead and tweak the values of mus and sigmas , and verify for yourself that the current values are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters! Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be. As an example, I will be using the Gaussian with: State-varying scale Mean that is dependent on the previously emitted value This is the AR-HMM with data generated from the ar_gaussian_heteroskedastic_emissions function. def ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ): logp = 0 initial_state = states [ 0 ] initial_emission_logp = norm ( 0 , sigmas [ initial_state ]) . logpdf ( emissions [ 0 ]) for previous_emission , current_emission , state in zip ( emissions [: - 1 ], emissions [ 1 :], states [ 1 :]): loc = k * previous_emission scale = sigmas [ state ] logp += norm ( loc , scale ) . logpdf ( current_emission ) return logp ar_gaussian_heteroskedastic_emissions_logp ( states , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -18605.714303907385 Now, we can write the full log likelihood of the entire AR-HMM: def ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k , sigmas , emissions ): return ( markov_state_logp ( states , p_transition ) + ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ) ) ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -19025.53165811543 For those of you who are familiar with Bayesian inference, as soon as we have a joint log likelihood that we can calculate between our model priors and data, using the simple Bayes' rule equation, we can obtain posterior distributions easily through an MCMC sampler. If this looks all foreign to you, then check out my other essay for a first look (or a refresher)! HMM Distributions in PyMC3 While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user @hstrey posting a Jupyter notebook with HMMs defined in there , many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included. Side note: I used @hstrey's implementation before setting out to write this essay. Thanks! The key thing to notice in this section is how the logp functions are defined . They will match the log probability functions that we have defined above, except written in Theano. HMM States Distribution Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states. import pymc3 as pm import theano.tensor as tt import theano.tensor.slinalg as sla # theano-wrapped scipy linear algebra import theano.tensor.nlinalg as nla # theano-wrapped numpy linear algebra import theano theano . config . gcc . cxxflags = \"-Wno-c++11-narrowing\" class HMMStates ( pm . Categorical ): def __init__ ( self , p_transition , p_equilibrium , n_states , * args , ** kwargs ): \"\"\"You can ignore this section for the time being.\"\"\" super ( pm . Categorical , self ) . __init__ ( * args , ** kwargs ) self . p_transition = p_transition self . p_equilibrium = p_equilibrium # This is needed self . k = n_states # This is only needed because discrete distributions must define a mode. self . mode = tt . cast ( 0 , dtype = 'int64' ) def logp ( self , x ): \"\"\"Focus your attention here!\"\"\" p_eq = self . p_equilibrium # Broadcast out the transition probabilities, # so that we can broadcast the calculation # of log-likelihoods p_tr = self . p_transition [ x [: - 1 ]] # the logp of the initial state evaluated against the equilibrium probabilities initial_state_logp = pm . Categorical . dist ( p_eq ) . logp ( x [ 0 ]) # the logp of the rest of the states. x_i = x [ 1 :] ou_like = pm . Categorical . dist ( p_tr ) . logp ( x_i ) transition_logp = tt . sum ( ou_like ) return initial_state_logp + transition_logp Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation. The categorical distribution is the generalization of the multinomial distribution, but unfortunately, it isn't implemented in the SciPy stats library, which is why we used the multinomial earlier on. Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions. def solve_equilibrium ( n_states , p_transition ): A = tt . dmatrix ( 'A' ) A = tt . eye ( n_states ) - p_transition + tt . ones ( shape = ( n_states , n_states )) p_equilibrium = pm . Deterministic ( \"p_equilibrium\" , sla . solve ( A . T , tt . ones ( shape = ( n_states )))) return p_equilibrium import warnings warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) n_states = 3 with pm . Model () as model : p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )) * 4 , # weakly informative prior shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) obs_states = HMMStates ( \"states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , observed = np . array ( states ) . astype ( \"float\" ) ) Now let's fit the model! with model : trace = pm . sample ( 2000 ) Auto - assigning NUTS sampler ... Initializing NUTS using jitter + adapt_diag ... Multiprocess sampling ( 4 chains in 4 jobs ) NUTS : [ p_transition ] Sampling 4 chains , 0 divergences : 0 %| | 0 / 10000 [ 00:00<?, ?draws/s ]/ home / ericmjl / anaconda / envs / bayesian - analysis - recipes / lib / python3 .8 / site - packages / theano / tensor / slinalg . py : 255 : LinAlgWarning : Ill - conditioned matrix ( rcond = 5.89311e-08 ) : result may not be accurate . rval = scipy . linalg . solve ( A , b ) / home / ericmjl / anaconda / envs / bayesian - analysis - recipes / lib / python3 .8 / site - packages / theano / tensor / slinalg . py : 255 : LinAlgWarning : Ill - conditioned matrix ( rcond = 5.89311e-08 ) : result may not be accurate . rval = scipy . linalg . solve ( A , b ) Sampling 4 chains , 0 divergences : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10000 / 10000 [ 00:08<00:00, 1192.11draws/s ] import arviz as az az . plot_forest ( trace , var_names = [ \"p_transition\" ]); It looks like we were able to recover the original transitions! HMM with Gaussian Emissions Let's try out now an HMM model with Gaussian emissions. class HMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , mu , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states # self.rate = rate self . mu = mu self . sigma = sigma def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states # rate = self.rate[states] # broadcast the rate across the states. mu = self . mu [ states ] sigma = self . sigma [ states ] return tt . sum ( pm . Normal . dist ( mu = mu , sigma = sigma ) . logp ( x )) n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( gaussian_ems ),) ) # Prior for mu and sigma mu = pm . Normal ( \"mu\" , mu = 0 , sigma = 1 , shape = ( n_states ,)) sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) # Observed emission likelihood obs = HMMGaussianEmissions ( \"emission\" , states = hmm_states , mu = mu , sigma = sigma , observed = gaussian_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling ( 4 chains in 4 jobs ) CompoundStep > NUTS : [ sigma, mu, p_transition ] > CategoricalGibbsMetropolis : [ hmm_states ] Sampling 4 chains , 0 divergences : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10000 / 10000 [ 11:59<00:00, 13.90draws/s ] The rhat statistic is larger than 1.4 for some parameters . The sampler did not converge . The estimated number of effective samples is smaller than 200 for some parameters . az . plot_trace ( trace , var_names = [ \"mu\" ]); az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); We are able to recover the parameters, but there is significant intra-chain homogeneity. That is fine, though one way to get around this is to explicitly instantiate prior distributions for each of the parameters instead. Autoregressive HMMs with Gaussian Emissions Let's now add in the autoregressive component to it. The data we will use is the ar_het_ems data, which were generated by using a heteroskedastic assumption, with Gaussian emissions whose mean depends on the previous value, while variance depends on state. As a reminder of what the data look like: ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0.6 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Let's now define the AR-HMM. class ARHMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , k , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states self . sigma = sigma # variance self . k = k # autoregressive coefficient. def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states sigma = self . sigma [ states ] k = self . k ar_mean = k * x [: - 1 ] ar_like = tt . sum ( pm . Normal . dist ( mu = ar_mean , sigma = sigma [ 1 :]) . logp ( x [ 1 :])) boundary_like = pm . Normal . dist ( mu = 0 , sigma = sigma [ 0 ]) . logp ( x [ 0 ]) return ar_like + boundary_like n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( ar_het_ems ),) ) # Prior for sigma and k sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) k = pm . Beta ( \"k\" , alpha = 2 , beta = 2 ) # a not-so-weak prior for k # Observed emission likelihood obs = ARHMMGaussianEmissions ( \"emission\" , states = hmm_states , sigma = sigma , k = k , observed = ar_het_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [k, sigma, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 6 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [12:34<00:00, 13.26draws/s] The acceptance probability does not match the target. It is 0.9096431867898114, but should be close to 0.8. Try to increase the number of tuning steps. There were 6 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Let's now take a look at the key parameters we might be interested in estimating: k k : the autoregressive coefficient, or how much previous emissions influence current emissions. \\sigma \\sigma : the variance that belongs to each state. az . plot_forest ( trace , var_names = [ \"k\" ]); az . plot_trace ( trace , var_names = [ \"k\" ]); It looks like we were able to obtain the value of k k correctly! az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); It also looks like we were able to obtain the correct sigma values too, except that the chains are mixed up. We would do well to take care when calculating means for each parameter on the basis of chains. How about the chain states? Did we get them right? fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( np . round ( trace [ \"hmm_states\" ] . mean ( axis = 0 )), label = \"true\" ) plt . plot ( 2 - np . array ( states ), label = \"inferred\" ) plt . legend (); I had to flip the states because they were backwards relative to the original. Qualitatively, not bad! If we wanted to be a bit more rigorous, we would quantify the accuracy of state identification. If the transition probabilities were a bit more extreme, we might have an easier time with the identifiability of the states. As it stands, because the variance is the only thing that changes, and because the variance of two of the three states are quite similar (one is 0.1 and the other is 0.5), distinguishing between these two states may be more difficult. Concluding Notes Nothing in statistics makes sense... ...unless in light of a \"data generating model\". I initially struggled with the math behind HMMs and its variants, because I had never taken the time to think through the \"data generating process\" carefully. Once we have the data generating process, and in particular, its structure , it becomes trivial to map the structure of the model to the equations that are needed to model it. (I think this is why physicists are such good Bayesians: they are well-trained at thinking about mechanistic, data generating models.) For example, with autoregressive HMMs, until I sat down and thought through the data generating process step-by-step, nothing made sense. Once I wrote out how the mean of the previous observation influenced the mean of the current observation, then things made a ton of sense. In fact, now that I look back on my learning journey in Bayesian statistics, if we can define a likelihood function for our data, we can trivially work backwards and design a data generating process. Model structure is important While writing out the PyMC3 implementations and conditioning them on data, I remember times when I mismatched the model to the data, thus generating posterior samples that exhibited pathologies: divergences and more. This is a reminder that getting the structure of the model is very important. Keep learning I hope this essay was useful for your learning journey as well. If you enjoyed it, please take a moment to star the repository ! Acknowledgements I would like to acknowledge the following colleagues and friends who have helped review the notebook. My colleagues, Zachary Barry and Balaji Goparaju, both of whom pointed out unclear phrasings in my prose and did some code review. Fellow PyMC developers, Colin Carroll (from whom I never cease to learn things), Alex Andorra (who also did code review), Junpeng Lao, Ravin Kumar, and Osvaldo Martin (also for their comments), Professor Allen Downey (of the Olin College of Engineering) who provided important pedagogical comments throughout the notebook. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Markov Models From The Bottom Up, with Python"},{"location":"machine-learning/markov-models/#markov-models-from-the-bottom-up-with-python","text":"Markov models are a useful class of models for sequential-type of data. Before recurrent neural networks (which can be thought of as an upgraded Markov model) came along, Markov Models and their variants were the in thing for processing time series and biological data. Just recently, I was involved in a project with a colleague, Zach Barry, where we thought the use of autoregressive hidden Markov models (AR-HMMs) might be a useful thing. Apart from our hack session one afternoon, it set off a series of self-study that culminated in this essay. By writing this down for my own memory, my hope is that it gives you a resource to refer back to as well. You'll notice that I don't talk about inference (i.e. inferring parameters from data) until the end: this is intentional. As I've learned over the years doing statistical modelling and machine learning, nothing makes sense without first becoming deeply familiar with the \"generative\" story of each model, i.e. the algorithmic steps that let us generate data. It's a very Bayesian-influenced way of thinking that I hope you will become familiar with too.","title":"Markov Models From The Bottom Up, with Python"},{"location":"machine-learning/markov-models/#markov-models-what-they-are-with-mostly-plain-english-and-some-math","text":"The simplest Markov models assume that we have a system that contains a finite set of states, and that the system transitions between these states with some probability at each time step t t , thus generating a sequence of states over time. Let's call these states S S , where S = \\{s_1, s_2, ..., s_n\\} S = \\{s_1, s_2, ..., s_n\\} To keep things simple, let's start with three states: S = \\{s_1, s_2, s_3\\} S = \\{s_1, s_2, s_3\\} A Markov model generates a sequence of states, with one possible realization being: \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} \\{s_1, s_1, s_1, s_3, s_3, s_3, s_2, s_2, s_3, s_3, s_3, s_3, s_1, ...\\} And generically, we represent it as a sequence of states x_t, x_{t+1}... x_{t+n} x_t, x_{t+1}... x_{t+n} . (We have chosen a different symbol to not confuse the \"generic\" state with the specific realization. Graphically, a plain and simple Markov model looks like the following:","title":"Markov Models: What they are, with mostly plain English and some math"},{"location":"machine-learning/markov-models/#initializing-a-markov-chain","text":"Every Markov chain needs to be initialized. To do so, we need an initial state probability vector , which tells us what the distribution of initial states will be. Let's call the vector p_S p_S , where the subscript S S indicates that it is for the \"states\". p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_{init} = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} Semantically, they allocate the probabilities of starting the sequence at a given state. For example, we might assume a discrete uniform distribution, which in Python would look like: import numpy as np p_init = np . array ([ 1 / 3. , 1 / 3. , 1 / 3. ]) Alternatively, we might assume a fixed starting point, which can be expressed as the p_S p_S array: p_init = np . array ([ 0 , 1 , 0 ]) Alternatively, we might assign non-zero probabilities to each in a non-uniform fashion: # State 0: 0.1 probability # State 1: 0.8 probability # State 2: 0.1 probability p_init = np . array ([ 0.1 , 0.8 , 0.1 ]) Finally, we might assume that the system was long-running before we started observing the sequence of states, and as such the initial state was drawn as one realization of some equilibrated distribution of states. Keep this idea in your head, as we'll need it later. For now, just to keep things concrete, let's specify an initial distribution as a non-uniform probability vector. import numpy as np p_init = np . array ([ 0.1 , 0.8 , 0.1 ])","title":"Initializing a Markov chain"},{"location":"machine-learning/markov-models/#modelling-transitions-between-states","text":"To know how a system transitions between states, we now need a transition matrix . The transition matrix describes the probability of transitioning from one state to another. (The probability of staying in the same state is semantically equivalent to transitioning to the same state.) By convention, transition matrix rows correspond to the state at time t t , while columns correspond to state at time t+1 t+1 . Hence, row probabilities sum to one, because the probability of transitioning to the next state depends on only the current state, and all possible states are known and enumerated. Let's call the transition matrix P_{transition} P_{transition} . The symbol etymology, which usually gets swept under the rug in mathematically-oriented papers, are as follows: transition transition doesn't refer to time but simply indicates that it is for transitioning states, P P is used because it is a probability matrix. P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} P_{transition} = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} Using the transition matrix, we can express that the system likes to stay in the state that it enters into, by assigning larger probability mass to the diagonals. Alternatively, we can express that the system likes to transition out of states that it enters into, by assigning larger probability mass to the off-diagonal. Alrighty, enough with that now, let's initialize a transition matrix below. p_transition = np . array ( [[ 0.90 , 0.05 , 0.05 ], [ 0.01 , 0.90 , 0.09 ], [ 0.07 , 0.03 , 0.9 ]] ) p_transition array([[0.9 , 0.05, 0.05], [0.01, 0.9 , 0.09], [0.07, 0.03, 0.9 ]]) And just to confirm with you that each row sums to one: assert p_transition [ 0 , :] . sum () == 1 assert p_transition [ 1 , :] . sum () == 1 assert p_transition [ 2 , :] . sum () == 1","title":"Modelling transitions between states"},{"location":"machine-learning/markov-models/#equilibrium-or-stationary-distribution","text":"Now, do you remember how above we talked about the Markov chain being in some \"equilibrated\" state? Well, the stationary or equilibrium distribution of a Markov chain is the distribution of observed states at infinite time. An interesting property is that regardless of what the initial state is, the equilibrium distribution will always be the same, as the equilibrium distribution only depends on the transition matrix. Here's how to think about the equilibrium: if you were to imagine instantiating a thousand Markov chains using the initial distribution p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} p_{init} = \\begin{pmatrix} 0.1 & 0.8 & 0.1 \\end{pmatrix} 10% would start out in state 1 80% would start out in state 2 10% would start out in state 3 However, if you ran each of the systems to a large number of time steps (say, 1 million time steps, to exaggerate the point) then how the states were distributed initially wouldn't matter, as how they transition from time step to time step begins to dominate. We could simulate this explicitly in Python, but as it turns out, there is a mathematical shortcut that involves simple dot products. Let's check it out. Assume we have an initial state and a transition matrix. We're going to reuse p_init from above, but use a different p_transition to make the equilibrium distribution values distinct. This will make it easier for us to plot later. p_transition_example = np . array ( [[ 0.6 , 0.2 , 0.2 ], [ 0.05 , 0.9 , 0.05 ], [ 0.1 , 0.2 , 0.7 ]] ) To simulate the distribution of states in the next time step, we take the initial distribution p_init and matrix multiply it against the transition matrix. p_next = p_init @ p_transition_example p_next array([0.11, 0.76, 0.13]) We can do it again to simulate the distribution of states in the next time step after: p_next = p_next @ p_transition_example p_next array([0.117, 0.732, 0.151]) Let's now write a for-loop to automate the process. p_state_t = [ p_init ] for i in range ( 200 ): # 200 time steps sorta, kinda, approximates infinite time :) p_state_t . append ( p_state_t [ - 1 ] @ p_transition_example ) To make it easier for you to see what we've generated, let's make the p_state_t list into a pandas DataFrame. import pandas as pd state_distributions = pd . DataFrame ( p_state_t ) state_distributions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 0 0.100000 0.800000 0.10000 1 0.110000 0.760000 0.13000 2 0.117000 0.732000 0.15100 3 0.121900 0.712400 0.16570 4 0.125330 0.698680 0.17599 ... ... ... ... 196 0.133333 0.666667 0.20000 197 0.133333 0.666667 0.20000 198 0.133333 0.666667 0.20000 199 0.133333 0.666667 0.20000 200 0.133333 0.666667 0.20000 201 rows \u00d7 3 columns Now, let's plot what the distributions look like. import matplotlib.pyplot as plt state_distributions . plot (); If you're viewing this notebook on Binder or locally, go ahead and modify the initial state to convince yourself that it doesn't matter what the initial state will be: the equilibrium state distribution, which is the fraction of time the Markov chain is in that state over infinite time, will always be the same as long as the transition matrix stays the same. print ( p_state_t [ - 1 ]) [0.13333333 0.66666667 0.2 ] As it turns out, there's also a way to solve for the equilibrium distribution analytically from the transition matrix. This involves solving a linear algebra problem, which we can do using Python. (Credit goes to this blog post from which I modified the code to fit the variable naming here.) def equilibrium_distribution ( p_transition ): n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) b = np . transpose ( np . array ([ 0 ] * n_states + [ 1 ])) p_eq = np . linalg . solve ( a = np . transpose ( A ) . dot ( A ), b = np . transpose ( A ) . dot ( b ) ) return p_eq # alternative def equilibrium_distribution ( p_transition ): \"\"\"This implementation comes from Colin Carroll, who kindly reviewed the notebook\"\"\" n_states = p_transition . shape [ 0 ] A = np . append ( arr = p_transition . T - np . eye ( n_states ), values = np . ones ( n_states ) . reshape ( 1 , - 1 ), axis = 0 ) # Moore-Penrose pseudoinverse = (A^TA)^{-1}A^T pinv = np . linalg . pinv ( A ) # Return last row return pinv . T [ - 1 ] print ( equilibrium_distribution ( p_transition_example )) [0.13333333 0.66666667 0.2 ]","title":"Equilibrium or Stationary Distribution"},{"location":"machine-learning/markov-models/#generating-a-markov-sequence","text":"Generating a Markov sequence means we \"forward\" simulate the chain by: (1) Optionally drawing an initial state from p_S p_S (let's call that s_{t} s_{t} ). This is done by drawing from a multinomial distribution: s_t \\sim Multinomial(1, p_S) s_t \\sim Multinomial(1, p_S) If we assume (and keep in mind that we don't have to) that the system was equilibrated before we started observing its state sequence, then the initial state distribution is equivalent to the equilibrium distribution. All this means that we don't necessarily have to specify the initial distribution explicitly. (2) Drawing the next state by indexing into the transition matrix p_T p_T , and drawing a new state based on the Multinomial distribution: s_{t+1} \\sim Multinomial(1, p_{T_i}) s_{t+1} \\sim Multinomial(1, p_{T_i}) where i i is the index of the state. I previously wrote about what probability distributions are , leveraging the SciPy probability distributions library. We're going to use that extensively here, as opposed to NumPy's random module, so that we can practice getting familiar with probability distributions as objects. In Python code: from scipy.stats import multinomial from typing import List def markov_sequence ( p_init : np . array , p_transition : np . array , sequence_length : int ) -> List [ int ]: \"\"\" Generate a Markov sequence based on p_init and p_transition. \"\"\" if p_init is None : p_init = equilibrium_distribution ( p_transition ) initial_state = list ( multinomial . rvs ( 1 , p_init )) . index ( 1 ) states = [ initial_state ] for _ in range ( sequence_length - 1 ): p_tr = p_transition [ states [ - 1 ]] new_state = list ( multinomial . rvs ( 1 , p_tr )) . index ( 1 ) states . append ( new_state ) return states With this function in hand, let's generate a sequence of length 1000. import seaborn as sns states = markov_sequence ( p_init , p_transition , sequence_length = 1000 ) fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( states ) plt . xlabel ( \"time step\" ) plt . ylabel ( \"state\" ) plt . yticks ([ 0 , 1 , 2 ]) sns . despine () As is pretty evident from the transition probabilities, once this Markov chain enters a state, it tends to maintain its current state rather than transitioning between states. If you've opened up this notebook in Binder or locally, feel free to modify the transition probabilities and initial state probabilities above to see how the Markov sequence changes. If a \"Markov sequence\" feels abstract at this point, one example to help you anchor your understanding would be human motion. The three states can be \"stationary\", \"walking\", and \"running\". We transition between the three states with some probability throughout the day, moving from \"stationary\" (sitting at my desk) to \"walking\" (to get water) to \"stationary\" (because I'm pouring water), to \"walking\" (out the door) to finally \"running\" (for exercise).","title":"Generating a Markov Sequence"},{"location":"machine-learning/markov-models/#emissions-when-markov-chains-not-only-produce-states-but-also-observable-data","text":"So as you've seen above, a Markov chain can produce \"states\". If we are given direct access to the \"states\", then a problem that we may have is inferring the transition probabilities given the states. A more common scenario, however, is that the states are latent , i.e. we cannot directly observe them. Instead, the latent states generate data that are given by some distribution conditioned on the state. We call these Hidden Markov Models . That all sounds abstract, so let's try to make it more concrete.","title":"Emissions: When Markov chains not only produce \"states\", but also observable data"},{"location":"machine-learning/markov-models/#gaussian-emissions-when-markov-chains-emit-gaussian-distributed-data","text":"With a three state model, we might say that the emissions are Gaussian distributed, but the location ( \\mu \\mu ) and scale ( \\sigma \\sigma ) vary based on which state we are in. In the simplest case: State 1 gives us data y_1 \\sim N(\\mu=1, \\sigma=0.2) y_1 \\sim N(\\mu=1, \\sigma=0.2) State 2 gives us data y_2 \\sim N(\\mu=0, \\sigma=0.5) y_2 \\sim N(\\mu=0, \\sigma=0.5) State 3 gives us data y_3 \\sim N(\\mu=-1, \\sigma=0.1) y_3 \\sim N(\\mu=-1, \\sigma=0.1) In terms of a graphical model, it would look something like this: Turns out, we can model this in Python code too! from scipy.stats import norm def gaussian_emissions ( states : List [ int ], mus : List [ float ], sigmas : List [ float ]) -> List [ float ]: emissions = [] for state in states : loc = mus [ state ] scale = sigmas [ state ] e = norm . rvs ( loc = loc , scale = scale ) emissions . append ( e ) return emissions Let's see what the emissions look like. gaussian_ems = gaussian_emissions ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ]) def plot_emissions ( states , emissions ): fig , axes = plt . subplots ( figsize = ( 16 , 8 ), nrows = 2 , ncols = 1 , sharex = True ) axes [ 0 ] . plot ( states ) axes [ 0 ] . set_title ( \"States\" ) axes [ 1 ] . plot ( emissions ) axes [ 1 ] . set_title ( \"Emissions\" ) sns . despine (); plot_emissions ( states , gaussian_ems )","title":"Gaussian Emissions: When Markov chains emit Gaussian-distributed data."},{"location":"machine-learning/markov-models/#emission-distributions-can-be-any-valid-distribution","text":"Nobody said we have to use Gaussian distributions for emissions; we can, in fact, have a ton of fun and start simulating data using other distributions! Let's try Poisson emissions. Here, then, the poisson rate \\lambda \\lambda is given one per state. In our example below: State 1 gives us data y_1 \\sim Pois(\\lambda=1) y_1 \\sim Pois(\\lambda=1) State 2 gives us data y_2 \\sim Pois(\\lambda=10) y_2 \\sim Pois(\\lambda=10) State 3 gives us data y_3 \\sim Pois(\\lambda=50) y_3 \\sim Pois(\\lambda=50) from scipy.stats import poisson def poisson_emissions ( states : List [ int ], lam : List [ float ]) -> List [ int ]: emissions = [] for state in states : rate = lam [ state ] e = poisson . rvs ( rate ) emissions . append ( e ) return emissions Once again, let's observe the emissions: poisson_ems = poisson_emissions ( states , lam = [ 1 , 10 , 50 ]) plot_emissions ( states , poisson_ems ) Hope the point is made: Take your favourite distribution and use it as the emission distribution, as long as it can serve as a useful model for the data that you observe!","title":"Emission Distributions can be any valid distribution!"},{"location":"machine-learning/markov-models/#autoregressive-emissions","text":"Autoregressive emissions make things even more interesting and flexible! They show up, for example, when we're trying to model \"motion states\" of people or animals: that's because people and animals don't abruptly change from one state to another, but gradually transition in. The \"autoregressive\" component thus helps us model that the emission value does not only depend on the current state, but also on previous state(s), which is what motion data, for example, might look like. How, though, can we enforce this dependency structure? Well, as implied by the term \"structure\", it means we have some set of equations that relate the parameters of the emission distribution to the value of the previous emission. In terms of a generic graphical model, it is represented as follows:","title":"Autoregressive Emissions"},{"location":"machine-learning/markov-models/#heteroskedastic-autoregressive-emissions","text":"Here's a \"simple complex\" example, where the location \\mu_t \\mu_t of the emission distribution at time t t depends on y_{t-1} y_{t-1} , and the scale \\sigma \\sigma depends only on the current state s_t s_t . A place where this model might be useful is when we believe that noise is the only thing that depends on state, while the location follows a random walk. (Stock markets might be an applicable place for this.) In probabilistic notation: y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) y_t \\sim N(\\mu=k y_{t-1}, \\sigma=\\sigma_{s_t}) Here, k k is a multiplicative autoregressive coefficient that scales how the previous emission affects the location \\mu \\mu of the current emission. We might also assume that the initial location \\mu=0 \\mu=0 . Because the scale \\sigma \\sigma varies with state, the emissions are called heteroskedastic , which means \"of non-constant variance\". In the example below: State 1 gives us \\sigma=0.5 \\sigma=0.5 (kind of small variance). State 2 gives us \\sigma=0.1 \\sigma=0.1 (smaller variance). State 3 gives us \\sigma=0.01 \\sigma=0.01 (very small varaince). In Python code, we would model it this way: def ar_gaussian_heteroskedastic_emissions ( states : List [ int ], k : float , sigmas : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc , scale = sigmas [ state ]) emissions . append ( e ) prev_loc = e return emissions ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Keep in mind, here, that given the way that we've defined the autoregressive heteroskedastic Gaussian HMM , it is the variance around the heteroskedastic autoregressive emissions that gives us information about the state, not the location. (To see this, notice how every time the system enters into state 2, the chain stops bouncing around much.) Contrast that against vanilla Gaussian emissions that are non-autoregressive: plot_emissions ( states , gaussian_ems )","title":"Heteroskedastic Autoregressive Emissions"},{"location":"machine-learning/markov-models/#how-does-the-autoregressive-coefficient-kk-affect-the-markov-chain-emissions","text":"As should be visible, the structure of autoregressiveness can really change how things look! What happens as k k changes? ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 1 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Interesting stuff! As k \\rightarrow 0 k \\rightarrow 0 , we approach a Gaussian centered exactly on zero, where only the variance of the observations, rather than the collective average location of the observations, give us information about the state.","title":"How does the autoregressive coefficient kk affect the Markov chain emissions?"},{"location":"machine-learning/markov-models/#homoskedastic-autoregressive-emissions","text":"What if we wanted instead the variance to remain the same, but desired instead that the emission location \\mu \\mu gives us information about the state while still being autoregressive? Well, we can bake that into the equation structure! y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) y_t \\sim N(\\mu=k y_{t-1} + \\mu_{s_t}, \\sigma=1) In Python code: def ar_gaussian_homoskedastic_emissions ( states : List [ int ], k : float , mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = k * prev_loc + mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 1 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) The variance is too small relative to the scale of the data, so it looks like smooth lines. If we change k k , however, we get interesting effects. ar_hom_ems = ar_gaussian_homoskedastic_emissions ( states , k = 0.8 , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , ar_hom_ems ) Notice how we get \"smoother\" transitions into each state. It's less jumpy. As mentioned earlier, this is extremely useful for modelling motion activity, for example, where people move into and out of states without having jumpy-switching. (We don't go from sitting to standing to walking by jumping frames, we ease into each.)","title":"Homoskedastic Autoregressive Emissions"},{"location":"machine-learning/markov-models/#non-autoregressive-homoskedastic-emissions","text":"With non-autoregressive homoskedastic Gaussian emissions, the mean \\mu \\mu depends only on the hidden state at time t t , and not on the previous hidden state or the previous emission value. In equations: y_t \\sim N(\\mu=f(x_t), \\sigma) y_t \\sim N(\\mu=f(x_t), \\sigma) , where f(x_t) f(x_t) could be a simple mapping: If x_t = 1 x_t = 1 , \\mu = -10 \\mu = -10 , If x_t = 2 x_t = 2 , \\mu = 0 \\mu = 0 , If x_t = 3 x_t = 3 , \\mu = 10 \\mu = 10 . What we can see here is that the mean gives us information about the state, but the scale doesn't. def gaussian_homoskedastic_emissions ( states : List [ int ], mus : List [ float ]) -> List [ float ]: emissions = [] prev_loc = 0 for state in states : e = norm . rvs ( loc = mus [ state ], scale = 1 ) emissions . append ( e ) prev_loc = e return emissions hom_ems = gaussian_homoskedastic_emissions ( states , mus = [ - 10 , 0 , 10 ]) plot_emissions ( states , hom_ems ) As you might intuit from looking at the equations, this is nothing more than a special case of the Heteroskedastic Gaussian Emissions example shown much earlier above.","title":"Non-Autoregressive Homoskedastic Emissions"},{"location":"machine-learning/markov-models/#the-framework","text":"There's the plain old Markov Model , in which we might generate a sequence of states S S , which are generated from some initial distribution and transition matrix. p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_S = \\begin{pmatrix} p_1 & p_2 & p_3 \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} p_T = \\begin{pmatrix} p_{11} & p_{12} & p_{13}\\\\ p_{21} & p_{22} & p_{23}\\\\ p_{31} & p_{32} & p_{33}\\\\ \\end{pmatrix} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} S = \\{s_t, s_{t+1}, ... s_{t+n}\\} Graphically: Then there's the \"Hidden\" Markov Model , in which we don't observe the states but rather the emissions generated from the states (according to some assumed distribution). Now, there's not only the initial distribution and transition matrix to worry about, but also the distribution of the emissions conditioned on the state. The general case is when we have some distribution e.g., the Gaussian or the Poisson or the Chi-Squared - whichever fits the likelihood of your data best. Usually, we would pick a parametric distribution both because of modelling convenience and because we think it would help us interpret our data. y_t|s_t \\sim Dist(\\theta_{t}) y_t|s_t \\sim Dist(\\theta_{t}) Where \\theta_t \\theta_t refers to the parameters for the generic distribution Dist Dist that are indexed by the state s_t s_t . (Think back to \"state 1 gives me N(-10, 1) N(-10, 1) , while state 2 gives me N(0, 1) N(0, 1) \", etc...) Your distributions probably generally come from the same family (e.g. \"Gaussians\"), or you can go super complicated and generate them from different distributions. Graphically: Here are some special cases of the general framework. Firstly, the parameters of the emission distribution can be held constant (i.e. simple random walks). This is equivalent to when k=1 k=1 and neither \\mu \\mu nor \\sigma \\sigma depend on current state. In this case, we get back the Gaussian random walk, where y_t \\sim N(k y_{t-1}, \\sigma) y_t \\sim N(k y_{t-1}, \\sigma) ! Secondly, the distribution parameters can depend on the solely on the current state. In this case, you get back basic HMMs! If you make the variance of the likelihood distribution vary based on state, you get heteroskedastic HMMs; conversely, if you keep the variance constant, then you have homoskedastic HMMs. Moving on, there's the \"Autoregressive\" Hidden Markov Models , in which the emissions generated from the states have a dependence on the previous states' emissions (and hence, indirectly, on the previous state). Here, we have the ultimate amount of flexibility to model our processes. y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) y_t|s_t \\sim Dist(f(y_{t-1}, \\theta_t)) Graphically: To keep things simple in this essay, we've only considered the case of lag of 1 (which is where the t-1 t-1 comes from). However, arbitrary numbers of time lags are possible too! And, as usual, you can make them homoskedastic or heteroskedastic by simply controlling the variance parameter of the Dist Dist distribution. Bonus point: your data don't necessarily have to be single dimensional; they can be multidimensional too! As long as you write the f(y_{t-1}, \\theta_t) f(y_{t-1}, \\theta_t) in a fashion that handles y y that are multidimensional, you're golden! Moreover, you can also write the function f f to be any function you like. The function f f doesn't have to be a linear function (like we did); it can instead be a neural network if you so choose, thus giving you a natural progression from Markov models to Recurrent Neural Networks. That, however, is out of scope for this essay.","title":"The Framework"},{"location":"machine-learning/markov-models/#bayesian-inference-on-markov-models","text":"Now that we've gone through the \"data generating process\" for Markov sequences with emissions, we can re-examine the entire class of models in a Bayesian light. If you've been observing the models that we've been \"forward-simulating\" all this while to generate data, you'll notice that there are a few key parameters that seemed like, \"well, if we changed them, then the data would change, right?\" If that's what you've been thinking, then bingo! You're on the right track. Moreover, you'll notice that I've couched everything in the language of probability distributions. The transition probabilities P(s_t | s_{t-1}) P(s_t | s_{t-1}) are given by a Multinomial distribution. The emissions are given by an arbitrary continuous (or discrete) distribution, depending on what you believe to be the likelihood distribution for the observed data. Given that we're working with probability distributions and data, you probably have been thinking about it already: we need a way to calculate the log-likelihoods of the data that we observe! (Why we use log-likelihoods instead of likelihoods is clarified here .)","title":"Bayesian Inference on Markov Models"},{"location":"machine-learning/markov-models/#markov-chain-log-likelihood-calculation","text":"Let's examine how we would calculate the log likelihood of state data given the parameters. This will lead us to the Markov chain log-likelihood. The likelihood of a given Markov chain states is: the probability of the first state given some assumed initial distribution, times the probability of the second state given the first state, times the probability of the third state given the second state, and so on... until the end. In math notation, given the states S = \\{s_1, s_2, s_3, ..., s_n\\} S = \\{s_1, s_2, s_3, ..., s_n\\} , this becomes: L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... L(S) = P(s_1) P(s_2|s_1) P(s_3|s_2) ... More explicitly, P(s_1) P(s_1) is nothing more than the probability of observing that state s_1 s_1 given an assumed initial (or equilibrium) distribution: s1 = [ 0 , 1 , 0 ] # assume we start in state 1 of {0, 1, 2} p_eq = equilibrium_distribution ( p_transition ) prob_s1 = p_eq [ s1 . index ( 1 )] prob_s1 0.27896995708154565 Then, P(s_2) P(s_2) is nothing more than the probability of observing that state s_2 s_2 given the transition matrix entry for state s_1 s_1 . # assume we enter into state 2 of {0, 1, 2} s2 = [ 0 , 0 , 1 ] transition_entry = p_transition [ s1 . index ( 1 )] prob_s2 = transition_entry [ s2 . index ( 1 )] prob_s2 0.09 Their joint likelihood is given then by prob_s1 times prob_s2 . prob_s1 * prob_s2 0.025107296137339107 And because we operate in log space to avoid underflow, we do joint log-likelihoods instead: np . log ( prob_s1 ) + np . log ( prob_s2 ) -3.6845967923219334 Let's generalize this in a math function. Since P(s_t|s_{t-1}) P(s_t|s_{t-1}) is a multinomial distribution , then if we are given the log-likelihood of \\{s_1, s_2, s_3, ..., s_n\\} \\{s_1, s_2, s_3, ..., s_n\\} , we can calculate the log-likelihood over \\{s_2,... s_n\\} \\{s_2,... s_n\\} , which is given by the sum of the log probabilities: def state_logp ( states , p_transition ): logp = 0 # states are 0, 1, 2, but we model them as [1, 0, 0], [0, 1, 0], [0, 0, 1] states_oh = np . eye ( len ( p_transition )) for curr_state , next_state in zip ( states [: - 1 ], states [ 1 :]): p_tr = p_transition [ curr_state ] logp += multinomial ( n = 1 , p = p_tr ) . logpmf ( states_oh [ next_state ]) return logp state_logp ( states , p_transition ) -418.65677519562405 We will also write a vectorized version of state_logp . def state_logp_vect ( states , p_transition ): states_oh = np . eye ( len ( p_transition )) p_tr = p_transition [ states [: - 1 ]] obs = states_oh [ states [ 1 :]] return np . sum ( multinomial ( n = 1 , p = p_tr ) . logpmf ( obs )) state_logp_vect ( states , p_transition ) -418.6567751956279 Now, there is a problem here: we also need the log likelihood of the first state. Remember that if we don't know what the initial distribution is supposed to be, one possible assumption we can make is that the Markov sequence began by drawing from the equilibrium distribution. Here is where equilibrium distribution calculation from before comes in handy! def initial_logp ( states , p_transition ): initial_state = states [ 0 ] states_oh = np . eye ( len ( p_transition )) eq_p = equilibrium_distribution ( p_transition ) return ( multinomial ( n = 1 , p = eq_p ) . logpmf ( states_oh [ initial_state ] . squeeze ()) ) initial_logp ( states , p_transition ) array(-1.16057901) Taken together, we get the following Markov chain log-likelihood: def markov_state_logp ( states , p_transition ): return ( state_logp_vect ( states , p_transition ) + initial_logp ( states , p_transition ) ) markov_state_logp ( states , p_transition ) -419.81735420804523","title":"Markov Chain Log-Likelihood Calculation"},{"location":"machine-learning/markov-models/#markov-chain-with-gaussian-emissions-log-likelihood-calculation","text":"Now that we know how to calculate the log-likelihood for the Markov chain sequence of states, we can move on to the log-likelihood calculation for the emissions. Let's first assume that we have emissions that are non-autoregressive, and have a Gaussian likelihood. For the benefit of those who need it written out explicitly, here's the for-loop version: def gaussian_logp ( states , mus , sigmas , emissions ): logp = 0 for ( emission , state ) in zip ( emissions , states ): logp += norm ( mus [ state ], sigmas [ state ]) . logpdf ( emission ) return logp gaussian_logp ( states , mus = [ 1 , 0 , - 1 ], sigmas = [ 0.2 , 0.5 , 0.1 ], emissions = gaussian_ems ) 250.57996114495296 And we'll also make a vectorized version of it: def gaussian_logp_vect ( states , mus , sigmas , emissions ): mu = mus [ states ] sigma = sigmas [ states ] return np . sum ( norm ( mu , sigma ) . logpdf ( emissions )) gaussian_logp_vect ( states , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) 250.5799611449528 The joint log likelihood of the emissions and states are then given by their summation. def gaussian_emission_hmm_logp ( states , p_transition , mus , sigmas , emissions ): return markov_state_logp ( states , p_transition ) + gaussian_logp_vect ( states , mus , sigmas , emissions ) gaussian_emission_hmm_logp ( states , p_transition , mus = np . array ([ 1 , 0 , - 1 ]), sigmas = np . array ([ 0.2 , 0.5 , 0.1 ]), emissions = gaussian_ems ) -169.23739306309244 If you're in a Binder or local Jupyter session, go ahead and tweak the values of mus and sigmas , and verify for yourself that the current values are the \"maximum likelihood\" values. After all, our Gaussian emission data were generated according to this exact set of parameters!","title":"Markov Chain with Gaussian Emissions Log-Likelihood Calculation"},{"location":"machine-learning/markov-models/#markov-chain-with-autoregressive-gaussian-emissions-log-likelihood-calculation","text":"I hope the pattern is starting to be clear here: since we have Gaussian emissions, we only have to calculate the parameters of the Gaussian to know what the logpdf would be. As an example, I will be using the Gaussian with: State-varying scale Mean that is dependent on the previously emitted value This is the AR-HMM with data generated from the ar_gaussian_heteroskedastic_emissions function. def ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ): logp = 0 initial_state = states [ 0 ] initial_emission_logp = norm ( 0 , sigmas [ initial_state ]) . logpdf ( emissions [ 0 ]) for previous_emission , current_emission , state in zip ( emissions [: - 1 ], emissions [ 1 :], states [ 1 :]): loc = k * previous_emission scale = sigmas [ state ] logp += norm ( loc , scale ) . logpdf ( current_emission ) return logp ar_gaussian_heteroskedastic_emissions_logp ( states , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -18605.714303907385 Now, we can write the full log likelihood of the entire AR-HMM: def ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k , sigmas , emissions ): return ( markov_state_logp ( states , p_transition ) + ar_gaussian_heteroskedastic_emissions_logp ( states , k , sigmas , emissions ) ) ar_gausian_heteroskedastic_hmm_logp ( states , p_transition , k = 1.0 , sigmas = [ 0.5 , 0.1 , 0.01 ], emissions = ar_het_ems ) -19025.53165811543 For those of you who are familiar with Bayesian inference, as soon as we have a joint log likelihood that we can calculate between our model priors and data, using the simple Bayes' rule equation, we can obtain posterior distributions easily through an MCMC sampler. If this looks all foreign to you, then check out my other essay for a first look (or a refresher)!","title":"Markov Chain with Autoregressive Gaussian Emissions Log-Likelihood Calculation"},{"location":"machine-learning/markov-models/#hmm-distributions-in-pymc3","text":"While PyMC4 is in development, PyMC3 remains one of the leading probabilistic programming languages that can be used for Bayesian inference. PyMC3 doesn't have the HMM distribution defined in the library, but thanks to GitHub user @hstrey posting a Jupyter notebook with HMMs defined in there , many PyMC3 users have had a great baseline distribution to study pedagogically and use in their applications, myself included. Side note: I used @hstrey's implementation before setting out to write this essay. Thanks! The key thing to notice in this section is how the logp functions are defined . They will match the log probability functions that we have defined above, except written in Theano.","title":"HMM Distributions in PyMC3"},{"location":"machine-learning/markov-models/#hmm-states-distribution","text":"Let's first look at the HMM States distribution, which will give us a way to calculate the log probability of the states. import pymc3 as pm import theano.tensor as tt import theano.tensor.slinalg as sla # theano-wrapped scipy linear algebra import theano.tensor.nlinalg as nla # theano-wrapped numpy linear algebra import theano theano . config . gcc . cxxflags = \"-Wno-c++11-narrowing\" class HMMStates ( pm . Categorical ): def __init__ ( self , p_transition , p_equilibrium , n_states , * args , ** kwargs ): \"\"\"You can ignore this section for the time being.\"\"\" super ( pm . Categorical , self ) . __init__ ( * args , ** kwargs ) self . p_transition = p_transition self . p_equilibrium = p_equilibrium # This is needed self . k = n_states # This is only needed because discrete distributions must define a mode. self . mode = tt . cast ( 0 , dtype = 'int64' ) def logp ( self , x ): \"\"\"Focus your attention here!\"\"\" p_eq = self . p_equilibrium # Broadcast out the transition probabilities, # so that we can broadcast the calculation # of log-likelihoods p_tr = self . p_transition [ x [: - 1 ]] # the logp of the initial state evaluated against the equilibrium probabilities initial_state_logp = pm . Categorical . dist ( p_eq ) . logp ( x [ 0 ]) # the logp of the rest of the states. x_i = x [ 1 :] ou_like = pm . Categorical . dist ( p_tr ) . logp ( x_i ) transition_logp = tt . sum ( ou_like ) return initial_state_logp + transition_logp Above, the categorical distribution is used for convenience - it can handle integers, while multinomial requires the one-hot transformation. The categorical distribution is the generalization of the multinomial distribution, but unfortunately, it isn't implemented in the SciPy stats library, which is why we used the multinomial earlier on. Now, we stated earlier on that the transition matrix can be treated as a parameter to tweak, or else a random variable for which we want to infer its parameters. This means there is a natural fit for placing priors on them! Dirichlet distributions are great priors for probability vectors, as they are the generalization of Beta distributions. def solve_equilibrium ( n_states , p_transition ): A = tt . dmatrix ( 'A' ) A = tt . eye ( n_states ) - p_transition + tt . ones ( shape = ( n_states , n_states )) p_equilibrium = pm . Deterministic ( \"p_equilibrium\" , sla . solve ( A . T , tt . ones ( shape = ( n_states )))) return p_equilibrium import warnings warnings . simplefilter ( action = \"ignore\" , category = FutureWarning ) n_states = 3 with pm . Model () as model : p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )) * 4 , # weakly informative prior shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) obs_states = HMMStates ( \"states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , observed = np . array ( states ) . astype ( \"float\" ) ) Now let's fit the model! with model : trace = pm . sample ( 2000 ) Auto - assigning NUTS sampler ... Initializing NUTS using jitter + adapt_diag ... Multiprocess sampling ( 4 chains in 4 jobs ) NUTS : [ p_transition ] Sampling 4 chains , 0 divergences : 0 %| | 0 / 10000 [ 00:00<?, ?draws/s ]/ home / ericmjl / anaconda / envs / bayesian - analysis - recipes / lib / python3 .8 / site - packages / theano / tensor / slinalg . py : 255 : LinAlgWarning : Ill - conditioned matrix ( rcond = 5.89311e-08 ) : result may not be accurate . rval = scipy . linalg . solve ( A , b ) / home / ericmjl / anaconda / envs / bayesian - analysis - recipes / lib / python3 .8 / site - packages / theano / tensor / slinalg . py : 255 : LinAlgWarning : Ill - conditioned matrix ( rcond = 5.89311e-08 ) : result may not be accurate . rval = scipy . linalg . solve ( A , b ) Sampling 4 chains , 0 divergences : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10000 / 10000 [ 00:08<00:00, 1192.11draws/s ] import arviz as az az . plot_forest ( trace , var_names = [ \"p_transition\" ]); It looks like we were able to recover the original transitions!","title":"HMM States Distribution"},{"location":"machine-learning/markov-models/#hmm-with-gaussian-emissions","text":"Let's try out now an HMM model with Gaussian emissions. class HMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , mu , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states # self.rate = rate self . mu = mu self . sigma = sigma def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states # rate = self.rate[states] # broadcast the rate across the states. mu = self . mu [ states ] sigma = self . sigma [ states ] return tt . sum ( pm . Normal . dist ( mu = mu , sigma = sigma ) . logp ( x )) n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( gaussian_ems ),) ) # Prior for mu and sigma mu = pm . Normal ( \"mu\" , mu = 0 , sigma = 1 , shape = ( n_states ,)) sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) # Observed emission likelihood obs = HMMGaussianEmissions ( \"emission\" , states = hmm_states , mu = mu , sigma = sigma , observed = gaussian_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling ( 4 chains in 4 jobs ) CompoundStep > NUTS : [ sigma, mu, p_transition ] > CategoricalGibbsMetropolis : [ hmm_states ] Sampling 4 chains , 0 divergences : 100 %| \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 | 10000 / 10000 [ 11:59<00:00, 13.90draws/s ] The rhat statistic is larger than 1.4 for some parameters . The sampler did not converge . The estimated number of effective samples is smaller than 200 for some parameters . az . plot_trace ( trace , var_names = [ \"mu\" ]); az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); We are able to recover the parameters, but there is significant intra-chain homogeneity. That is fine, though one way to get around this is to explicitly instantiate prior distributions for each of the parameters instead.","title":"HMM with Gaussian Emissions"},{"location":"machine-learning/markov-models/#autoregressive-hmms-with-gaussian-emissions","text":"Let's now add in the autoregressive component to it. The data we will use is the ar_het_ems data, which were generated by using a heteroskedastic assumption, with Gaussian emissions whose mean depends on the previous value, while variance depends on state. As a reminder of what the data look like: ar_het_ems = ar_gaussian_heteroskedastic_emissions ( states , k = 0.6 , sigmas = [ 0.5 , 0.1 , 0.01 ]) plot_emissions ( states , ar_het_ems ) Let's now define the AR-HMM. class ARHMMGaussianEmissions ( pm . Continuous ): def __init__ ( self , states , k , sigma , * args , ** kwargs ): super () . __init__ ( * args , ** kwargs ) self . states = states self . sigma = sigma # variance self . k = k # autoregressive coefficient. def logp ( self , x ): \"\"\" x: observations \"\"\" states = self . states sigma = self . sigma [ states ] k = self . k ar_mean = k * x [: - 1 ] ar_like = tt . sum ( pm . Normal . dist ( mu = ar_mean , sigma = sigma [ 1 :]) . logp ( x [ 1 :])) boundary_like = pm . Normal . dist ( mu = 0 , sigma = sigma [ 0 ]) . logp ( x [ 0 ]) return ar_like + boundary_like n_states = 3 with pm . Model () as model : # Priors for transition matrix p_transition = pm . Dirichlet ( \"p_transition\" , a = tt . ones (( n_states , n_states )), shape = ( n_states , n_states )) # Solve for the equilibrium state p_equilibrium = solve_equilibrium ( n_states , p_transition ) # HMM state hmm_states = HMMStates ( \"hmm_states\" , p_transition = p_transition , p_equilibrium = p_equilibrium , n_states = n_states , shape = ( len ( ar_het_ems ),) ) # Prior for sigma and k sigma = pm . Exponential ( \"sigma\" , lam = 2 , shape = ( n_states ,)) k = pm . Beta ( \"k\" , alpha = 2 , beta = 2 ) # a not-so-weak prior for k # Observed emission likelihood obs = ARHMMGaussianEmissions ( \"emission\" , states = hmm_states , sigma = sigma , k = k , observed = ar_het_ems ) with model : trace = pm . sample ( 2000 ) Multiprocess sampling (4 chains in 4 jobs) CompoundStep >NUTS: [k, sigma, p_transition] >CategoricalGibbsMetropolis: [hmm_states] Sampling 4 chains, 6 divergences: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [12:34<00:00, 13.26draws/s] The acceptance probability does not match the target. It is 0.9096431867898114, but should be close to 0.8. Try to increase the number of tuning steps. There were 6 divergences after tuning. Increase `target_accept` or reparameterize. The rhat statistic is larger than 1.4 for some parameters. The sampler did not converge. The estimated number of effective samples is smaller than 200 for some parameters. Let's now take a look at the key parameters we might be interested in estimating: k k : the autoregressive coefficient, or how much previous emissions influence current emissions. \\sigma \\sigma : the variance that belongs to each state. az . plot_forest ( trace , var_names = [ \"k\" ]); az . plot_trace ( trace , var_names = [ \"k\" ]); It looks like we were able to obtain the value of k k correctly! az . plot_trace ( trace , var_names = [ \"sigma\" ]); az . plot_forest ( trace , var_names = [ \"sigma\" ]); It also looks like we were able to obtain the correct sigma values too, except that the chains are mixed up. We would do well to take care when calculating means for each parameter on the basis of chains. How about the chain states? Did we get them right? fig , ax = plt . subplots ( figsize = ( 12 , 4 )) plt . plot ( np . round ( trace [ \"hmm_states\" ] . mean ( axis = 0 )), label = \"true\" ) plt . plot ( 2 - np . array ( states ), label = \"inferred\" ) plt . legend (); I had to flip the states because they were backwards relative to the original. Qualitatively, not bad! If we wanted to be a bit more rigorous, we would quantify the accuracy of state identification. If the transition probabilities were a bit more extreme, we might have an easier time with the identifiability of the states. As it stands, because the variance is the only thing that changes, and because the variance of two of the three states are quite similar (one is 0.1 and the other is 0.5), distinguishing between these two states may be more difficult.","title":"Autoregressive HMMs with Gaussian Emissions"},{"location":"machine-learning/markov-models/#concluding-notes","text":"","title":"Concluding Notes"},{"location":"machine-learning/markov-models/#nothing-in-statistics-makes-sense","text":"...unless in light of a \"data generating model\". I initially struggled with the math behind HMMs and its variants, because I had never taken the time to think through the \"data generating process\" carefully. Once we have the data generating process, and in particular, its structure , it becomes trivial to map the structure of the model to the equations that are needed to model it. (I think this is why physicists are such good Bayesians: they are well-trained at thinking about mechanistic, data generating models.) For example, with autoregressive HMMs, until I sat down and thought through the data generating process step-by-step, nothing made sense. Once I wrote out how the mean of the previous observation influenced the mean of the current observation, then things made a ton of sense. In fact, now that I look back on my learning journey in Bayesian statistics, if we can define a likelihood function for our data, we can trivially work backwards and design a data generating process.","title":"Nothing in statistics makes sense..."},{"location":"machine-learning/markov-models/#model-structure-is-important","text":"While writing out the PyMC3 implementations and conditioning them on data, I remember times when I mismatched the model to the data, thus generating posterior samples that exhibited pathologies: divergences and more. This is a reminder that getting the structure of the model is very important.","title":"Model structure is important"},{"location":"machine-learning/markov-models/#keep-learning","text":"I hope this essay was useful for your learning journey as well. If you enjoyed it, please take a moment to star the repository !","title":"Keep learning"},{"location":"machine-learning/markov-models/#acknowledgements","text":"I would like to acknowledge the following colleagues and friends who have helped review the notebook. My colleagues, Zachary Barry and Balaji Goparaju, both of whom pointed out unclear phrasings in my prose and did some code review. Fellow PyMC developers, Colin Carroll (from whom I never cease to learn things), Alex Andorra (who also did code review), Junpeng Lao, Ravin Kumar, and Osvaldo Martin (also for their comments), Professor Allen Downey (of the Olin College of Engineering) who provided important pedagogical comments throughout the notebook.","title":"Acknowledgements"},{"location":"machine-learning/markov-models/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"machine-learning/message-passing/","text":"Computational Representations of Message Passing Abstract: Message passing on graphs, also known as graph convolutions, have become a popular research topic. In this piece, I aim to provide a short technical primer on ways to implement message passing on graphs. The goal is to provide clear pedagogy on what message passing means mathematically, and hopefully point towards cleaner computational implementations of the key algorithmic pieces. Assumed knowledge: We assume our reader has familiarity with elementary graph concepts. More specifically, the terms \u201cgraph\u201d, \u201cnodes\u201d, and \u201cedges\u201d should be familiar terms. Code examples in this technical piece will be written using the Python programming language, specifically using Python 3.7, NumPy 1.17 (in JAX), and NetworkX 2.2. Introduction to Message Passing Functions on Nodes Message passing starts with a \u201cfunction defined over nodes\u201d, which we will denote here as f(v) f(v) (for \u201cfunction of node/vertex v\u201d). What is this, one might ask? In short, this is nothing more than a numeric value of some kind attached to every node in a graph. This value could be scalar, vector, matrix, or tensor. The semantic meaning of that value is typically defined by the application domain that the graph is being used in. As a concrete example, in molecules, a \u201cfunction\u201d defined over the molecular graph could be the scalar-valued proton number. Carbon would be represented by the function f(v) = 6 f(v) = 6 . Alternatively, it could be a vector of values encompassing both the atomic mass and the number of valence electrons. In this case, carbon would be represented by the function f(v) = (6, 4) f(v) = (6, 4) . Visually, one might represent it as follows: Message Passing What then is message passing, or, as the deep learning community has adopted, \u201cgraph convolution\u201d? At its core, message passing is nothing more than a generic mathematical operation defined between a node\u2019s function value and its neighbors function value. As an example, one may define a message passing operation to be the summation the function evaluated at a node with the function evaluated on its neighbor\u2019s nodes. Here is a simplistic example, shown using a scalar on water: Summation is not the only message passing operation that can be defined. In principle, given any node (or vertex) v v and its neighbors N(v) N(v) values, we may write down a generic function f(v, N(v)) f(v, N(v)) that defines how the function value on each node is to be shared with its neighbors. Computational Implementations of Message Passing For simplicity, let us stay with the particular case where the message passing operation is defined as the summation of one\u2019s neighbors values with one\u2019s values. Object-Oriented Implementation With this definition in place, we may then define a message passing operation in Python as follows: 1 2 3 4 5 6 7 8 9 10 11 12 def message_passing ( G ): \"\"\"Object-oriented message passing operation.\"\"\" G_new = G . copy () for node , data in G . nodes ( data = True ): new_value = data [ \"value\" ] # assuming the value is stored under this key neighbors = G . neighbors ( node ) for neighbor in neighbors : new_value += G . nodes [ neighbor ][ \"value\" ] G_new . node [ node ][ \"value\" ] = new_value return G Thinking about computational considerations, we would naturally consider this implementation to be slow, because it involves a for-loop over Python objects. If we had multiple graphs over which we wanted message passing to be performed, the type-checking overhead in Python will naturally accumulate, and may even dominate. Linear Algebra Implementation How might we speed things up? As it turns out, linear algebra may be useful. We know that every graph may be represented as an adjacency matrix A , whose shape is (n_nodes, n_nodes) . As long as we maintain proper node ordering, we may also define a compatibly-shaped matrix F for node function values, whose shape is (n_nodes, n_features) . Taking advantage of this, in order define the \u201cself plus neighbors\u201d message passing operation in terms of linear algebra operations, we may then modify A by adding to it a diagonal matrix of ones. (In graph terminology, this is equivalent to adding a self-loop to the adjacency matrix.) Then, message passing, as defined above, is trivially the dot product of A and F : 1 2 3 4 5 6 7 8 9 def message_passing ( A , F ): \"\"\" Message passing done by linear algebra. :param A: Adjacency-like matrix, whose shape is (n_nodes, n_nodes). :param F: Feature matrix, whose shape is (n_nodes, n_features). \"\"\" return np . dot ( A , F ) In principle, variants on the adjacency matrix are possible. The only hard requirement for the matrix A is that it has the shape (n_nodes, n_nodes) . Adjacency Variant 1: N-degree adjacency matrix The adjacency matrix represents connectivity by degree 1. If we take the second matrix power of the adjacency matrix, we get back the connectivity of nodes at two degrees of separation away. More generically: 1 2 3 4 5 6 7 8 def n_degree_adjacency ( A , n : int ): \"\"\" Return the n-degree of separation adjacency matrix. :param A: Adjacency matrix, of shape (n_nodes, n_nodes) :param n: Number of degrees of separation. \"\"\" return np . linalg . matrix_power ( A , n ) Performing message passing using the N-degree adjacency matrix effectively describes sharing of information between nodes that are N-degrees of separation apart, skipping intermediate neighbors. Adjacency Variant 2: Graph laplacian matrix The graph laplacian matrix is defined as the diagonal degree matrix D (where the diagonal entries are the degree of each node) minus the adjacency matrix A : L = D - A . This matrix is the discrete analog to the Laplacian operator, and can give us information about the discrete gradient between a node and its neighbors. Message Passing on Multiple Graphs Thus far, we have seen an efficient implementation of message passing on a single graph using linear algebra. How would one perform message passing on multiple graphs, though? This is a question that has applications in graph neural networks (especially in cheminformatics). For the learning task where one has a batch of graphs, and the supervised learning task is to predict a scalar (or vector) value per graph, knowing how to efficiently message pass over multiple graphs is crucial to developing a performant graph neural network model. The challenge here, though, is that graphs generally are of variable size, hence it is not immediately obvious how to \u201ctensorify\u201d the operations. Let us look at a few alternatives, starting with the most obvious (but also most inefficient), building towards more efficient solutions. Implementation 1: For-loops over pairs of adjacency and feature matrices If we multiple graphs, they may be represented as a list of feature matrices and a list of adjacency matrices. The message passing operation, then, may be defined by writing a for-loop over pairs of these matrices. 1 2 3 4 5 def message_passing ( As , Fs ): outputs = [] for A , F in zip ( As , Fs ): outputs . append ( np . dot ( A , F )) return outputs Because of the for-loop, the obvious downside here is the overhead induced by running a for-loop over pairs of As and Fs. Implementation 2: Sparse Matrices Sparse matrices are an attractive alternative. Instead of treating graphs as independent samples, we may treat them as a single large graph on which we perform message passing. If we order the nodes in our adjacency matrix and feature matrix correctly, we will end up with a block diagonal adjacency matrix, and vertically stacked feature matrices. If we prepare the multiple graphs as a large disconnected graph, then we will have a dense feature matrix of shape (sum(n_nodes), n_feats) , and a sparse adjacency matrix of shape (sum(n_nodes), sum(n_nodes)) . Message passing then becomes a sparse-dense dot product: 1 2 def message_passing ( A , F ): return sparse . dot ( A , F ) The upside here is that message passing has been returned back to its natural form (a dot product). The downsides here are that the data must be prepared as a single large graph, hence we effectively lose what one would call the \u201csample\u201d (or \u201cbatch\u201d) dimension. Additionally, the most widely used deep learning libraries do not support automatic differentiation on sparse-dense or dense-sparse dot products, hence limiting the use of this implementation in deep learning. Implementation 3: Size-batched matrix multiplication An alternative way to conceptualize message passing is to think of graphs of the same size as belonging to a \u201csize batch\u201d. We may then vertically stack the feature and adjacency matrices of graphs of the same size together, and perform a batched matrix multiplication, ensuring that we preserve the sample/batch dimension in the final result. In terms of Python code, this requires special preparation of the graphs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from collections import defaultdict from jax.lax import batch_matmul def feature_matrix ( G ): # ... return F def prep_data ( Gs : list ): adjacency_matrices = defaultdict ( list ) feature_matrices = defaultdict ( list ) for G in Gs : size = len ( G ) F = feature_matrix ( G ) A = nx . adjacency_matrix ( G ) + np . ones ( size ) adjacency_matrices [ size ] . append ( A ) feature_matrices [ size ] . append ( A ) for size , As in adjacency_matrices . items (): adjacency_matrices [ size ] = np . stack ( As ) for size , Fs in feature_matrices . items (): feature_matrices [ size ] = np . stack ( Fs ) return adjacency_matrices , feature_matrices def message_passing ( As , Fs ): result = dict () for size in As . keys (): F = Fs [ size ] A = As [ size ] result [ size ] = batch_matmul ( A , F ) return result In this implementation, we use jax.lax.batch_matmul , which inherently assumes that the first dimension is the sample/batch dimension, and that the matrix multiplication happens on the subsequent dimensions. An advantage here is that the number of loop overhead calls in Python is reduced to the number of unique graph sizes that are present in the graph. The disadvantage, though, is that we have a dictionary data structure that we have to deal with, which makes data handling in Python less natural when dealing with linear algebra libraries. Implementation 4: Batched padded matrix multiplication In this implementation, we prepare the data in a different way. Firstly, we must know the size of the largest graph ahead-of-time. 1 size = ... # largest graph size We then pad every graph\u2019s feature matrix with zeros along the node axis until the node axis is as long as the largest graph size. 1 2 3 4 5 6 7 8 9 def prep_feats ( F , size ): # F is of shape (n_nodes, n_feats) return np . pad ( F , [ ( 0 , size - F . shape [ 0 ]), ( 0 , 0 ) ], ) We do the same with every adjacency matrix. 1 2 3 4 5 6 7 8 9 def prep_adjs ( A , size ): # A is of shape (n_nodes, n_nodes) return np . pad ( A , [ ( 0 , size - A . shape [ 0 ]), ( 0 , size - A . shape [ 0 ]), ], ) Finally, we simply stack them into the data matrix: 1 2 As = np . stack ([ prep_adjs ( A , size ) for A in As ] Fs = np . stack ([ prep_feats ( F , size ) for F in Fs ] Now, the shapes of our matrices are as follows: F takes on the shape (n_graphs, n_nodes, n_feats) A takes on the shape (n_graphs, n_nodes, n_nodes) If we desire to be semantically consistent with our shapes, then we might, by convention, assign the first dimension to be the sample/batch dimension. Finally, message passing is now trivially defined as a batch matrix multiply: 1 2 def message_passing ( A , F ): return batch_matmul ( A , F ) Visually, this is represented as follows: To this author\u2019s best knowledge, this should be the most efficient implementation of batched message passing across multiple graphs that also supports automatic differentiation, while also maintaining parity with the written equation form, hence preserving readability. The problems associated with a for-loop, sparse matrix multiplication, and dictionary carries, are removed. Moreover, the sample/batch dimension is preserved, hence it is semantically easy to map each graph to its corresponding output value. Given the current state of automatic differentiation libraries, no additional machinery is necessary to support sparse matrix products. The only disadvantage that this author can think of is that zero-padding may not be intuitive at first glance, and that the data must still be specially prepared and stacked first. Concluding Words This essay was initially motivated by the myriad of difficult-to-read message passing implementations present in the deep learning literature. Frequently, a for-loop of some kind is invoked, or an undocumented list data structure is created, in order to accomplish the message passing operation. Moreover, the model implementation is frequently not separated from the data preparation step, which makes for convoluted and mutually incompatible implementations of message passing in neural networks. It is my hope that while the research field is still in vogue, a technical piece that advises researchers on easily-readable and efficient implementations of message passing on graphs may help advance research practice. In particular, if our code can more closely match the equations listed in papers, that will help facilitate communication and verification of model implementations. To help researchers get started, an example implementation for the full data preparation and batched padded matrix multiplies in JAX is available on GitHub, archived on Zenodo. Acknowledgments I thank Rif. A. Saurous for our discussion at the PyMC4 developer summit in Montreal, QC, where his laser-like focus on \u201ctensorify everything\u201d inspired many new thoughts in my mind. Many thanks to my wife, Nan Li, who first pointed me to the linear algebra equivalents of graphs. I also thank David Duvenaud and Matthew J. Johnson for their pedagogy while they were at Harvard. Appendix Equivalence between padded and non-padded message passing To readers who may need an example to be convinced that matrix multiplying the padded matrices is equivalent to matrix multiplying the originals, we show the Python example below. Firstly, without padding: F = np . array ([[ 1 , 0 ], [ 1 , 1 ]]) A = np . array ([[ 1 , 0 ], [ 0 , 1 ]]) M = np . dot ( A , F ) # Value of M # DeviceArray([[1, 0], # [1, 1]], dtype=int32) And now, with padding: pad_size = 2 F_pad = np . pad ( F , pad_width = [ ( 0 , pad_size ), ( 0 , 0 ), ] ) A_pad = np . pad ( A , pad_width = [ ( 0 , pad_size ), ( 0 , pad_size ), ] ) # F_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32) # A_pad: # DeviceArray([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 0], # [0, 0, 0, 0]], dtype=int32) M_pad = np . dot ( A_pad , F_pad ) # M_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32) Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Computational Representations of Message Passing"},{"location":"machine-learning/message-passing/#computational-representations-of-message-passing","text":"Abstract: Message passing on graphs, also known as graph convolutions, have become a popular research topic. In this piece, I aim to provide a short technical primer on ways to implement message passing on graphs. The goal is to provide clear pedagogy on what message passing means mathematically, and hopefully point towards cleaner computational implementations of the key algorithmic pieces. Assumed knowledge: We assume our reader has familiarity with elementary graph concepts. More specifically, the terms \u201cgraph\u201d, \u201cnodes\u201d, and \u201cedges\u201d should be familiar terms. Code examples in this technical piece will be written using the Python programming language, specifically using Python 3.7, NumPy 1.17 (in JAX), and NetworkX 2.2.","title":"Computational Representations of Message Passing"},{"location":"machine-learning/message-passing/#introduction-to-message-passing","text":"","title":"Introduction to Message Passing"},{"location":"machine-learning/message-passing/#functions-on-nodes","text":"Message passing starts with a \u201cfunction defined over nodes\u201d, which we will denote here as f(v) f(v) (for \u201cfunction of node/vertex v\u201d). What is this, one might ask? In short, this is nothing more than a numeric value of some kind attached to every node in a graph. This value could be scalar, vector, matrix, or tensor. The semantic meaning of that value is typically defined by the application domain that the graph is being used in. As a concrete example, in molecules, a \u201cfunction\u201d defined over the molecular graph could be the scalar-valued proton number. Carbon would be represented by the function f(v) = 6 f(v) = 6 . Alternatively, it could be a vector of values encompassing both the atomic mass and the number of valence electrons. In this case, carbon would be represented by the function f(v) = (6, 4) f(v) = (6, 4) . Visually, one might represent it as follows:","title":"Functions on Nodes"},{"location":"machine-learning/message-passing/#message-passing","text":"What then is message passing, or, as the deep learning community has adopted, \u201cgraph convolution\u201d? At its core, message passing is nothing more than a generic mathematical operation defined between a node\u2019s function value and its neighbors function value. As an example, one may define a message passing operation to be the summation the function evaluated at a node with the function evaluated on its neighbor\u2019s nodes. Here is a simplistic example, shown using a scalar on water: Summation is not the only message passing operation that can be defined. In principle, given any node (or vertex) v v and its neighbors N(v) N(v) values, we may write down a generic function f(v, N(v)) f(v, N(v)) that defines how the function value on each node is to be shared with its neighbors.","title":"Message Passing"},{"location":"machine-learning/message-passing/#computational-implementations-of-message-passing","text":"For simplicity, let us stay with the particular case where the message passing operation is defined as the summation of one\u2019s neighbors values with one\u2019s values.","title":"Computational Implementations of Message Passing"},{"location":"machine-learning/message-passing/#object-oriented-implementation","text":"With this definition in place, we may then define a message passing operation in Python as follows: 1 2 3 4 5 6 7 8 9 10 11 12 def message_passing ( G ): \"\"\"Object-oriented message passing operation.\"\"\" G_new = G . copy () for node , data in G . nodes ( data = True ): new_value = data [ \"value\" ] # assuming the value is stored under this key neighbors = G . neighbors ( node ) for neighbor in neighbors : new_value += G . nodes [ neighbor ][ \"value\" ] G_new . node [ node ][ \"value\" ] = new_value return G Thinking about computational considerations, we would naturally consider this implementation to be slow, because it involves a for-loop over Python objects. If we had multiple graphs over which we wanted message passing to be performed, the type-checking overhead in Python will naturally accumulate, and may even dominate.","title":"Object-Oriented Implementation"},{"location":"machine-learning/message-passing/#linear-algebra-implementation","text":"How might we speed things up? As it turns out, linear algebra may be useful. We know that every graph may be represented as an adjacency matrix A , whose shape is (n_nodes, n_nodes) . As long as we maintain proper node ordering, we may also define a compatibly-shaped matrix F for node function values, whose shape is (n_nodes, n_features) . Taking advantage of this, in order define the \u201cself plus neighbors\u201d message passing operation in terms of linear algebra operations, we may then modify A by adding to it a diagonal matrix of ones. (In graph terminology, this is equivalent to adding a self-loop to the adjacency matrix.) Then, message passing, as defined above, is trivially the dot product of A and F : 1 2 3 4 5 6 7 8 9 def message_passing ( A , F ): \"\"\" Message passing done by linear algebra. :param A: Adjacency-like matrix, whose shape is (n_nodes, n_nodes). :param F: Feature matrix, whose shape is (n_nodes, n_features). \"\"\" return np . dot ( A , F ) In principle, variants on the adjacency matrix are possible. The only hard requirement for the matrix A is that it has the shape (n_nodes, n_nodes) .","title":"Linear Algebra Implementation"},{"location":"machine-learning/message-passing/#adjacency-variant-1-n-degree-adjacency-matrix","text":"The adjacency matrix represents connectivity by degree 1. If we take the second matrix power of the adjacency matrix, we get back the connectivity of nodes at two degrees of separation away. More generically: 1 2 3 4 5 6 7 8 def n_degree_adjacency ( A , n : int ): \"\"\" Return the n-degree of separation adjacency matrix. :param A: Adjacency matrix, of shape (n_nodes, n_nodes) :param n: Number of degrees of separation. \"\"\" return np . linalg . matrix_power ( A , n ) Performing message passing using the N-degree adjacency matrix effectively describes sharing of information between nodes that are N-degrees of separation apart, skipping intermediate neighbors.","title":"Adjacency Variant 1: N-degree adjacency matrix"},{"location":"machine-learning/message-passing/#adjacency-variant-2-graph-laplacian-matrix","text":"The graph laplacian matrix is defined as the diagonal degree matrix D (where the diagonal entries are the degree of each node) minus the adjacency matrix A : L = D - A . This matrix is the discrete analog to the Laplacian operator, and can give us information about the discrete gradient between a node and its neighbors.","title":"Adjacency Variant 2: Graph laplacian matrix"},{"location":"machine-learning/message-passing/#message-passing-on-multiple-graphs","text":"Thus far, we have seen an efficient implementation of message passing on a single graph using linear algebra. How would one perform message passing on multiple graphs, though? This is a question that has applications in graph neural networks (especially in cheminformatics). For the learning task where one has a batch of graphs, and the supervised learning task is to predict a scalar (or vector) value per graph, knowing how to efficiently message pass over multiple graphs is crucial to developing a performant graph neural network model. The challenge here, though, is that graphs generally are of variable size, hence it is not immediately obvious how to \u201ctensorify\u201d the operations. Let us look at a few alternatives, starting with the most obvious (but also most inefficient), building towards more efficient solutions.","title":"Message Passing on Multiple Graphs"},{"location":"machine-learning/message-passing/#implementation-1-for-loops-over-pairs-of-adjacency-and-feature-matrices","text":"If we multiple graphs, they may be represented as a list of feature matrices and a list of adjacency matrices. The message passing operation, then, may be defined by writing a for-loop over pairs of these matrices. 1 2 3 4 5 def message_passing ( As , Fs ): outputs = [] for A , F in zip ( As , Fs ): outputs . append ( np . dot ( A , F )) return outputs Because of the for-loop, the obvious downside here is the overhead induced by running a for-loop over pairs of As and Fs.","title":"Implementation 1: For-loops over pairs of adjacency and feature matrices"},{"location":"machine-learning/message-passing/#implementation-2-sparse-matrices","text":"Sparse matrices are an attractive alternative. Instead of treating graphs as independent samples, we may treat them as a single large graph on which we perform message passing. If we order the nodes in our adjacency matrix and feature matrix correctly, we will end up with a block diagonal adjacency matrix, and vertically stacked feature matrices. If we prepare the multiple graphs as a large disconnected graph, then we will have a dense feature matrix of shape (sum(n_nodes), n_feats) , and a sparse adjacency matrix of shape (sum(n_nodes), sum(n_nodes)) . Message passing then becomes a sparse-dense dot product: 1 2 def message_passing ( A , F ): return sparse . dot ( A , F ) The upside here is that message passing has been returned back to its natural form (a dot product). The downsides here are that the data must be prepared as a single large graph, hence we effectively lose what one would call the \u201csample\u201d (or \u201cbatch\u201d) dimension. Additionally, the most widely used deep learning libraries do not support automatic differentiation on sparse-dense or dense-sparse dot products, hence limiting the use of this implementation in deep learning.","title":"Implementation 2: Sparse Matrices"},{"location":"machine-learning/message-passing/#implementation-3-size-batched-matrix-multiplication","text":"An alternative way to conceptualize message passing is to think of graphs of the same size as belonging to a \u201csize batch\u201d. We may then vertically stack the feature and adjacency matrices of graphs of the same size together, and perform a batched matrix multiplication, ensuring that we preserve the sample/batch dimension in the final result. In terms of Python code, this requires special preparation of the graphs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 from collections import defaultdict from jax.lax import batch_matmul def feature_matrix ( G ): # ... return F def prep_data ( Gs : list ): adjacency_matrices = defaultdict ( list ) feature_matrices = defaultdict ( list ) for G in Gs : size = len ( G ) F = feature_matrix ( G ) A = nx . adjacency_matrix ( G ) + np . ones ( size ) adjacency_matrices [ size ] . append ( A ) feature_matrices [ size ] . append ( A ) for size , As in adjacency_matrices . items (): adjacency_matrices [ size ] = np . stack ( As ) for size , Fs in feature_matrices . items (): feature_matrices [ size ] = np . stack ( Fs ) return adjacency_matrices , feature_matrices def message_passing ( As , Fs ): result = dict () for size in As . keys (): F = Fs [ size ] A = As [ size ] result [ size ] = batch_matmul ( A , F ) return result In this implementation, we use jax.lax.batch_matmul , which inherently assumes that the first dimension is the sample/batch dimension, and that the matrix multiplication happens on the subsequent dimensions. An advantage here is that the number of loop overhead calls in Python is reduced to the number of unique graph sizes that are present in the graph. The disadvantage, though, is that we have a dictionary data structure that we have to deal with, which makes data handling in Python less natural when dealing with linear algebra libraries.","title":"Implementation 3: Size-batched matrix multiplication"},{"location":"machine-learning/message-passing/#implementation-4-batched-padded-matrix-multiplication","text":"In this implementation, we prepare the data in a different way. Firstly, we must know the size of the largest graph ahead-of-time. 1 size = ... # largest graph size We then pad every graph\u2019s feature matrix with zeros along the node axis until the node axis is as long as the largest graph size. 1 2 3 4 5 6 7 8 9 def prep_feats ( F , size ): # F is of shape (n_nodes, n_feats) return np . pad ( F , [ ( 0 , size - F . shape [ 0 ]), ( 0 , 0 ) ], ) We do the same with every adjacency matrix. 1 2 3 4 5 6 7 8 9 def prep_adjs ( A , size ): # A is of shape (n_nodes, n_nodes) return np . pad ( A , [ ( 0 , size - A . shape [ 0 ]), ( 0 , size - A . shape [ 0 ]), ], ) Finally, we simply stack them into the data matrix: 1 2 As = np . stack ([ prep_adjs ( A , size ) for A in As ] Fs = np . stack ([ prep_feats ( F , size ) for F in Fs ] Now, the shapes of our matrices are as follows: F takes on the shape (n_graphs, n_nodes, n_feats) A takes on the shape (n_graphs, n_nodes, n_nodes) If we desire to be semantically consistent with our shapes, then we might, by convention, assign the first dimension to be the sample/batch dimension. Finally, message passing is now trivially defined as a batch matrix multiply: 1 2 def message_passing ( A , F ): return batch_matmul ( A , F ) Visually, this is represented as follows: To this author\u2019s best knowledge, this should be the most efficient implementation of batched message passing across multiple graphs that also supports automatic differentiation, while also maintaining parity with the written equation form, hence preserving readability. The problems associated with a for-loop, sparse matrix multiplication, and dictionary carries, are removed. Moreover, the sample/batch dimension is preserved, hence it is semantically easy to map each graph to its corresponding output value. Given the current state of automatic differentiation libraries, no additional machinery is necessary to support sparse matrix products. The only disadvantage that this author can think of is that zero-padding may not be intuitive at first glance, and that the data must still be specially prepared and stacked first.","title":"Implementation 4: Batched padded matrix multiplication"},{"location":"machine-learning/message-passing/#concluding-words","text":"This essay was initially motivated by the myriad of difficult-to-read message passing implementations present in the deep learning literature. Frequently, a for-loop of some kind is invoked, or an undocumented list data structure is created, in order to accomplish the message passing operation. Moreover, the model implementation is frequently not separated from the data preparation step, which makes for convoluted and mutually incompatible implementations of message passing in neural networks. It is my hope that while the research field is still in vogue, a technical piece that advises researchers on easily-readable and efficient implementations of message passing on graphs may help advance research practice. In particular, if our code can more closely match the equations listed in papers, that will help facilitate communication and verification of model implementations. To help researchers get started, an example implementation for the full data preparation and batched padded matrix multiplies in JAX is available on GitHub, archived on Zenodo.","title":"Concluding Words"},{"location":"machine-learning/message-passing/#acknowledgments","text":"I thank Rif. A. Saurous for our discussion at the PyMC4 developer summit in Montreal, QC, where his laser-like focus on \u201ctensorify everything\u201d inspired many new thoughts in my mind. Many thanks to my wife, Nan Li, who first pointed me to the linear algebra equivalents of graphs. I also thank David Duvenaud and Matthew J. Johnson for their pedagogy while they were at Harvard.","title":"Acknowledgments"},{"location":"machine-learning/message-passing/#appendix","text":"","title":"Appendix"},{"location":"machine-learning/message-passing/#equivalence-between-padded-and-non-padded-message-passing","text":"To readers who may need an example to be convinced that matrix multiplying the padded matrices is equivalent to matrix multiplying the originals, we show the Python example below. Firstly, without padding: F = np . array ([[ 1 , 0 ], [ 1 , 1 ]]) A = np . array ([[ 1 , 0 ], [ 0 , 1 ]]) M = np . dot ( A , F ) # Value of M # DeviceArray([[1, 0], # [1, 1]], dtype=int32) And now, with padding: pad_size = 2 F_pad = np . pad ( F , pad_width = [ ( 0 , pad_size ), ( 0 , 0 ), ] ) A_pad = np . pad ( A , pad_width = [ ( 0 , pad_size ), ( 0 , pad_size ), ] ) # F_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32) # A_pad: # DeviceArray([[1, 0, 0, 0], # [0, 1, 0, 0], # [0, 0, 0, 0], # [0, 0, 0, 0]], dtype=int32) M_pad = np . dot ( A_pad , F_pad ) # M_pad: # DeviceArray([[1, 0], # [1, 1], # [0, 0], # [0, 0]], dtype=int32)","title":"Equivalence between padded and non-padded message passing"},{"location":"machine-learning/message-passing/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"machine-learning/reimplementing-models/","text":"Reimplementing and Testing Deep Learning Models At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting. As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX). Benefits of re-implementing deep learning models Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested. Developing familiarity with deep learning frameworks Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.) My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages. Developing a mechanistic understanding of the model It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction. Developing an ability to test and sanity-check the model If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model. Reimplementing models is not a waste of time By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste. A proposed workflow for reimplementing deep learning models I will now propose a workflow for re-implementing deep learning models. Identify a coding partner Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model. Doing so helps a few ways. Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up. Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing. Pick out the \"forward\" step of the model The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations. A few keywords to look out for are the forward() and __call__() class methods. class MyModel ( nn . Model ): # ... def forward ( self , X ): # Implementation of model happens here. something = ... return something For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called encoder() and decoder() , which themselves reference another model that would have a forward() or __call__() defined. class AutoEncoder ( nn . Model ): # ... def forward ( self , X ): something = self . encoder ( X ) output = self . decoder ( something ) return output Re-implementing the forward() part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data. Inspect the shapes of the weights While the equations give the model structure , the weights and biases, or the parameters , are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside the priors, and fit the model parameters.) Because much of deep learning hinges on linear algebra, and because most of the transformations that happen involve transforming the input space into the output space , getting the shapes of the parameters is very important. In a re-implementation exercise with my intern, where we re-implemented a specially designed recurrent neural network layer in JAX, we did a manual sanity check through our implementation to identify what the shapes would need to be for the inputs and outputs. After that, we encoded that manual test into an automatic test. Later on, after we built another test that integrated which paradoxically failed on shapes, we eventually uncovered that we were indexing into the wrong dimensions in our implementation. This led to us (1) fixing the bug, (2) writing a more comprehensive documentation and test suite, and (3) writing better documentations for the semantic meaning of each tensor axis. Write tests for the neural network components Once we have the neural network model and its components implemented, writing tests for those components is a wonderful way of making sure that (1) the implementation is correct, to the best of our knowledge, and that (2) we can catch when the implementation might have been broken inadvertently. The shape test (as described above) is one way of doing this. def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data ) assert output . shape [ 1 ] == output_dims If there are special elementwise transforms performed on the data, such as a ReLU or exponential transform, we can test that the numerical properties of the output are correct: def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data , nonlinearity = \"relu\" ) assert np . all ( output >= 0 ) Write tests for the entire training loop Once the model has been re-implemented in its entirety, prepare a small set of training data, and pass it through the model, and attempt to train it for a few epochs. If the model, as implemented, is doing what we think it should be, then after a dozen epochs or so, the training loss should go down. We can then test that the training loss at the end is less than the training loss at the beginning. If the loss does go down, it's necessary but not sufficient for knowing that the model is implemented correctly. However, if the loss does not go down, then we will definitely know that a problem exists somewhere in the code, and can begin to debug. An example with pseudocode below might look like the following: from data import dummy_graph_data from model import gnn_model from params import make_gnn_params from losses import mse_loss from jax import grad from jax.experimental.optimizers import adam def test_gnn_training (): # Prepare training data x , y = dummy_graph_data ( * args , ** kwargs ) params = make_gnn_params ( * args , ** kwargs ) dloss = grad ( mse_loss ) init , update , get_params = adam ( step_size = 0.005 ) start_loss = mse_loss ( params , model , x , y ) state = init ( params ) for i in range ( 10 ): g = dloss ( params , model , x , y ) state = update ( i , g , state ) params = get_params ( state ) end_loss = mse_loss ( params , model , x , y ) assert end_loss < start_loss A side benefit of this is that if you commit to only judiciously changing the tests, you will end up with a stable and copy/paste-able training loop that you know you can trust on new learning tasks, and hence only need to worry about swapping out the data. Build little tools for yourself that automate repetitive (boring) things You may notice in the above integration test, we wrote a lot of other functions that make testing much easier, such as dummy data generators, and parameter initializers. These are tools that make composing parts of the entire training process modular and easy to compose. I strongly recommend writing these things, and also backing them with more tests (since we will end up relying on them anyways). Now run your deep learning experiments Once we have the model re-implemented and tested, the groundwork is present for us to conduct extensive experiments with the confidence that we know how to catch bugs in the model in a fairly automated fashion. Concluding words Re-implementing deep learning models can be a very fun and rewarding exercise, because it serves as an excellent tool to check our understanding of the models that we work with. Without the right safeguards in place, though, it can also very quickly metamorphose into a nightmare rabbithole of debugging. Placing basic safeguards in place when re-implementing models helps us avoid as many of these rabbitholes as possible. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Reimplementing and Testing Deep Learning Models"},{"location":"machine-learning/reimplementing-models/#reimplementing-and-testing-deep-learning-models","text":"At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting. As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX).","title":"Reimplementing and Testing Deep Learning Models"},{"location":"machine-learning/reimplementing-models/#benefits-of-re-implementing-deep-learning-models","text":"Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested.","title":"Benefits of re-implementing deep learning models"},{"location":"machine-learning/reimplementing-models/#developing-familiarity-with-deep-learning-frameworks","text":"Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.) My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages.","title":"Developing familiarity with deep learning frameworks"},{"location":"machine-learning/reimplementing-models/#developing-a-mechanistic-understanding-of-the-model","text":"It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction.","title":"Developing a mechanistic understanding of the model"},{"location":"machine-learning/reimplementing-models/#developing-an-ability-to-test-and-sanity-check-the-model","text":"If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model.","title":"Developing an ability to test and sanity-check the model"},{"location":"machine-learning/reimplementing-models/#reimplementing-models-is-not-a-waste-of-time","text":"By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste.","title":"Reimplementing models is not a waste of time"},{"location":"machine-learning/reimplementing-models/#a-proposed-workflow-for-reimplementing-deep-learning-models","text":"I will now propose a workflow for re-implementing deep learning models.","title":"A proposed workflow for reimplementing deep learning models"},{"location":"machine-learning/reimplementing-models/#identify-a-coding-partner","text":"Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model. Doing so helps a few ways. Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up. Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing.","title":"Identify a coding partner"},{"location":"machine-learning/reimplementing-models/#pick-out-the-forward-step-of-the-model","text":"The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations. A few keywords to look out for are the forward() and __call__() class methods. class MyModel ( nn . Model ): # ... def forward ( self , X ): # Implementation of model happens here. something = ... return something For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called encoder() and decoder() , which themselves reference another model that would have a forward() or __call__() defined. class AutoEncoder ( nn . Model ): # ... def forward ( self , X ): something = self . encoder ( X ) output = self . decoder ( something ) return output Re-implementing the forward() part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data.","title":"Pick out the \"forward\" step of the model"},{"location":"machine-learning/reimplementing-models/#inspect-the-shapes-of-the-weights","text":"While the equations give the model structure , the weights and biases, or the parameters , are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside the priors, and fit the model parameters.) Because much of deep learning hinges on linear algebra, and because most of the transformations that happen involve transforming the input space into the output space , getting the shapes of the parameters is very important. In a re-implementation exercise with my intern, where we re-implemented a specially designed recurrent neural network layer in JAX, we did a manual sanity check through our implementation to identify what the shapes would need to be for the inputs and outputs. After that, we encoded that manual test into an automatic test. Later on, after we built another test that integrated which paradoxically failed on shapes, we eventually uncovered that we were indexing into the wrong dimensions in our implementation. This led to us (1) fixing the bug, (2) writing a more comprehensive documentation and test suite, and (3) writing better documentations for the semantic meaning of each tensor axis.","title":"Inspect the shapes of the weights"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-neural-network-components","text":"Once we have the neural network model and its components implemented, writing tests for those components is a wonderful way of making sure that (1) the implementation is correct, to the best of our knowledge, and that (2) we can catch when the implementation might have been broken inadvertently. The shape test (as described above) is one way of doing this. def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data ) assert output . shape [ 1 ] == output_dims If there are special elementwise transforms performed on the data, such as a ReLU or exponential transform, we can test that the numerical properties of the output are correct: def test_layer_shapes (): weights = np . random . normal ( size = ( input_dims , output_dims )) data = np . random . normal ( size = ( batch_size , input_dims )) output = nn_layer ( weights , data , nonlinearity = \"relu\" ) assert np . all ( output >= 0 )","title":"Write tests for the neural network components"},{"location":"machine-learning/reimplementing-models/#write-tests-for-the-entire-training-loop","text":"Once the model has been re-implemented in its entirety, prepare a small set of training data, and pass it through the model, and attempt to train it for a few epochs. If the model, as implemented, is doing what we think it should be, then after a dozen epochs or so, the training loss should go down. We can then test that the training loss at the end is less than the training loss at the beginning. If the loss does go down, it's necessary but not sufficient for knowing that the model is implemented correctly. However, if the loss does not go down, then we will definitely know that a problem exists somewhere in the code, and can begin to debug. An example with pseudocode below might look like the following: from data import dummy_graph_data from model import gnn_model from params import make_gnn_params from losses import mse_loss from jax import grad from jax.experimental.optimizers import adam def test_gnn_training (): # Prepare training data x , y = dummy_graph_data ( * args , ** kwargs ) params = make_gnn_params ( * args , ** kwargs ) dloss = grad ( mse_loss ) init , update , get_params = adam ( step_size = 0.005 ) start_loss = mse_loss ( params , model , x , y ) state = init ( params ) for i in range ( 10 ): g = dloss ( params , model , x , y ) state = update ( i , g , state ) params = get_params ( state ) end_loss = mse_loss ( params , model , x , y ) assert end_loss < start_loss A side benefit of this is that if you commit to only judiciously changing the tests, you will end up with a stable and copy/paste-able training loop that you know you can trust on new learning tasks, and hence only need to worry about swapping out the data.","title":"Write tests for the entire training loop"},{"location":"machine-learning/reimplementing-models/#build-little-tools-for-yourself-that-automate-repetitive-boring-things","text":"You may notice in the above integration test, we wrote a lot of other functions that make testing much easier, such as dummy data generators, and parameter initializers. These are tools that make composing parts of the entire training process modular and easy to compose. I strongly recommend writing these things, and also backing them with more tests (since we will end up relying on them anyways).","title":"Build little tools for yourself that automate repetitive (boring) things"},{"location":"machine-learning/reimplementing-models/#now-run-your-deep-learning-experiments","text":"Once we have the model re-implemented and tested, the groundwork is present for us to conduct extensive experiments with the confidence that we know how to catch bugs in the model in a fairly automated fashion.","title":"Now run your deep learning experiments"},{"location":"machine-learning/reimplementing-models/#concluding-words","text":"Re-implementing deep learning models can be a very fun and rewarding exercise, because it serves as an excellent tool to check our understanding of the models that we work with. Without the right safeguards in place, though, it can also very quickly metamorphose into a nightmare rabbithole of debugging. Placing basic safeguards in place when re-implementing models helps us avoid as many of these rabbitholes as possible.","title":"Concluding words"},{"location":"machine-learning/reimplementing-models/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"miscellaneous/dashboarding-landscape/","text":"A Review of the Python Data Science Dashboarding Landscape in 2019 Introduction As Pythonista data scientists, we are spoiled for choice when it comes to developing front-ends for our data apps. We used to have to fiddle with HTML in Flask (or Plotly's Dash), but now, there are tools in which \"someone wrote the HTML/JS so I didn't have to\". Let me give a quick tour of the landscape of tools as I've experienced it in 2019. Beginnings: Voila Previously, I had test-driven Voila . The key advantage I saw back then was that in my workflow, once I had the makings of a UI present in the Jupyter notebook, and just needed a way to serve it up independent of having my end-users run a Jupyter server, then Voila helped solve that use case. By taking advantage of existing the ipywidgets ecosystem and adding on a way to run and serve the HTML output of a notebook, Voila solved that part of the dashboarding story quite nicely. In many respects, I regard Voila as the first proper dashboarding tool for Pythonistas. That said, development in a Jupyter notebook didn't necessarily foster best practices (such as refactoring and testing code). When my first project at work ended, and I didn't have a need for further dashboarding, I didn't touch Voila for a long time. Another player: Panel Later, Panel showed up. Panel's development model allowed a more modular app setup, including importing of plotting functions defined inside .py files that returned individual plots. Panel also allowed me to prototype in a notebook and see the output live before moving the dashboard code into a source .py file. At work, we based a one-stop shop dashboard for a project on Panel, and in my personal life, I also built a minimal panel app that I also deployed to Heroku . Panel was definitely developed targeting notebook and source file use cases in mind, and this shows through in its source development model. That said, panel apps could be slow to load, and without having a \"spinner\" solution in place (i.e. something to show the user that the app is \"doing something\" in the background), it sometimes made apps feel slow even though the slowness was not Panel's fault really. (My colleagues and I pulled out all the tricks in our bag to speed things up.) Additionally, any errors that show up don't get surfaced to the app's UI, where developer eyeballs are on - instead, they get buried in the browser's JavaScript console or in the Python terminal where the app is being served. When deployed, this makes it difficult to see where errors show up and debug errors. Enter Streamlit Now, Streamlit comes along, and some of its initial demos are pretty rad. In order to test-drive it, I put together this little tutorial on the Beta probability distribution for my colleagues. Streamlit definitely solves some of the pain points that I've observed with Panel and Voila. The most important one that I see is that errors are captured by Streamlit and bubbled up to the UI, where our eyeballs are going to be when developing the app. For me, this is a very sensible decision to make, for two reasons: Firstly, it makes debugging interactions that much easier. Instead of needing to have two interfaces open, the error message shows up right where the interaction fails, in the same browser window as the UI elements. Secondly, it makes it possible for us to use the error messages as a UI \"hack\" to inform users where their inputs (e.g. free text) might be invalid, thereby giving them informative error messages . (Try it out in the Beta distribution app: it'll give you an error message right below if you try to type something that cant be converted into a float!) The other key thing that Streamlit provides as a UI nice-ity is the ability to signal to end-users that a computation is happening. Streamlit does this in three ways, two of which always come for free. Firstly , if something is \"running\", then in the top-right hand corner of the page, the \"Running\" spinner will animate. Secondly , anything that is re-rendering will automatically be greyed out. Finally , we can use a special context manager to provide a custom message on the front-end: import streamlit as st with st . spinner ( \"Message goes here...\" ): # stuff happens So all-in-all, Streamlit seems to have a solution of some kind for the friction points that I have observed with Panel and Voila. Besides that, Streamlit, I think, uses a procedural paradigm, rather than a callback paradigm, for app construction. We just have to think of the app as a linear sequence of actions that happen from top to bottom. State is never really an issue, because every code change and interaction re-runs the source file from top to bottom, from scratch. When building quick apps, this paradigm really simplifies things compared to a callback-based paradigm. Finally, Streamlit also provides a convenient way to add text to the UI by automatically parsing as Markdown any raw strings unassigned to a variable in a .py file and rendering them as HTML. This opens the door to treating a .py file as a literate programming document , hosted by a Python-based server in the backend. It'd be useful especially in teaching scenarios. (With pyiodide bringing the PyData stack to the browser, I can't wait to see standalone .py files rendered to the DOM!) Now, this isn't to say that Streamlit is problem-free. There are still rough edges, the most glaring (as of today) in the current release is the inability to upload a file and operate on it. This has been fixed in a recent pull request , so I'm expecting this should show up in a new release any time soon. The other not-so-big-problem that I see with Streamlit at the moment is the procedural paradigm - by always re-running code from top-to-bottom afresh on every single change, apps that rely on long compute may need a bit more thought to construct, including the use of Streamlit's caching mechanism. Being procedural does make things easier for development though, and on balance, I would not discount Streamlit's simplicity here. Where does Streamlit fit? As I see it, Streamlit's devs are laser-focused on enabling devs to very quickly get to a somewhat good-looking app prototype. In my experience, the development time for the Beta distribution app took about 3 hours, 2.5 of which were spent on composing prose. So effectively, I only used half an hour doing code writing, with a live and auto-reloading preview greatly simplifying the development process. (I conservatively estimate that this is about 1.5 times as fast as I would be using Panel.) Given Streamlit, I would use it to develop two classes of apps: (1) very tightly-focused utility apps that do one lightweight thing well, and (2) bespoke, single-document literate programming education material. I would be quite hesitant to build more complex things; then again, for me, that statement would be true more generally anyways with whatever tool. In any case, I think bringing UNIX-like thinking to the web is probably a good idea: we make little utilities/functional tools that can pipe standard data formats from to another. Common pain points across all three dashboarding tools A design pattern I have desired is to be able to serve up a fleet of small, individual utilities served up from the same codebase, served up by individual server processes, but all packaged within the same container. The only way I can think of at the moment is to build a custom Flask-based gateway to redirect properly to each utility's process. That said, I think this is probably out of scope for the individual dashboarding projects. How do we go forward? The ecosystem is ever-evolving, and, rather than being left confused by the multitude of options available to us, I find myself actually being very encouraged at the development that has been happening. There's competing ideas with friendly competition between the developers, but they are also simultaneously listening to each other and their users and converging on similar things in the end. That said, I think it would be premature to go \"all-in\" on a single solution at this moment. For the individual data scientist, I would advise to be able to build something using each of the dashboarding frameworks. My personal recommendations are to know how to use: Voila + ipywidgets in a Jupyter notebook Panel in Jupyter notebooks and standalone .py files Streamlit in .py files. These recommendations stem mainly from the ability to style and layout content without needing much knowledge of HTML. In terms of roughly when to use what, my prior experience has been that Voila and Streamlit are pretty good for quicker prototypes, while Panel has been good for more complex ones, though in all cases, we have to worry about speed impacting user experience. From my experience at work, being able to quickly hash out key visual elements in a front-end prototype gives us the ability to better communicate with UI/UX designers and developers on what we're trying to accomplish. Knowing how to build front-ends ourselves lowers the communication and engineering barrier when taking a project to production. It's a worthwhile skill to have; be sure to have it in your toolbox! Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"A Review of the Python Data Science Dashboarding Landscape in 2019"},{"location":"miscellaneous/dashboarding-landscape/#a-review-of-the-python-data-science-dashboarding-landscape-in-2019","text":"","title":"A Review of the Python Data Science Dashboarding Landscape in 2019"},{"location":"miscellaneous/dashboarding-landscape/#introduction","text":"As Pythonista data scientists, we are spoiled for choice when it comes to developing front-ends for our data apps. We used to have to fiddle with HTML in Flask (or Plotly's Dash), but now, there are tools in which \"someone wrote the HTML/JS so I didn't have to\". Let me give a quick tour of the landscape of tools as I've experienced it in 2019.","title":"Introduction"},{"location":"miscellaneous/dashboarding-landscape/#beginnings-voila","text":"Previously, I had test-driven Voila . The key advantage I saw back then was that in my workflow, once I had the makings of a UI present in the Jupyter notebook, and just needed a way to serve it up independent of having my end-users run a Jupyter server, then Voila helped solve that use case. By taking advantage of existing the ipywidgets ecosystem and adding on a way to run and serve the HTML output of a notebook, Voila solved that part of the dashboarding story quite nicely. In many respects, I regard Voila as the first proper dashboarding tool for Pythonistas. That said, development in a Jupyter notebook didn't necessarily foster best practices (such as refactoring and testing code). When my first project at work ended, and I didn't have a need for further dashboarding, I didn't touch Voila for a long time.","title":"Beginnings: Voila"},{"location":"miscellaneous/dashboarding-landscape/#another-player-panel","text":"Later, Panel showed up. Panel's development model allowed a more modular app setup, including importing of plotting functions defined inside .py files that returned individual plots. Panel also allowed me to prototype in a notebook and see the output live before moving the dashboard code into a source .py file. At work, we based a one-stop shop dashboard for a project on Panel, and in my personal life, I also built a minimal panel app that I also deployed to Heroku . Panel was definitely developed targeting notebook and source file use cases in mind, and this shows through in its source development model. That said, panel apps could be slow to load, and without having a \"spinner\" solution in place (i.e. something to show the user that the app is \"doing something\" in the background), it sometimes made apps feel slow even though the slowness was not Panel's fault really. (My colleagues and I pulled out all the tricks in our bag to speed things up.) Additionally, any errors that show up don't get surfaced to the app's UI, where developer eyeballs are on - instead, they get buried in the browser's JavaScript console or in the Python terminal where the app is being served. When deployed, this makes it difficult to see where errors show up and debug errors.","title":"Another player: Panel"},{"location":"miscellaneous/dashboarding-landscape/#enter-streamlit","text":"Now, Streamlit comes along, and some of its initial demos are pretty rad. In order to test-drive it, I put together this little tutorial on the Beta probability distribution for my colleagues. Streamlit definitely solves some of the pain points that I've observed with Panel and Voila. The most important one that I see is that errors are captured by Streamlit and bubbled up to the UI, where our eyeballs are going to be when developing the app. For me, this is a very sensible decision to make, for two reasons: Firstly, it makes debugging interactions that much easier. Instead of needing to have two interfaces open, the error message shows up right where the interaction fails, in the same browser window as the UI elements. Secondly, it makes it possible for us to use the error messages as a UI \"hack\" to inform users where their inputs (e.g. free text) might be invalid, thereby giving them informative error messages . (Try it out in the Beta distribution app: it'll give you an error message right below if you try to type something that cant be converted into a float!) The other key thing that Streamlit provides as a UI nice-ity is the ability to signal to end-users that a computation is happening. Streamlit does this in three ways, two of which always come for free. Firstly , if something is \"running\", then in the top-right hand corner of the page, the \"Running\" spinner will animate. Secondly , anything that is re-rendering will automatically be greyed out. Finally , we can use a special context manager to provide a custom message on the front-end: import streamlit as st with st . spinner ( \"Message goes here...\" ): # stuff happens So all-in-all, Streamlit seems to have a solution of some kind for the friction points that I have observed with Panel and Voila. Besides that, Streamlit, I think, uses a procedural paradigm, rather than a callback paradigm, for app construction. We just have to think of the app as a linear sequence of actions that happen from top to bottom. State is never really an issue, because every code change and interaction re-runs the source file from top to bottom, from scratch. When building quick apps, this paradigm really simplifies things compared to a callback-based paradigm. Finally, Streamlit also provides a convenient way to add text to the UI by automatically parsing as Markdown any raw strings unassigned to a variable in a .py file and rendering them as HTML. This opens the door to treating a .py file as a literate programming document , hosted by a Python-based server in the backend. It'd be useful especially in teaching scenarios. (With pyiodide bringing the PyData stack to the browser, I can't wait to see standalone .py files rendered to the DOM!) Now, this isn't to say that Streamlit is problem-free. There are still rough edges, the most glaring (as of today) in the current release is the inability to upload a file and operate on it. This has been fixed in a recent pull request , so I'm expecting this should show up in a new release any time soon. The other not-so-big-problem that I see with Streamlit at the moment is the procedural paradigm - by always re-running code from top-to-bottom afresh on every single change, apps that rely on long compute may need a bit more thought to construct, including the use of Streamlit's caching mechanism. Being procedural does make things easier for development though, and on balance, I would not discount Streamlit's simplicity here.","title":"Enter Streamlit"},{"location":"miscellaneous/dashboarding-landscape/#where-does-streamlit-fit","text":"As I see it, Streamlit's devs are laser-focused on enabling devs to very quickly get to a somewhat good-looking app prototype. In my experience, the development time for the Beta distribution app took about 3 hours, 2.5 of which were spent on composing prose. So effectively, I only used half an hour doing code writing, with a live and auto-reloading preview greatly simplifying the development process. (I conservatively estimate that this is about 1.5 times as fast as I would be using Panel.) Given Streamlit, I would use it to develop two classes of apps: (1) very tightly-focused utility apps that do one lightweight thing well, and (2) bespoke, single-document literate programming education material. I would be quite hesitant to build more complex things; then again, for me, that statement would be true more generally anyways with whatever tool. In any case, I think bringing UNIX-like thinking to the web is probably a good idea: we make little utilities/functional tools that can pipe standard data formats from to another.","title":"Where does Streamlit fit?"},{"location":"miscellaneous/dashboarding-landscape/#common-pain-points-across-all-three-dashboarding-tools","text":"A design pattern I have desired is to be able to serve up a fleet of small, individual utilities served up from the same codebase, served up by individual server processes, but all packaged within the same container. The only way I can think of at the moment is to build a custom Flask-based gateway to redirect properly to each utility's process. That said, I think this is probably out of scope for the individual dashboarding projects.","title":"Common pain points across all three dashboarding tools"},{"location":"miscellaneous/dashboarding-landscape/#how-do-we-go-forward","text":"The ecosystem is ever-evolving, and, rather than being left confused by the multitude of options available to us, I find myself actually being very encouraged at the development that has been happening. There's competing ideas with friendly competition between the developers, but they are also simultaneously listening to each other and their users and converging on similar things in the end. That said, I think it would be premature to go \"all-in\" on a single solution at this moment. For the individual data scientist, I would advise to be able to build something using each of the dashboarding frameworks. My personal recommendations are to know how to use: Voila + ipywidgets in a Jupyter notebook Panel in Jupyter notebooks and standalone .py files Streamlit in .py files. These recommendations stem mainly from the ability to style and layout content without needing much knowledge of HTML. In terms of roughly when to use what, my prior experience has been that Voila and Streamlit are pretty good for quicker prototypes, while Panel has been good for more complex ones, though in all cases, we have to worry about speed impacting user experience. From my experience at work, being able to quickly hash out key visual elements in a front-end prototype gives us the ability to better communicate with UI/UX designers and developers on what we're trying to accomplish. Knowing how to build front-ends ourselves lowers the communication and engineering barrier when taking a project to production. It's a worthwhile skill to have; be sure to have it in your toolbox!","title":"How do we go forward?"},{"location":"miscellaneous/dashboarding-landscape/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"miscellaneous/learning-to-learn/","text":"How I Learned to Learn In this essay, I'd like to reflect back on how I learned to learn new things. For a data scientist, it's impossible to know everything , but I do think that having a broad knowledge base can be very handy. Especially when confronted with a new problem class, having a broad toolkit of methods to solve it can give us a leg-up in terms of efficiency. This set of reflections hopefully lights up some lighbulbs for your own learning journey. Learning by doing/building/making \"Carve out time to reinvent the wheel, to learn about the wheel.\" One way that I think is very effective in learning new topics is to learn by making things from scratch. This trick, I believe, is particularly effective when learning about the foundational topics that underlie the API abstractions that we interact with as data scientists. For example, I learned quite a ton about architecting a deep learning library by trying to make one myself. The end result is fundl , a deep learning framework that I wrote that supports my own learning about the so-called fancy math that belies deep learning models. fundl fits in the \"model\", \"loss\", \"optimizer\" thought framework that I rely on for reasoning about deep learning models, and helps me focus on the \"model\" portion. In there, I have used it at work to re-implement models that I have seen implemented in other frameworks (e.g. PyTorch and TensorFlow), and translate them into the NumPy API. In doing so, I not only build familiarity with the models, but also gain familiarity with the other tensor library APIs, helping me to keep pace with framework development while also leveraging existing knowledge that I have (in the NumPy API). Through the process of implementing deep learning models, I have also found my mental model of linear algebra and data transformations has also grown. For example, I no longer am satisfied to think of a deep learning model in terms of an amorphous black box. Rather, thanks to reimplemtation, I am much more inclined to think about the model as doing some form of rotation and projection in n-dimensional space, which is exactly what dot products are all about. Thinking this way, I think, prevents a predisposition towards anthropomorphization of machine learning models, which is just a fancy term for ascribing human properties to models. Learning by teaching \"Having made the wheel, share how it was made.\" Teaching something is also an incredibly effective method to learn a new topic. I was able to learn graph theory during graduate school not only because I used it as a tool in my research, but also because I made teaching material in Python and brought it to conferences to share the knowledge. I think one of the key reasons why teaching is so unreasonably effective in learning is that it forces us to demonstrate our mastery over our knowledge in two ways. Firstly, in preparing the teaching material, we anticipate the questions that may arise from others. To address those questions, in turn, we must be prepared with knowledge deeper than what we have chosen to present. Secondly, any presentation of the material involves a linearization of a messy knowledge graph. In my conception, when I present material, I am tracing a path through the knowledge graph, while sprinkling in edges that branch off a main knowledge trunk. graph LR; A((A)) ==> B((B)); A((A)) --> C((C)); B((B)) ==> D((D)); C((C)) ==> E((E)); D((D)) ==> C((C)); B((B)) --> E((E)); D((D)) --> E((E)); The third point pertains to learning by teaching in quantitative topics. By forcing myself to \"teach\" the ultimate dumb student - a Python interpreter - to do math-y things, I not only make concrete an abstract topic, I also have to verify that the abstract topic is implemented correctly, because a Python interpreter will definitely get it wrong if I implemented it wrong. I've been incredibly fortunate to have a few platforms to do teaching, the primary one being the Python and data science conferences that I attend. That said, there are many avenues for teaching that you could take advantage of, including at work (1-on-1 pair coding or workshops), regional or international conferences, e-learning platforms, and more, and I would encourage you to leverage the platform that suits your situation best. Leveraging existing knowledge \"Pick projects that are adjacenct to what I know how to do.\" Continuing the \"knowledge graph\" analogy referenced above, I have made an effort in my learning journey to leverage as much existing knowledge that I can. It seems to me that knowledge is best picked up and made to stick when I can use one topic to anchor another, and vice versa. A few lightweight examples that have showed up in my learning journey: Connecting graph message passing with linear algebra Implementing Bayesian models from scratch but leveraging Python Digging into deep learning starting from linear regression In the process of leveraging my existing knowledge to learn new things, I find that tying the learning process to the creation of \"minimally complex examples\" greatly accelerates my own learning process. Minimally Complex Examples These are examples that are simple to grok, but not trivial. For example, it's trivial to illustrate sampling from a (multivariate) Gaussian distribution, which is how sampyl illustrates MCMC sampling on its docs page. However, it is non-trivial, and in fact quite illuminating, to illustrate sampling from a joint distribution of data, likelihood, and priors involving a Gaussian and its parameters. Seeking learning partners and teachers Learn and teach with others. I also have to state that I have benefited much from learning from others. For example, my primary deep learning teacher was David Duvenaud, back when he was a post-doc at Harvard. (He is now a professor at the University of Toronto.) It was from him through which I gained the framework of deep learning as \"model + loss + optimizer\", and if I remember correctly, he was the one that taught me how to think about linear regression in that exact framework. Additionally, a friend from amongst the PyMC developers, Colin Carroll, has been particularly helpful and inspiring. I read his blog in which he writes about his own learnings and insights. In particular, I very much appreciate how he uses \"minimal complex examples\" to illustrate how things work. He was also the one who kept reminding me that gradient descent doesn't happen in MCMC, which thus inspired the essay on MCMC . More generally, I find that identifying learning partners and teachers against whom I can check understanding is a great \"social\" strategy for picking up ideas. I generally try to find win-win scenarios, where I can offer something in exchange, as this helps balance out the learning partnership and makes it win-win for my fellow learner too. Asking the \"dumb\" questions One thing I do know I'm infamous for is asking dumb questions. By \"dumb\" questions, I mostly mean questions that clarify basic ideas that I might have missed, or still have a gap on. In my mind, there are very, very few dumb questions. (I would probably classify repetitively asking the same basic questions over and over as not being particularly smart - use a notebook for heaven's sake!) In a more intimate learning situation, say, a 1-on-1 session, clarifying basic questions as soon as they come up is a wonderful way of ensuring that our foundational knowledge is strengthened. In larger settings, I am almost always certain that someone else will share the same basic questions that I do. Concluding Words This was neither a comprehensive reflection on how exactly I learn nor a comprehensive overview of how everybody learns. Nonetheless, it is my hope that you find it useful to reflect on, and that it gives you ideas for learning new technical topics. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"How I Learned to Learn"},{"location":"miscellaneous/learning-to-learn/#how-i-learned-to-learn","text":"In this essay, I'd like to reflect back on how I learned to learn new things. For a data scientist, it's impossible to know everything , but I do think that having a broad knowledge base can be very handy. Especially when confronted with a new problem class, having a broad toolkit of methods to solve it can give us a leg-up in terms of efficiency. This set of reflections hopefully lights up some lighbulbs for your own learning journey.","title":"How I Learned to Learn"},{"location":"miscellaneous/learning-to-learn/#learning-by-doingbuildingmaking","text":"\"Carve out time to reinvent the wheel, to learn about the wheel.\" One way that I think is very effective in learning new topics is to learn by making things from scratch. This trick, I believe, is particularly effective when learning about the foundational topics that underlie the API abstractions that we interact with as data scientists. For example, I learned quite a ton about architecting a deep learning library by trying to make one myself. The end result is fundl , a deep learning framework that I wrote that supports my own learning about the so-called fancy math that belies deep learning models. fundl fits in the \"model\", \"loss\", \"optimizer\" thought framework that I rely on for reasoning about deep learning models, and helps me focus on the \"model\" portion. In there, I have used it at work to re-implement models that I have seen implemented in other frameworks (e.g. PyTorch and TensorFlow), and translate them into the NumPy API. In doing so, I not only build familiarity with the models, but also gain familiarity with the other tensor library APIs, helping me to keep pace with framework development while also leveraging existing knowledge that I have (in the NumPy API). Through the process of implementing deep learning models, I have also found my mental model of linear algebra and data transformations has also grown. For example, I no longer am satisfied to think of a deep learning model in terms of an amorphous black box. Rather, thanks to reimplemtation, I am much more inclined to think about the model as doing some form of rotation and projection in n-dimensional space, which is exactly what dot products are all about. Thinking this way, I think, prevents a predisposition towards anthropomorphization of machine learning models, which is just a fancy term for ascribing human properties to models.","title":"Learning by doing/building/making"},{"location":"miscellaneous/learning-to-learn/#learning-by-teaching","text":"\"Having made the wheel, share how it was made.\" Teaching something is also an incredibly effective method to learn a new topic. I was able to learn graph theory during graduate school not only because I used it as a tool in my research, but also because I made teaching material in Python and brought it to conferences to share the knowledge. I think one of the key reasons why teaching is so unreasonably effective in learning is that it forces us to demonstrate our mastery over our knowledge in two ways. Firstly, in preparing the teaching material, we anticipate the questions that may arise from others. To address those questions, in turn, we must be prepared with knowledge deeper than what we have chosen to present. Secondly, any presentation of the material involves a linearization of a messy knowledge graph. In my conception, when I present material, I am tracing a path through the knowledge graph, while sprinkling in edges that branch off a main knowledge trunk. graph LR; A((A)) ==> B((B)); A((A)) --> C((C)); B((B)) ==> D((D)); C((C)) ==> E((E)); D((D)) ==> C((C)); B((B)) --> E((E)); D((D)) --> E((E)); The third point pertains to learning by teaching in quantitative topics. By forcing myself to \"teach\" the ultimate dumb student - a Python interpreter - to do math-y things, I not only make concrete an abstract topic, I also have to verify that the abstract topic is implemented correctly, because a Python interpreter will definitely get it wrong if I implemented it wrong. I've been incredibly fortunate to have a few platforms to do teaching, the primary one being the Python and data science conferences that I attend. That said, there are many avenues for teaching that you could take advantage of, including at work (1-on-1 pair coding or workshops), regional or international conferences, e-learning platforms, and more, and I would encourage you to leverage the platform that suits your situation best.","title":"Learning by teaching"},{"location":"miscellaneous/learning-to-learn/#leveraging-existing-knowledge","text":"\"Pick projects that are adjacenct to what I know how to do.\" Continuing the \"knowledge graph\" analogy referenced above, I have made an effort in my learning journey to leverage as much existing knowledge that I can. It seems to me that knowledge is best picked up and made to stick when I can use one topic to anchor another, and vice versa. A few lightweight examples that have showed up in my learning journey: Connecting graph message passing with linear algebra Implementing Bayesian models from scratch but leveraging Python Digging into deep learning starting from linear regression In the process of leveraging my existing knowledge to learn new things, I find that tying the learning process to the creation of \"minimally complex examples\" greatly accelerates my own learning process. Minimally Complex Examples These are examples that are simple to grok, but not trivial. For example, it's trivial to illustrate sampling from a (multivariate) Gaussian distribution, which is how sampyl illustrates MCMC sampling on its docs page. However, it is non-trivial, and in fact quite illuminating, to illustrate sampling from a joint distribution of data, likelihood, and priors involving a Gaussian and its parameters.","title":"Leveraging existing knowledge"},{"location":"miscellaneous/learning-to-learn/#seeking-learning-partners-and-teachers","text":"Learn and teach with others. I also have to state that I have benefited much from learning from others. For example, my primary deep learning teacher was David Duvenaud, back when he was a post-doc at Harvard. (He is now a professor at the University of Toronto.) It was from him through which I gained the framework of deep learning as \"model + loss + optimizer\", and if I remember correctly, he was the one that taught me how to think about linear regression in that exact framework. Additionally, a friend from amongst the PyMC developers, Colin Carroll, has been particularly helpful and inspiring. I read his blog in which he writes about his own learnings and insights. In particular, I very much appreciate how he uses \"minimal complex examples\" to illustrate how things work. He was also the one who kept reminding me that gradient descent doesn't happen in MCMC, which thus inspired the essay on MCMC . More generally, I find that identifying learning partners and teachers against whom I can check understanding is a great \"social\" strategy for picking up ideas. I generally try to find win-win scenarios, where I can offer something in exchange, as this helps balance out the learning partnership and makes it win-win for my fellow learner too.","title":"Seeking learning partners and teachers"},{"location":"miscellaneous/learning-to-learn/#asking-the-dumb-questions","text":"One thing I do know I'm infamous for is asking dumb questions. By \"dumb\" questions, I mostly mean questions that clarify basic ideas that I might have missed, or still have a gap on. In my mind, there are very, very few dumb questions. (I would probably classify repetitively asking the same basic questions over and over as not being particularly smart - use a notebook for heaven's sake!) In a more intimate learning situation, say, a 1-on-1 session, clarifying basic questions as soon as they come up is a wonderful way of ensuring that our foundational knowledge is strengthened. In larger settings, I am almost always certain that someone else will share the same basic questions that I do.","title":"Asking the \"dumb\" questions"},{"location":"miscellaneous/learning-to-learn/#concluding-words","text":"This was neither a comprehensive reflection on how exactly I learn nor a comprehensive overview of how everybody learns. Nonetheless, it is my hope that you find it useful to reflect on, and that it gives you ideas for learning new technical topics.","title":"Concluding Words"},{"location":"miscellaneous/learning-to-learn/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"miscellaneous/pydata-landscape/","text":"An Opinionated and Unofficial Guide to the PyData Ecosystem Last Updated: 30 May 2020 This essay is the culmination of many years of learning to navigate the PyData ecosystem. Other data science ecosystems appear to have a major supportive entity, for example the R world has RStudio putting brain power and personnel into software and community development. Julia also has one, Julia Computing. The Python world, by contrast, seems a bit more fragmented, with a myriad of large and small groups; highly famed individuals and other individuals toiling quietly in the background; commercial and not-for-profit entities developing tools together, and more. What is considered \u201cstandard\u201d and perhaps the \u201cone and preferably only one way of doing something\u201d sometimes isn\u2019t entirely clear. In addition, given the age of the language and its growth during the Homo interconnectus era, ways of \u201cdoing things\u201d have evolved over time, and yet vestiges of the past lurk all over the place on the internet. For example, the matplotlib pylab API has been long deprecated, but a newcomer who stumbles upon an old StackOverflow Q&A may not know better if they see the now-deprecated line, from pylab import * . The combination of longevity and diversity means navigating the Python data science ecosystem can sometimes be intimidating for someone who is used to a more unified ecosystem. A colleague of mine who moved from Matlab to R found RStudio\u2019s open source approach refreshing (and yes, I really dig what they\u2019re doing as a Public Benefit Corporation too!), but when he dipped his toes into the Python world, he found the Python world a bit like the \u201cWild Wild West\u201d - confusing, to say the least. The keys to navigating the PyData world Personally, I think that one of the keys to navigating the PyData ecosystem is to discard the \u201cone vendor and tool to rule them all\u201d mentality, and to embrace the fact that you can glue (almost) anything you want together. In other words, to embrace the latent hacker inside of you . Every person is a hacker at heart; we MacGyver/improvise all the time, and it\u2019s no different when we do modern development of software. Another key to navigating the PyData world is to recognize that it is comprised of a collection of wonderfully talented individuals who are using the language to solve a myriad of problems, and that open source tools are often a byproduct of this creative energy. The corollary is that every tool is geared with a bias towards the particular problems that the author was originally solving. If you find your use cases are similar, that\u2019s great! And if you find that you use cases are not similar, the right way to approach the community is to either (1) work with package authors to improve their tool, (2) develop patches/workarounds around that tool, (3) or develop and release your own. Disparaging another tool just because it doesn't work the way you think it does should never be on your list of appropriate actions. I would call you an \u201cinsecure bro\u201d if you do that. How to use this essay? Alright, with those preface words in place, you probably came to this essay because you want a \u201cmap\u201d of where to go when you want to do stuff. That will be the structure of this essay; I will intentionally keep descriptions of tools and packages high-level. My goal here is to illuminate the variety of packages and their associated practical contexts in which they get used. I will structure the essay in terms of \"what should I use to do X\", or \"where should I go to do Y\". Use the right navigation bar to find the question that you're interested in having answered. If you can't find the exact thing, use the search bar above. And if you still can't find the thing you're interested in learning about, send me a question on the Issue Tracker for this repository! What do I do when I want to... What should I use to handle tabular/structured data? If your data are small enough to fit in RAM, pandas ](https://pandas.pydata.org/) should be your \u201cgo-to\u201d package. Its API has become a community idiom, and lots of derivative packages target the pandas API for drop-in compatibility. Also, it\u2019s just hit version 1.0! (Happy birthday, Pandas!) Amongst the derivative packages include: dask vaex both provide a scalable DataFrame API. modin also scales the pandas API. Building on top of the pandas API, I ported over the janitor R package functions to pyjanitor , to provide convenient data cleaning functions that one can \u201cchain\u201d. It\u2019s now grown to the point where multiple individuals have made their first open source contributions through the project! Which plotting packages should I use? matplotlib is the workhorse package that has been steadily evolving for a very long time! It was also the first one that I picked up for general purpose plotting. Because it provides a lot of low-level plotting components, it is extremely powerful and flexible. I also made my first open source contribution through the package, in which I helped rework the examples to use the pyplot API rather than the deprecated pylab API. Even today, I still use it for highly bespoke and customized plotting; its SVG output also makes for git diff-able figures too! For a convenient statistical visualization API, seaborn is where you would want to go. It provides the ability to do faceting, pair plots, and more, using a declarative API that interoperates nicely with pandas . To go to web plotting, Bokeh , HoloViews , Altair and Plotly are where you probably would want to head to. hvPlot is the sibling package to HoloViews, providing a high-level plotting API. Everything is moving quickly in the \u201cPython web plotting\u201d space, so the best way to invest in yourself is to know how to read code from each of the libraries. HoloViews and hvPlot are part of the pyviz ecosystem of packages supported and developed by Anaconda developers. (I \u2764\ufe0f what Anaconda has been doing all this while!) What tools should I use for dashboarding? The original thing I would have done to build a dashboard is to program a Flask app, write HTML templates and style it with custom CSS, and inject plot figures as base64-encoded strings or Bokeh plots. Thankfully, those days are over now! The key thing you\u2019ll want to look for in a dashboarding package is a set of widgets that you can program interactions on, as well as tools to display visualizations on the interface. Here\u2019s an overview of the tools that I\u2019ve worked with. Firstly, Voila builds upon the Jupyter ipywidgets ecosystem, and allows a data scientist to compose prose and widgets together while staying within the Jupyter notebook environment. Voila, in my opinion, is \u201c l\u2019original \u201d of dashboard tooling. Panel , which comes from the PyViz ecosystem of packages, uses the Bokeh ecosystem of widgets. This piece of history makes sense only in light of the packages being developed originally at Anaconda. The Panel devs, however, are now also building in ipywidgets interoperability. Panel allows you to move outside of the Jupyter notebook to build apps. Streamlit is another dashboard creation package that embraces the \u201cscripting\u201d mindset. Streamlit apps are essentially one long Python script that is continually executed from top to bottom (not unlike what we would expect from a Jupyter notebook). Streamlit apps give you an extremely fast path to realize your idea as a prototype; because of its idioms, it is my current \u201cgo-to\u201d tool. From personal experience, Streamlit has helped me go fast from idea to prototype. However, its programming idioms encourage a \u201cdump everything into the script\u201d programming style, which may make apps harder to maintain and modify long-term. Panel and Voila encourage the more modular style of programming, but that slows me down to getting a prototype stood up. It seems the tradeoff lies not in the package per se , but in the programming model for building UIs. The other package that I have not yet used is Plotly\u2019s Dash . From my colleagues\u2019 description, it plays very nicely with HTML standards, so for production systems that require branding because they are external, client-facing, this might be a great way to go. (I, on the other hand, happen to not work on data products that require a branding.) How do I deploy my dashboard app? Deploying a dashboard app usually means you need a place to host it separate from the machine on which you do your exploratory project work. The key idea here is to use a platform as a service, or to use a Docker container. If your company provides a PaaS, such as a Heroku-like service (they are the originals!), then you should be using it! With a PaaS, you declare the resources that you need for your app using configuration files, and let the PaaS system reproducibly build exactly what you've declared on every push to your master branch ! Doing things this way seriously simplifies the deployment of your dashboard app. If your company doesn't provide a PaaS, then I would recommend figuring out how to use Docker. Docker itself won't allow you to deploy the app on a separate computer (you need other tooling and possibly will have to work with your DevOps team to get containers deployed to the web), but it will at least get you a build of your project that is isolated from your development environment. What packages should I use for machine learning? Thankfully, the community has really coalesced around scikit-learn , with many packages going as far as adopting its API idioms. (Adopting and conforming to API idioms is a good thing! Don't let Oracle's lawsuit against Google fool you otherwise.) For the vast majority of ML problems (which don\u2019t really need deep learning methods), scikit-learn should be your go-to package. The ecosystem of packages that all adopt scikit-learn 's APIs include: XGBoost CatBoost TPOT : AutoML on top of scikit-learn PyCaret : low-code machine learning on top of scikit-learn -compatible packages. At this point, I usually default to PyCaret to get baseline models quickly set up. It is basically painless and automates many routine tasks that I would otherwise end up writing many lines of code for. What libraries do I use for tensor computing and deep learning? Those who know me well enough will know that I'm a fan of community standard APIs. So when it comes to Tensor computing, I naturally will first recommend the packages that play nicely with one another . Here, there are obvious ones: NumPy : the workhorse! Also basically evolving into a community-standard API. CuPy : NumPy API on GPUs Dask Arrays : Distributed CPU and GPU arrays JAX : NumPy on CPU, GPU and TPU, plus automatic differentiation as a first-class citizen! In particular, I want to highlight JAX. It's non-trivial to build a robust and general purpose automatic differentiation system that also works within the boundaries of known community idioms, but that is exactly what the JAX (and original autograd ) devs did. In fact, the guys who taught me deep learning are the JAX developers, for which I remain eternally thankful: anthropomorphisms of neural networks no longer capture my attention. Built on top of JAX include a number of neural network libraries whose semantics play very nicely with the NumPy API. This includes: stax : Built into jax, under jax.experimental.stax flax : Designed for flexibility trax : Built for speed. I believe DeepMind is the Google subgroup that built this. Regardless, though, there's still a smaller (but sizeable) group of people who use the deep learning frameworks quite productively, even though they do not have perfect interoperability with the NumPy API: PyTorch TensorFlow Gluon PyTorch and Chainer share a lineage, and if I understand history correctly, Chainer was the original. That said, PyTorch was backed by a behemoth tech firm in SV (Facebook), while Chainer was supported by a Japanese consulting firm (Preferred Networks), and we know that the monopolistic tendencies of Silicon Valley won out. The Chainer team at Preferred Networks have discontinued further development, instead providing CuPy and Optuna. What do I do if I need to do statistical inference? It\u2019s taken me many years of studying to finally reach this conclusion: it seriously pays to know the core concepts of \u201ceffect size\u201d and probability distributions. Those, more than significance tests, matter. Additionally, being able to describe data generating processes through the lens of probability distributions is extremely fundamental. All of this is a long-winded way to say, don't rely on canned statistics! Go learn it well. And if you are lucky enough to do so, learn it Bayesian first, or rather, from the lens of simulating data generating processes. Everything in statistics makes sense only in light of data generating distributions. Now, with that said, how do we do statistical inference, especially Bayesian statistical inference? PyMC3 is one of the two major probabilistic programming languages for Pythonistas, the other being Stan (through the PyStan interface). I personally have developed and contributed to PyMC3, and am extremely fond of being able to do Bayesian statistics without needing to learn another domain-specific language, so I have a biased interest in the package. It pairs with ArviZ , a package that provides visual diagnostics for Bayesian statistical analysis workflows. Theano, the tensor library that PyMC3 is built on top of, seems to have a few years more of life in its belt, though the compatibilities with modern operating systems are starting to show through the cracks. PyMC4 is the next generation of PyMC, and is built on top of Tensorflow Probability, and already has an alpha release made. Other probabilistic programming languages are also under constant development, not least one of which I'm paying close attention to being mcx , which is built on top of JAX! If you are well-versed in statistics, and you know the assumptions of the models you\u2019re using, and you\u2019re able to correctly interpret them, then statsmodels provides a unified interface to \u201cstandard\u201d statistical inference algorithms, including linear models, time series models, and more. As with all models, know the tools that you\u2019re wielding well enough to know when they can fail! What do I use if my data don't fit in RAM? In the PyData ecosystem, there are two major players: Dask and Spark. In my personal experience, Dask has been the more productive tool to use. It\u2019s easy to install, provides a near-perfect drop-in replacement for the NumPy and Pandas APIs, and is easy to get going without additional software on existing cluster systems (e.g. GridEngine). Coiled Computing , led by Dask creator Matthew Rocklin, is also apparently working on tooling to enable individual data scientists to scale to the cloud easily from a single laptop. I can\u2019t wait to see what happens there. That said, if you\u2019re stuck with Spark because of legacy systems, there\u2019s thankfully a not-so-bad ecosystem of packages in the Spark world. (Its API policy, though, plays less nicely with the rest of the FOSS PyData world.) NVIDIA has also been investing heavily in the PyData stack with a suite of GPU-accelerated tools, RAPIDS . In there are a suite of packages that are actively developed for GPU-accelerated DataFrames (cuDF), graph analytics (cuGraph), SQL query engines (BlazingSQL), machine learning (cuML), and more. Definitely worth keeping an eye out on. How should I obtain a Python? I\u2019ve deliberately worded it as \u201ca Python\u201d. If you\u2019re doing data science work, in my opinion, the best way to install Python is to grab miniconda , and ensure that each of your environments have their own conda environment. What are environments, and how should I structure/maintain them? There are basically two idiomatic ways. For data science users, conda probably should be your default, especially if you need access to non-Python libraries. For non-data science users, use of venv is probably a good way to go, though it only handles Python packages. Each project you work on really should have its own conda environment. The way to do this is to make sure each project lives in its own isolated directory on your filesystem, and contains an environment.yml file that fully specifies every single package you need to work on the project. If you want to take this one step further, use Docker containers! VSCode has the ability [to open any repository inside a \"development container\"][devcontainer]. I've test-driven this on some projects that I work on, including pyjanitor , notes on causal inference, Network Analysis Made Simple, and more. Which Integrated Development Environment (IDE) and/or text editor should I use? Which IDE/text editor you use is personal, but the general idea here is to \u201clearn one really well, and be dangerous enough with 2-3 more.\u201d Here\u2019s a partial list you can think about: IDEs: Visual Studio Code for software development. Their notebook support is also not bad. The remote extension lets you work really well. Also LiveShare works wonders. Finally, opening repositories on GitHub inside pre-defined \"development containers\" seriously simplifies the whole process of getting setup in a new computer. Jupyter Lab for notebook + software development. Notebooks are first-class citizens here. PyCharm . They have free licenses for open source developers. Sublime Text . I go to this one for times I need a fast text editor up and running. Personally, I stick with Jupyter and VSCode, though I\u2019ve picked up enough nano hacks to work with it in the terminal. More than anything, though, don\u2019t disparage anybody\u2019s use of their favorite text editor. At the same time, be ready to use the ones that allow collaborative text editing. Should I worry about git ? What tools around it should I use? The short answer is a resounding yes! The longer answer is as follows: Knowing how to use a version control system will make you more versatile as a data scientist. The biggest advantage of knowing a modern version control system is the ability to introduce a workflow that lets you experiment on branches isolated from a gold standard source of verified, tested, and \u201cknown-to-work\u201d collection code and notebook. Everything around that builds on this tooling. This includes the most important downstream feature of using git on the web: being able to automatically trigger builds of anything , including static sites, Docker development containers, documentation, and more. Once you embrace this workflow plus all of the automation surrounding it, you level up your skill level! Through whatever accidents of history, git has emerged as the de facto version control system for modern software and data science workflows. Knowing it will let you, a data scientist, work more effectively with your engineering counterparts. In using git , there is only one other tool I would recommend knowing, that is pre-commit . It gives you the ability to apply automatic code and notebook checks before you commit anything into the repository, thus preventing \u201cbad practices\u201d from seeping in, such as having large figures in your notebooks checked into your version control repository. Pre-commit will stop the committing from happening if code and/or notebook checks fail. Some tools like black and nbstripout will also automagically modify the offending files for you, so that when you do the commit once more after that, the checks will pass. How do I build documentation for my projects? Documentation is usually the last thing on everybody\u2019s minds with a project, but it is also the only thing that will scale you and expand the reach of your project. Investing in documentation, thus, increases your impact. Don\u2019t neglect it. With documentation, static site generators that produce standalone HTML documents are what you probably want to go for. This allows you to host the documentation on the internet (or your organization\u2019s intranet). (Where to host the HTML pages is a different question, this section only discusses how to generate them.) So what tools are available for documentation? The workhorse of the Python documentation world is Sphinx . It comes with a lot of features and extensions, and many developers swear by it. Many others, however, also swear a ton because of it, because it is complicated to learn, and the syntax is not easy to remember \u2014 especially if one doesn\u2019t write documentation every day like code! If you\u2019re willing to invest the time to learn, you\u2019ll be amazed at the power that Sphinx provides for building static HTML documentation. Think of Sphinx as the matplotlib of documentation: old, low-level, and extremely powerful. If you prefer Markdown documentation, then consider using MkDocs , which is an up-and-coming documentation system. Markdown is quite idiomatic to write, has a nice one-to-one correspondence to HTML, lets you interleave HTML, and as such makes writing documentation much simpler. An ecosystem of packages that you can mix-and-match are available to help with automatic parsing of docstrings (for API docs), and the mkdocs-material package provides very fancy ways to write device screen-responsive documentation with built-in static site search (no server required). Personally, I\u2019ve switched to writing all new docs using MkDocs and mkdocs material . (This site itself is built using mkdocs material!) There are some compatibility rough spots that I have seen, but that should not discourage you from trying it out. What do I do to solve my code copy/pasting problem? You\u2019ve found yourself copying and pasting code across notebooks, and now it\u2019s unsustainable to keep modifying them in 13 places. Writing a software library will help you consolidate your code in a single source of truth, which will let you update once to be applied everywhere the code is called. Python comes with all of the tools you need to make your own software library and import it into your notebooks and apps. There is an official Python Packaging Tutorial that you should refer to when building your own Python packages. How do I test my code? Running and writing tests may sound like something only Quality Assurance engineers should be doing, but because data scientists are writing code, having good software skills helps. You don\u2019t need to go far, you only need to have the basics on hand. unittest is built into the Python standard library, and you can get started pretty easily. On the other hand, pytest has become a very common testing package. The developers put a lot of thought into how the API is developed. As such, it is easy to get started with, extremely capable for complex situations, and runs everything in between very productively. To take your testing to the next level, add in Hypothesis to your list of packages to experiment with. Hypothesis has helped me find edge cases in my code that I never would have thought to find. The docs are also amazing, and it's easy to get started with on select parts of your code base (especially if you're not ready to commit all-in)! How do I guarantee the reproducibility of my notebooks? You\u2019ll want to know how to do this if you are writing notebooks that contain crucial analyses that you need to reproduce. Thankfully, there is one idiomatic way to handle this, and that is by using nbconvert . The nbconvert docs provide documentation on how to programmatically execute your notebooks from Python and the command line. Then, you'll want to subject your notebooks to \"continuous execution\". The general idea is to master a \u201ccontinuous pipeline runner\u201d. A few for open source projects include: Azure Pipelines Travis CI CircleCI Jenkins (your company may host it internally). If you develop the ability to work with git properly, then the entire world of automatic pipeline runners is at your fingertips! Almost every pipeline runner can be triggered on new commits to any pre-specified branches, and also come with the ability to execute checks automatically on any pull request branches, thereby guaranteeing that you have fully reproducible builds! What package should I use for network analysis? I would strongly recommend the NetworkX package! It provides a very consistent, easy-to-learn API, which lowers the barrier of entry to learning graph theory concepts. Together with Mridul Seth, I teach Network Analysis Made Simple at PyCon, SciPy and ODSC conferences. It's the culmination of our respective learning journeys, shared freely with everyone! We also have courses on DataCamp where you can learn how to use the NetworkX package in an interactive learning environment. NetworkX provides the data structures and algorithms for working with graphs in memory, but in case you want additional speed, the igraph package is written in C++ with Python and R bindings. What can I do to improve my code quality? There's a few things you can do! Firstly, use type hints in your function signatures. They help a code reader reason about the types of the objects that you pass into a function. Secondly, learn how to design software from the outside in. As my friend Jesse Johnson puts it, it is tempting to write software focusing on the most interesting piece first, but you end up making it difficult to write composable code. Thirdly, learn how to test your code! That will give you confidence that you've written correct code. It'll also train you to write software in a composable fashion. Was there something missing that you still have questions about? If so, head over to the issue tracker and post it there; don't forget to reference the essay! With thanks Special shout-out to my Patreon supporters, Eddie Janowicz, and Carol Willing, and Hector Munoz! Thank you for keeping me caffeinated and supporting my work that gets shared with the rest of the PyData community! Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"An Opinionated and Unofficial Guide to the PyData Ecosystem"},{"location":"miscellaneous/pydata-landscape/#an-opinionated-and-unofficial-guide-to-the-pydata-ecosystem","text":"Last Updated: 30 May 2020 This essay is the culmination of many years of learning to navigate the PyData ecosystem. Other data science ecosystems appear to have a major supportive entity, for example the R world has RStudio putting brain power and personnel into software and community development. Julia also has one, Julia Computing. The Python world, by contrast, seems a bit more fragmented, with a myriad of large and small groups; highly famed individuals and other individuals toiling quietly in the background; commercial and not-for-profit entities developing tools together, and more. What is considered \u201cstandard\u201d and perhaps the \u201cone and preferably only one way of doing something\u201d sometimes isn\u2019t entirely clear. In addition, given the age of the language and its growth during the Homo interconnectus era, ways of \u201cdoing things\u201d have evolved over time, and yet vestiges of the past lurk all over the place on the internet. For example, the matplotlib pylab API has been long deprecated, but a newcomer who stumbles upon an old StackOverflow Q&A may not know better if they see the now-deprecated line, from pylab import * . The combination of longevity and diversity means navigating the Python data science ecosystem can sometimes be intimidating for someone who is used to a more unified ecosystem. A colleague of mine who moved from Matlab to R found RStudio\u2019s open source approach refreshing (and yes, I really dig what they\u2019re doing as a Public Benefit Corporation too!), but when he dipped his toes into the Python world, he found the Python world a bit like the \u201cWild Wild West\u201d - confusing, to say the least.","title":"An Opinionated and Unofficial Guide to the PyData Ecosystem"},{"location":"miscellaneous/pydata-landscape/#the-keys-to-navigating-the-pydata-world","text":"Personally, I think that one of the keys to navigating the PyData ecosystem is to discard the \u201cone vendor and tool to rule them all\u201d mentality, and to embrace the fact that you can glue (almost) anything you want together. In other words, to embrace the latent hacker inside of you . Every person is a hacker at heart; we MacGyver/improvise all the time, and it\u2019s no different when we do modern development of software. Another key to navigating the PyData world is to recognize that it is comprised of a collection of wonderfully talented individuals who are using the language to solve a myriad of problems, and that open source tools are often a byproduct of this creative energy. The corollary is that every tool is geared with a bias towards the particular problems that the author was originally solving. If you find your use cases are similar, that\u2019s great! And if you find that you use cases are not similar, the right way to approach the community is to either (1) work with package authors to improve their tool, (2) develop patches/workarounds around that tool, (3) or develop and release your own. Disparaging another tool just because it doesn't work the way you think it does should never be on your list of appropriate actions. I would call you an \u201cinsecure bro\u201d if you do that.","title":"The keys to navigating the PyData world"},{"location":"miscellaneous/pydata-landscape/#how-to-use-this-essay","text":"Alright, with those preface words in place, you probably came to this essay because you want a \u201cmap\u201d of where to go when you want to do stuff. That will be the structure of this essay; I will intentionally keep descriptions of tools and packages high-level. My goal here is to illuminate the variety of packages and their associated practical contexts in which they get used. I will structure the essay in terms of \"what should I use to do X\", or \"where should I go to do Y\". Use the right navigation bar to find the question that you're interested in having answered. If you can't find the exact thing, use the search bar above. And if you still can't find the thing you're interested in learning about, send me a question on the Issue Tracker for this repository!","title":"How to use this essay?"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-when-i-want-to","text":"","title":"What do I do when I want to..."},{"location":"miscellaneous/pydata-landscape/#what-should-i-use-to-handle-tabularstructured-data","text":"If your data are small enough to fit in RAM, pandas ](https://pandas.pydata.org/) should be your \u201cgo-to\u201d package. Its API has become a community idiom, and lots of derivative packages target the pandas API for drop-in compatibility. Also, it\u2019s just hit version 1.0! (Happy birthday, Pandas!) Amongst the derivative packages include: dask vaex both provide a scalable DataFrame API. modin also scales the pandas API. Building on top of the pandas API, I ported over the janitor R package functions to pyjanitor , to provide convenient data cleaning functions that one can \u201cchain\u201d. It\u2019s now grown to the point where multiple individuals have made their first open source contributions through the project!","title":"What should I use to handle tabular/structured data?"},{"location":"miscellaneous/pydata-landscape/#which-plotting-packages-should-i-use","text":"matplotlib is the workhorse package that has been steadily evolving for a very long time! It was also the first one that I picked up for general purpose plotting. Because it provides a lot of low-level plotting components, it is extremely powerful and flexible. I also made my first open source contribution through the package, in which I helped rework the examples to use the pyplot API rather than the deprecated pylab API. Even today, I still use it for highly bespoke and customized plotting; its SVG output also makes for git diff-able figures too! For a convenient statistical visualization API, seaborn is where you would want to go. It provides the ability to do faceting, pair plots, and more, using a declarative API that interoperates nicely with pandas . To go to web plotting, Bokeh , HoloViews , Altair and Plotly are where you probably would want to head to. hvPlot is the sibling package to HoloViews, providing a high-level plotting API. Everything is moving quickly in the \u201cPython web plotting\u201d space, so the best way to invest in yourself is to know how to read code from each of the libraries. HoloViews and hvPlot are part of the pyviz ecosystem of packages supported and developed by Anaconda developers. (I \u2764\ufe0f what Anaconda has been doing all this while!)","title":"Which plotting packages should I use?"},{"location":"miscellaneous/pydata-landscape/#what-tools-should-i-use-for-dashboarding","text":"The original thing I would have done to build a dashboard is to program a Flask app, write HTML templates and style it with custom CSS, and inject plot figures as base64-encoded strings or Bokeh plots. Thankfully, those days are over now! The key thing you\u2019ll want to look for in a dashboarding package is a set of widgets that you can program interactions on, as well as tools to display visualizations on the interface. Here\u2019s an overview of the tools that I\u2019ve worked with. Firstly, Voila builds upon the Jupyter ipywidgets ecosystem, and allows a data scientist to compose prose and widgets together while staying within the Jupyter notebook environment. Voila, in my opinion, is \u201c l\u2019original \u201d of dashboard tooling. Panel , which comes from the PyViz ecosystem of packages, uses the Bokeh ecosystem of widgets. This piece of history makes sense only in light of the packages being developed originally at Anaconda. The Panel devs, however, are now also building in ipywidgets interoperability. Panel allows you to move outside of the Jupyter notebook to build apps. Streamlit is another dashboard creation package that embraces the \u201cscripting\u201d mindset. Streamlit apps are essentially one long Python script that is continually executed from top to bottom (not unlike what we would expect from a Jupyter notebook). Streamlit apps give you an extremely fast path to realize your idea as a prototype; because of its idioms, it is my current \u201cgo-to\u201d tool. From personal experience, Streamlit has helped me go fast from idea to prototype. However, its programming idioms encourage a \u201cdump everything into the script\u201d programming style, which may make apps harder to maintain and modify long-term. Panel and Voila encourage the more modular style of programming, but that slows me down to getting a prototype stood up. It seems the tradeoff lies not in the package per se , but in the programming model for building UIs. The other package that I have not yet used is Plotly\u2019s Dash . From my colleagues\u2019 description, it plays very nicely with HTML standards, so for production systems that require branding because they are external, client-facing, this might be a great way to go. (I, on the other hand, happen to not work on data products that require a branding.)","title":"What tools should I use for dashboarding?"},{"location":"miscellaneous/pydata-landscape/#how-do-i-deploy-my-dashboard-app","text":"Deploying a dashboard app usually means you need a place to host it separate from the machine on which you do your exploratory project work. The key idea here is to use a platform as a service, or to use a Docker container. If your company provides a PaaS, such as a Heroku-like service (they are the originals!), then you should be using it! With a PaaS, you declare the resources that you need for your app using configuration files, and let the PaaS system reproducibly build exactly what you've declared on every push to your master branch ! Doing things this way seriously simplifies the deployment of your dashboard app. If your company doesn't provide a PaaS, then I would recommend figuring out how to use Docker. Docker itself won't allow you to deploy the app on a separate computer (you need other tooling and possibly will have to work with your DevOps team to get containers deployed to the web), but it will at least get you a build of your project that is isolated from your development environment.","title":"How do I deploy my dashboard app?"},{"location":"miscellaneous/pydata-landscape/#what-packages-should-i-use-for-machine-learning","text":"Thankfully, the community has really coalesced around scikit-learn , with many packages going as far as adopting its API idioms. (Adopting and conforming to API idioms is a good thing! Don't let Oracle's lawsuit against Google fool you otherwise.) For the vast majority of ML problems (which don\u2019t really need deep learning methods), scikit-learn should be your go-to package. The ecosystem of packages that all adopt scikit-learn 's APIs include: XGBoost CatBoost TPOT : AutoML on top of scikit-learn PyCaret : low-code machine learning on top of scikit-learn -compatible packages. At this point, I usually default to PyCaret to get baseline models quickly set up. It is basically painless and automates many routine tasks that I would otherwise end up writing many lines of code for.","title":"What packages should I use for machine learning?"},{"location":"miscellaneous/pydata-landscape/#what-libraries-do-i-use-for-tensor-computing-and-deep-learning","text":"Those who know me well enough will know that I'm a fan of community standard APIs. So when it comes to Tensor computing, I naturally will first recommend the packages that play nicely with one another . Here, there are obvious ones: NumPy : the workhorse! Also basically evolving into a community-standard API. CuPy : NumPy API on GPUs Dask Arrays : Distributed CPU and GPU arrays JAX : NumPy on CPU, GPU and TPU, plus automatic differentiation as a first-class citizen! In particular, I want to highlight JAX. It's non-trivial to build a robust and general purpose automatic differentiation system that also works within the boundaries of known community idioms, but that is exactly what the JAX (and original autograd ) devs did. In fact, the guys who taught me deep learning are the JAX developers, for which I remain eternally thankful: anthropomorphisms of neural networks no longer capture my attention. Built on top of JAX include a number of neural network libraries whose semantics play very nicely with the NumPy API. This includes: stax : Built into jax, under jax.experimental.stax flax : Designed for flexibility trax : Built for speed. I believe DeepMind is the Google subgroup that built this. Regardless, though, there's still a smaller (but sizeable) group of people who use the deep learning frameworks quite productively, even though they do not have perfect interoperability with the NumPy API: PyTorch TensorFlow Gluon PyTorch and Chainer share a lineage, and if I understand history correctly, Chainer was the original. That said, PyTorch was backed by a behemoth tech firm in SV (Facebook), while Chainer was supported by a Japanese consulting firm (Preferred Networks), and we know that the monopolistic tendencies of Silicon Valley won out. The Chainer team at Preferred Networks have discontinued further development, instead providing CuPy and Optuna.","title":"What libraries do I use for tensor computing and deep learning?"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-if-i-need-to-do-statistical-inference","text":"It\u2019s taken me many years of studying to finally reach this conclusion: it seriously pays to know the core concepts of \u201ceffect size\u201d and probability distributions. Those, more than significance tests, matter. Additionally, being able to describe data generating processes through the lens of probability distributions is extremely fundamental. All of this is a long-winded way to say, don't rely on canned statistics! Go learn it well. And if you are lucky enough to do so, learn it Bayesian first, or rather, from the lens of simulating data generating processes. Everything in statistics makes sense only in light of data generating distributions. Now, with that said, how do we do statistical inference, especially Bayesian statistical inference? PyMC3 is one of the two major probabilistic programming languages for Pythonistas, the other being Stan (through the PyStan interface). I personally have developed and contributed to PyMC3, and am extremely fond of being able to do Bayesian statistics without needing to learn another domain-specific language, so I have a biased interest in the package. It pairs with ArviZ , a package that provides visual diagnostics for Bayesian statistical analysis workflows. Theano, the tensor library that PyMC3 is built on top of, seems to have a few years more of life in its belt, though the compatibilities with modern operating systems are starting to show through the cracks. PyMC4 is the next generation of PyMC, and is built on top of Tensorflow Probability, and already has an alpha release made. Other probabilistic programming languages are also under constant development, not least one of which I'm paying close attention to being mcx , which is built on top of JAX! If you are well-versed in statistics, and you know the assumptions of the models you\u2019re using, and you\u2019re able to correctly interpret them, then statsmodels provides a unified interface to \u201cstandard\u201d statistical inference algorithms, including linear models, time series models, and more. As with all models, know the tools that you\u2019re wielding well enough to know when they can fail!","title":"What do I do if I need to do statistical inference?"},{"location":"miscellaneous/pydata-landscape/#what-do-i-use-if-my-data-dont-fit-in-ram","text":"In the PyData ecosystem, there are two major players: Dask and Spark. In my personal experience, Dask has been the more productive tool to use. It\u2019s easy to install, provides a near-perfect drop-in replacement for the NumPy and Pandas APIs, and is easy to get going without additional software on existing cluster systems (e.g. GridEngine). Coiled Computing , led by Dask creator Matthew Rocklin, is also apparently working on tooling to enable individual data scientists to scale to the cloud easily from a single laptop. I can\u2019t wait to see what happens there. That said, if you\u2019re stuck with Spark because of legacy systems, there\u2019s thankfully a not-so-bad ecosystem of packages in the Spark world. (Its API policy, though, plays less nicely with the rest of the FOSS PyData world.) NVIDIA has also been investing heavily in the PyData stack with a suite of GPU-accelerated tools, RAPIDS . In there are a suite of packages that are actively developed for GPU-accelerated DataFrames (cuDF), graph analytics (cuGraph), SQL query engines (BlazingSQL), machine learning (cuML), and more. Definitely worth keeping an eye out on.","title":"What do I use if my data don't fit in RAM?"},{"location":"miscellaneous/pydata-landscape/#how-should-i-obtain-a-python","text":"I\u2019ve deliberately worded it as \u201ca Python\u201d. If you\u2019re doing data science work, in my opinion, the best way to install Python is to grab miniconda , and ensure that each of your environments have their own conda environment.","title":"How should I obtain a Python?"},{"location":"miscellaneous/pydata-landscape/#what-are-environments-and-how-should-i-structuremaintain-them","text":"There are basically two idiomatic ways. For data science users, conda probably should be your default, especially if you need access to non-Python libraries. For non-data science users, use of venv is probably a good way to go, though it only handles Python packages. Each project you work on really should have its own conda environment. The way to do this is to make sure each project lives in its own isolated directory on your filesystem, and contains an environment.yml file that fully specifies every single package you need to work on the project. If you want to take this one step further, use Docker containers! VSCode has the ability [to open any repository inside a \"development container\"][devcontainer]. I've test-driven this on some projects that I work on, including pyjanitor , notes on causal inference, Network Analysis Made Simple, and more.","title":"What are environments, and how should I structure/maintain them?"},{"location":"miscellaneous/pydata-landscape/#which-integrated-development-environment-ide-andor-text-editor-should-i-use","text":"Which IDE/text editor you use is personal, but the general idea here is to \u201clearn one really well, and be dangerous enough with 2-3 more.\u201d Here\u2019s a partial list you can think about: IDEs: Visual Studio Code for software development. Their notebook support is also not bad. The remote extension lets you work really well. Also LiveShare works wonders. Finally, opening repositories on GitHub inside pre-defined \"development containers\" seriously simplifies the whole process of getting setup in a new computer. Jupyter Lab for notebook + software development. Notebooks are first-class citizens here. PyCharm . They have free licenses for open source developers. Sublime Text . I go to this one for times I need a fast text editor up and running. Personally, I stick with Jupyter and VSCode, though I\u2019ve picked up enough nano hacks to work with it in the terminal. More than anything, though, don\u2019t disparage anybody\u2019s use of their favorite text editor. At the same time, be ready to use the ones that allow collaborative text editing.","title":"Which Integrated Development Environment (IDE) and/or text editor should I use?"},{"location":"miscellaneous/pydata-landscape/#should-i-worry-about-git-what-tools-around-it-should-i-use","text":"The short answer is a resounding yes! The longer answer is as follows: Knowing how to use a version control system will make you more versatile as a data scientist. The biggest advantage of knowing a modern version control system is the ability to introduce a workflow that lets you experiment on branches isolated from a gold standard source of verified, tested, and \u201cknown-to-work\u201d collection code and notebook. Everything around that builds on this tooling. This includes the most important downstream feature of using git on the web: being able to automatically trigger builds of anything , including static sites, Docker development containers, documentation, and more. Once you embrace this workflow plus all of the automation surrounding it, you level up your skill level! Through whatever accidents of history, git has emerged as the de facto version control system for modern software and data science workflows. Knowing it will let you, a data scientist, work more effectively with your engineering counterparts. In using git , there is only one other tool I would recommend knowing, that is pre-commit . It gives you the ability to apply automatic code and notebook checks before you commit anything into the repository, thus preventing \u201cbad practices\u201d from seeping in, such as having large figures in your notebooks checked into your version control repository. Pre-commit will stop the committing from happening if code and/or notebook checks fail. Some tools like black and nbstripout will also automagically modify the offending files for you, so that when you do the commit once more after that, the checks will pass.","title":"Should I worry about git? What tools around it should I use?"},{"location":"miscellaneous/pydata-landscape/#how-do-i-build-documentation-for-my-projects","text":"Documentation is usually the last thing on everybody\u2019s minds with a project, but it is also the only thing that will scale you and expand the reach of your project. Investing in documentation, thus, increases your impact. Don\u2019t neglect it. With documentation, static site generators that produce standalone HTML documents are what you probably want to go for. This allows you to host the documentation on the internet (or your organization\u2019s intranet). (Where to host the HTML pages is a different question, this section only discusses how to generate them.) So what tools are available for documentation? The workhorse of the Python documentation world is Sphinx . It comes with a lot of features and extensions, and many developers swear by it. Many others, however, also swear a ton because of it, because it is complicated to learn, and the syntax is not easy to remember \u2014 especially if one doesn\u2019t write documentation every day like code! If you\u2019re willing to invest the time to learn, you\u2019ll be amazed at the power that Sphinx provides for building static HTML documentation. Think of Sphinx as the matplotlib of documentation: old, low-level, and extremely powerful. If you prefer Markdown documentation, then consider using MkDocs , which is an up-and-coming documentation system. Markdown is quite idiomatic to write, has a nice one-to-one correspondence to HTML, lets you interleave HTML, and as such makes writing documentation much simpler. An ecosystem of packages that you can mix-and-match are available to help with automatic parsing of docstrings (for API docs), and the mkdocs-material package provides very fancy ways to write device screen-responsive documentation with built-in static site search (no server required). Personally, I\u2019ve switched to writing all new docs using MkDocs and mkdocs material . (This site itself is built using mkdocs material!) There are some compatibility rough spots that I have seen, but that should not discourage you from trying it out.","title":"How do I build documentation for my projects?"},{"location":"miscellaneous/pydata-landscape/#what-do-i-do-to-solve-my-code-copypasting-problem","text":"You\u2019ve found yourself copying and pasting code across notebooks, and now it\u2019s unsustainable to keep modifying them in 13 places. Writing a software library will help you consolidate your code in a single source of truth, which will let you update once to be applied everywhere the code is called. Python comes with all of the tools you need to make your own software library and import it into your notebooks and apps. There is an official Python Packaging Tutorial that you should refer to when building your own Python packages.","title":"What do I do to solve my code copy/pasting problem?"},{"location":"miscellaneous/pydata-landscape/#how-do-i-test-my-code","text":"Running and writing tests may sound like something only Quality Assurance engineers should be doing, but because data scientists are writing code, having good software skills helps. You don\u2019t need to go far, you only need to have the basics on hand. unittest is built into the Python standard library, and you can get started pretty easily. On the other hand, pytest has become a very common testing package. The developers put a lot of thought into how the API is developed. As such, it is easy to get started with, extremely capable for complex situations, and runs everything in between very productively. To take your testing to the next level, add in Hypothesis to your list of packages to experiment with. Hypothesis has helped me find edge cases in my code that I never would have thought to find. The docs are also amazing, and it's easy to get started with on select parts of your code base (especially if you're not ready to commit all-in)!","title":"How do I test my code?"},{"location":"miscellaneous/pydata-landscape/#how-do-i-guarantee-the-reproducibility-of-my-notebooks","text":"You\u2019ll want to know how to do this if you are writing notebooks that contain crucial analyses that you need to reproduce. Thankfully, there is one idiomatic way to handle this, and that is by using nbconvert . The nbconvert docs provide documentation on how to programmatically execute your notebooks from Python and the command line. Then, you'll want to subject your notebooks to \"continuous execution\". The general idea is to master a \u201ccontinuous pipeline runner\u201d. A few for open source projects include: Azure Pipelines Travis CI CircleCI Jenkins (your company may host it internally). If you develop the ability to work with git properly, then the entire world of automatic pipeline runners is at your fingertips! Almost every pipeline runner can be triggered on new commits to any pre-specified branches, and also come with the ability to execute checks automatically on any pull request branches, thereby guaranteeing that you have fully reproducible builds!","title":"How do I guarantee the reproducibility of my notebooks?"},{"location":"miscellaneous/pydata-landscape/#what-package-should-i-use-for-network-analysis","text":"I would strongly recommend the NetworkX package! It provides a very consistent, easy-to-learn API, which lowers the barrier of entry to learning graph theory concepts. Together with Mridul Seth, I teach Network Analysis Made Simple at PyCon, SciPy and ODSC conferences. It's the culmination of our respective learning journeys, shared freely with everyone! We also have courses on DataCamp where you can learn how to use the NetworkX package in an interactive learning environment. NetworkX provides the data structures and algorithms for working with graphs in memory, but in case you want additional speed, the igraph package is written in C++ with Python and R bindings.","title":"What package should I use for network analysis?"},{"location":"miscellaneous/pydata-landscape/#what-can-i-do-to-improve-my-code-quality","text":"There's a few things you can do! Firstly, use type hints in your function signatures. They help a code reader reason about the types of the objects that you pass into a function. Secondly, learn how to design software from the outside in. As my friend Jesse Johnson puts it, it is tempting to write software focusing on the most interesting piece first, but you end up making it difficult to write composable code. Thirdly, learn how to test your code! That will give you confidence that you've written correct code. It'll also train you to write software in a composable fashion.","title":"What can I do to improve my code quality?"},{"location":"miscellaneous/pydata-landscape/#was-there-something-missing-that-you-still-have-questions-about","text":"If so, head over to the issue tracker and post it there; don't forget to reference the essay!","title":"Was there something missing that you still have questions about?"},{"location":"miscellaneous/pydata-landscape/#with-thanks","text":"Special shout-out to my Patreon supporters, Eddie Janowicz, and Carol Willing, and Hector Munoz! Thank you for keeping me caffeinated and supporting my work that gets shared with the rest of the PyData community!","title":"With thanks"},{"location":"miscellaneous/pydata-landscape/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"miscellaneous/static-sites-on-dokku/","text":"Static Sites and Apps On Your Own Dokku Server Summary: In this essay, I'm going to share with you how you can deploy your own static sites and apps on a Dokku server. Introduction You've worked on this awesome Streamlit app, or a Panel dashboard, or a Plotly Dash web frontend for your data science work, and now you've decided to share the work. Or you've built documentation for the project, and you need to serve it up. Except, if your company doesn't have a dedicated platform for apps, you're stuck! That's because you've now got to share it from your laptop/workstation and point your colleagues to there (right... go ahead and email them a link to http://10.16.213.24:8501 right now...) and keep your computer running perpetually to serve the app in order for them to interact with it. Or, you copy/paste the docs into a separate hosted solution, and now the docs are divorced from your code, leading to documentation rot in the long-run because it's too troublesome to maintain two things simultaneously. That's much too fragile. What you need is another hosting machine that isolated from your development machine, so that you can develop in peace while the hosting machine reliably serves up the working, stable version of your work. The specific thing that you really need is a \"Platform as a Service\". There's a lot of commercial offerings, but if you're \"cheap\" and don't mind learning some new concepts to help you get around the web, then this essay is for you. Here, I'm going to show you how to configure and deploy Dokku as a personal PaaS solution that you can use at work and for hobby projects. I'm then going to show you how to deploy a static site (which can be useful for hosting blogs and documentation), and finally I'll show you how to deploy a Streamlit app, which you can use to show off a front-end to your fancy modelling work. Along the way, I hope to also point out the \"generalizable\" ideas behind the steps listed here, and give you a framework (at least, one useful one) for building things on the web. But, but why? If you're part of a company... Your organization might not be equipped with modern PaaS tools that will enable you, a data scientist, to move quickly from local prototype to share-able prototype. If, however, you have access to bare metal cloud resources (i.e. just gimme a Linux box!), then as a hacker-type, you might be able to stand up your own PaaS and help demonstrate to your infrastructure team the value of having one. If you're doing this for your hobby projects... You might be as cheap as I am, but need a bit more beefy power than the restrictions given to you on Heroku (512MB RAM, 30 minute timeouts, and limited number of records in databases), and you don't want to pay $7/month for each project. Additionally, you might want a bit more control over your hosting options, but you don't feel completely ready to go fiddling with containers and networking without a software stack to help out just a bit . More generally... You might like the idea of containers, but find it kind of troublesome to learn yet another thing that's not trivial to configure, execute and debug (i.e. Docker). Dokku can be a bridge to get you there, as it automates much of the workflow surrounding Docker containers. It also comes with an API that both matches closely to Heroku (which is famous for being very developer-friendly) and also helps you handle proxy port mapping and domains easily. Are you ready? Let's go! Pre-requisites I'm assuming you know how to generate and use SSH keys to remotely access another machine. This is an incredibly useful thing to know how to do, and so I'd recommend that you pick this skill up. (As usual, DigitalOcean's community pages have a great tutorial .) I am also assuming that you have access to a Linux box of some kind with an IP address that you can expose to the \"internet\". The \"internet\" in this case can mean the world wide web if you're working on a personal project, or your organization's \"intranet\" if you're planning on only letting those in your organization access the sites and apps that you will build. I'm also assuming familiarity with git , which I consider to be an indispensable tool in a data scientist's toolkit. This last point is not an assumption, but an exhortation: you should be building your data app prototype in accordance to 12-factor app principles. It'll make (most of) your Engineering colleagues delight in working with you. (Yes, there are some esoteric types that don't subscribe to 12 factor app principles...) If you've never heard of it, go check it out here . It's a wonderful read that will change how you build your data app prototypes for the better. It will also make handing over the app to Engineering much easier in the long-run, improving your relationship with the Engineering team that will take care of your app! Set up a Box in the Cloud (optional if you have one) If you don't already have another computer that you have access to, or if you're curious on how to get set up in the cloud, then follow along these instructions. To make things a bit more concrete, I'm going to rent a server on DigitalOcean (DO). If you don't have a DigitalOcean account, feel free to use my referral link to sign up (and get free $100 credits that you can spend over 60 days) if you haven't got a DigitalOcean account already. (Disclaimer: I benefit too, but your support helps me make more of this kind of content!) Once you've signed up and logged into the DigitalOcean cloud dashboard, you can now set up a new Droplet. Droplets? \"Droplet\" is nothing more than DO's fancy name for a cloud server, or basically a computer that they're running. To do so, click on the big green \"Create\" button, set up a Droplet with the following settings: Ubuntu operating system \"Standard\" plan at $5/mo or $10/mo 1 Additional options: \"monitoring\" Authentication: you should use SSH keys (this is a pre-requisite for this essay). Hostname: Give it something memorable. Backups: Highly recommended. Give yourself peace of mind that you can rollback anytime. Once you're done with that, hit the big green \"Create Droplet\" button right at the bottom of the screen! Once your droplet is set up, you can go ahead and click on the \"Manage > Droplets\" left menu item, and that will bring you to a place where you can see all of your rented computers. The Cloud FYI, that's all the cloud is: renting someone else's computers. It turns out to be a pretty lucrative business! And because it's just someone else's computers, don't think of it as something magical. Setup Dokku on your Shiny Rented Machine Let's now go ahead and set up Dokku on your shiny new droplet. Run the Dokku installation commands Dokku installation on Ubuntu is quite easy; the following instructions are taken from the Dokku docs . SSH into your machine, then type the following: wget https://raw.githubusercontent.com/dokku/dokku/v0.20.4/bootstrap.sh ; # CHECK FOR LATEST DOKKU VERSION! # http://dokku.viewdocs.io/dokku/getting-started/installation/ sudo DOKKU_TAG = v0.20.4 bash bootstrap.sh Wrap up Dokku installation If you're on an Ubuntu installation, you'll want to navigate to your Linux box's IP address, and finish up the installation instructions, which involves letting your Dokku installation know about your SSH public keys. These keys are important, as the are literally the keys to letting you push your app to your Dokku box! SSH Keys and Dokku More pedantically, these SSH keys identify your machine as being \"authorized\" to push to the git server that is running on your Dokku machine. This is what allows you to git push dokku master later on! For those of you who have CentOS or other flavours of Linux, you will need to follow analogous instructions on the Dokku website. I have had experience following the CentOS instructions at work, and had to modify the installation commands a little bit to work with our internal proxies. Test that Dokku is working To test that your Dokku installation is working, type the following command: dokku help That should show the Dokku help menu, and you'll know the installation has completed successfully! Optionally set proxies for Docker Now, because Dokku builds upon Docker, if you're behind a corporate proxy, you might need to configure your Docker daemon proxies as well. You'll then want to follow instructions on the Docker daemon documentation . Those steps will generally be the same as what's in the docs, though the specifics will change (e.g. your proxy server address). Configure domain names The allure of Heroku is that it gives your app a name: myapp.herokuapp.com , or myshinyapp.herokuapp.com quite automagically. With Dokku and a bit of extra legwork, we can replicate this facility. We're going to set up your Dokku box uch that its main domain will be mydomain.com , and apps will get a subdomain myapp.mydomain.com . Register a domain name Firstly, you'll need a domain name from a Domain Name Service (DNS) registrar. Cloudflare seems to be doing all of the right things at the moment, so their domain name registration service is something I'm not hesitant to recommend (at least as of 2020). For historical reasons, I'm currently using Google Domains. At work, we have an internal service that lets us register an internal domain name. What matters most is that you have the ability to assign a domain name that points to your Dokku machine's IP address. Go ahead and register a domain name that reflects who \"you\" are on the web. For myself, I have a personal domain name that I use. At work, I registered a name that reflects the research group that I work in. Make sure that the name \"points\"/\"forwards\" to the IP address of your Dokku box. Enable subdomains! To enable the ability to use subdomains like myapp.mydomain.com for each app, you'll want to also configure the DNS settings. On your domain registrar, look for the place where you can customize \"resource records\". On Google Domains, it's under \"DNS > Custom resource records\". There, you'll want to add an \"A\" record (as opposed to other options that you might see, like \"CNAME\", \"AAAA\", and others). The \"name\" should be * , literally just an asterisk. The IPv4 address should point to your Dokku machine. This is all that is needed. What is an 'A' record? The most comprehensive explainer I've seen is at dnsimple , but the long story short is that it is an \"Address\" record. Yes, \"A\" stands for \"Address\", and it is nothing more than a pointer that maps \"string\" address to IP address. What then about the name * ? What we just did up there was to say, point everything *.mydomain.com to to the Dokku box IP address. How then do we get subdomains if we don't configure them with our DNS? Well, the secret here is that Dokku can help us with subdomains. Read on for how configuration of your Dokku box! To test whether your domain name is setup correctly, head over to the domain in your favourite browser. At this point, you should see the default NGINX landing page, as you have no apps deployed and no domains configured. How do you pronounce 'NGINX'? The official way is \"engine-X\". The wrong way is \"en-jinx\". Don't get it wrong! And what is NGINX? From Wikipedia : Nginx is a web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. For our purposes, we treat it as a thing that routes URLs to containers. Deploy a test app Heroku provides a \"Python getting started\" repository that we will use to check that the installation is working correctly. This one deploys reliably with all of the vanilla commands entered. Leveraging this, I will also show you how to leverage your * A record to put in nice subdomains! Clone the test app Firstly, git clone Heroku's python-getting-started repository to your laptop/local machine (i.e. not your Dokku box). Next, cd into the repository: cd python-getting-started After that, add your Dokku box as a git remote to the repository: git remote add dokku dokku@your-domain-name:python-getting-started Be sure to replace your-domain-name with your newfangled domain that you registered. App name The python-getting-started after the colon is the \"app name\" that you will see at the command line when interacting with Dokku later. Now, push the app to your Dokku box! git push dokku master Unlike your usual pushes to GitHub, GitLab or Bitbucket, you'll see a series of remote outputs being beamed back to your terminal. What's happening here is the build of the app! In particular, a Docker build is happening behind-the-scenes, so your app is completely self-contained and containerized on the Dokku box! If everything went well, the last output beamed back to you should look like: ===== > Application deployed: http://mydomain.com:10161 Wonderful! Now let's go back to Dokku and configure your app. Configure the app domain and ports We're now going to configure Dokku to recognize which subdomains should point to which apps. Firstly, get familiar with the Dokku domains command: # On your Dokku box dokku domains:help That should list out all of the Dokku domains sub-commands. Usage: dokku domains [ :COMMAND ] Manage domains used by the proxy Additional commands: domains:add <app> <domain> [ <domain> ... ] Add domains to app domains:add-global <domain> [ <domain> ... ] Add global domain names domains:clear <app> Clear all domains for app domains:clear-global Clear global domain names domains:disable <app> Disable VHOST support domains:enable <app> Enable VHOST support domains:remove <app> <domain> [ <domain> ... ] Remove domains from app domains:remove-global <domain> [ <domain> ... ] Remove global domain names domains:report [ <app> | --global ] [ <flag> ] Displays a domains report for one or more apps domains:set <app> <domain> [ <domain> ... ] Set domains for app domains:set-global <domain> [ <domain> ... ] Set global domain names You can report domains used for the app, python-getting-started : # On your Dokku box dokku domains:report python-getting-started The output should look something like this: $ dokku domains:report python-getting-started ===== > python-getting-started domains information Domains app enabled: false Domains app vhosts: Domains global enabled: false Domains global vhosts: This tells us that python-getting-started has no domains configured for it. We can now set it: # On your Dokku box dokku domains:set python-getting-started python-getting-started.mydomain.com The output will look like this: -----> Added python-getting-started.mydomain.com to python-getting-started -----> Configuring python-getting-started.mydomain.com... ( using built-in template ) -----> Creating http nginx.conf Reloading nginx Now, you should be able to go to http://python-getting-started.mydomain.com , and the page that gets loaded should be the \"Getting Started with Python on Heroku\" landing page! So, what magic happened here? What's happening here is that NGINX resolving subdomains to particular containers, and mapping them to the appropriate container port that is being exposed. If everything deployed correctly up till this point, you're good to go with deploying a data app on your Dokku machine! Deploy your data app Deploying the python-getting-started app should have given you: the confidence that your Dokku installation is working correctly, firsthand experience configuring Dokku, a taste of the workflow for deploying an app. Now, we're going to apply that to a Streamlit app. I've chosen Streamlit because it's got the easiest programming model amongst all of the Dashboard/app development frameworks that I've seen; in fact, I was able to stand up an explainer on the Beta distribution in under 3 hours, the bulk of which was spent on writing prose, not figuring out how to program with Streamlit. Build a streamlit app (skip if you already have an app) If you don't have a Streamlit app, here's one that you can use as a starter, which simply displays some text and a button: # app.py import streamlit as st \"\"\" # First Streamlit App! This is a dummy streamlit app. \"\"\" finished = st . button ( \"Click me!\" ) if finished : st . balloons () Save it as app.py in your project directory. Now, you can run the app with Streamlit: # On your local machine streamlit run app.py You should see the following show up in your terminal: You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://<your_ip_address>:8501 You can go to the local URL and confirm that the app is running correctly, and that it does exactly what's expected. Add project-specific configuration files for Dokku Now, we need to add a few configuration files that Dokku will recognize. requirements.txt Firstly, make sure you have a requirements.txt file in the project root directory, in which you specify all of the requirements for your app to run. # requirements.txt streamlit==0.57.3 # pinning version numbers is good for apps. # put more below as necessary, e.g.: numpy==0.16 With a requirements.txt file, Dokku (and Heroku) will automagically recognize that you have a Python app. Dokku will then create a Docker container equipped with Python, and install all of the dependencies in there. Declarative configuration FTW! Procfile Next, you need a Procfile in the project root directory: # Procfile web: streamlit run app.py Procfile A quick note: It is Procfile , with no file extensions. Don't save it as Procfile.txt , because that will not get recognized by Dokku/Heroku. To learn more, read about it on Heroku . The general syntax in the Procfile is: <process_type>: <command> The command is always a single line, and tells Dokku/Heroku what commands to execute in order to run the app. In our case, we simply execute the same command that we used to run the app locally for development purposes. More complex Procfile commands You can have multiple bash commands in a single line, though if it gets complicated, you may want to extract the commands out into a separate bash script that you execute instead, e.g.: # dokku.sh pip install -e . export CONFIG_DIR=\"/path/to/config/files\" streamlit run app.py For the process_type , in the case of Dokku, is always \"web\". Heroku, on the other hand, can handle other process types. Since we're dealing with Dokku and not Heroku, don't bother changing process_type . Configure git Remote with Dokku Now, let's configure your Dokku remote. # On your local machine git remote add dokku dokku@mydomain.com:streamlit-app Remember two points! Firstly, change mydomain.com to your domain. Secondly, you can use any app name you want, it doesn't have to be streamlit-app . A convention that has helped me is to have a 1:1 mapping between app and project repository folder name. It means one less thing to be confused about. Once you're done configuring the remote, now push it up! # On your local machine git push dokku master The same build commands will take place. While they are taking place, go ahead and open a new Terminal, and SSH into the Dokku box. We're going to configure the new app on Dokku! Configure Dokku Subdomain Let's start with the subdomain name first. For this tutorial, I'm going to use the domain name streamlit-app.mydomain.com . Let's configure the app streamlit-app with that domain name: # On Dokku box dokku domains:set streamlit-app streamlit-app.mydomain.com Configure Dokku port mapping Next, we have to configure the port mapping that Dokku's proxy server will recognize. By default, every container has the \"hosting box\" (i.e. the machine Dokku is running on) port 80 mapped to \"container box\" (i.e. the container the app is running on) port 5000. You can see this with: # On Dokku box dokku proxy:report streamlit-app That will give you something like: ===== > python-getting-started proxy information Proxy enabled: true Proxy port map: http:80:5000 Proxy type: nginx Now, because streamlit is going to be run on port 8501 (in the container) by default, we need to change the port mapping from http:80:5000 to http:80:8501 . To do so: # On Dokku box dokku proxy:ports-set streamlit-app http:80:8501 Putting these two configurations together, i.e. setting the subdomain and port mapping, we have now told Dokku, \"Each time you get a request from http://streamlit-app.mydomain.com , forward it to port 8501 on the streamlit-app container.\" Test it out! Well, we now can test it out. Go ahead and head over to your app URL, and see if the app works for you! Debugging If things look like they're crashing, how do you debug? Well, you always should know how to look at the logs: dokku logs my_app_name -t That will keep the logs updating in the terminal as you refresh the page. Use the information in the logs to help you debug. Also, see if you can reproduce the error in the logs locally. Additionally, if you get nginx errors, you can look at the nginx logs to help you debug proxy errors as they show up: dokku nginx:access-logs my_app_name -t Look at the logs and dig through for anything that might help you with your Google searches. Follow this pattern, and soon enough, you'll become an expert at debugging your web apps! Deploy a static site Now that you've seen how to deploy an app that's powered by a container behind-the-scenes, let's now figure out how to deploy a static site that is built upon every deploy. It's essentially the same. We have configuration files (in this case, slightly different ones) that declare what kind of environment we need. We basically treat the static site generator as an \"app\" that generates the HTML pages that we serve up freshly on each build. For this example, I'm going to use mkdocs , as it is also easy to use to build sites, and can be extended with some pretty awesome templates (like mkdocs-material ) for responsive docs generated from Markdown files. If you've got another static site builder (I have used Lektor , sphinx , and Nikola before), the places where we use mkdocs commands can be easily replaced by the relevant ones for your situation. Build a static site (skip if you already have one) If you don't already have a static site, then feel free to use the following example. In your project root directory, create a docs/ directory, and then place a dummy index.md in there: <!-- index.md --> # Index Page Hello world! Now, in the project root directory, create a mkdocs.yml file, in which you configure mkdocs to build the static site: # mkdocs.yml site_name : Marshmallow Generator This is a minimal mkdocs configuration. Now, make sure you have mkdocs installed in the Python environment that you're using. It's available on PyPI: # On your local machine pip install -U mkdocs Once installation has finished, you can now command mkdocs to build the static site to view locally: # On your local machine mkdocs serve If you can successfully view the static site on your local machine, i.e. you see the contents of index.md show up as a beautifully rendered HTML page, you're good to move on! Add project-specific configuration files for Dokku We're now going to add the necessary configuration files to work with Dokku. Firstly, we have to add in a .static file in the project root directory. This file tells Dokku that the site that is going to be built is a static site. To do so in the terminal, you only have to touch the file at the command line: # On your local machine touch .static Secondly, we have to add a .buildpacks file, where we specify that we are using two \"buildpacks\": one to provide the environment to run the commands that build the site, and another to build the site and serve up the static site files. In the case of our dummy mkdocs static sites, we need in .buildpacks : https://github.com/heroku/heroku-buildpack-python.git https://github.com/dokku/buildpack-nginx.git They have to go in that order, so that the first one is used for building, and the second one is used for serving the site. What are 'buildpacks? Once again, Heroku's docs have the most comprehensive explanation , as they're the originators of the idea. The short answer is that they are pre-built and configured \"base\" environments that you can build off. It's like having an opinionated Dockerfile that you can extend, except we extend it using declared configuration files in the repository instead. Thirdly, instead of a Procfile , we add an app.json file that contains the command for building the static site. { \"scripts\" : { \"dokku\" : { \"predeploy\" : \"cd /app/www && mkdocs build\" } } } Deployment tasks If you want to read more about this file, as well as the custom \"deployment tasks\" bit of Dokku, then check out the docs pages here .) OK, we just created a bunch of files, but I haven't explained how they're interacting with Dokku. There's definitely some opinionated things that we'll have to unpack. Firstly, the Dokku buildpack-nginx buildpack makes the opinionated assumption that your repository will be copied over into the Docker container's /app/www directory. That is why we have the cd /app/www command. Then, we follow it up with a mkdocs build , which you can change depending on what static site generator you're using. Secondly, the predeploy key declares to Dokku to execute the commands in the value (i.e. cd /app/www && mkdocs build ) before starting up the nginx server that points to the static site files. As you probably can grok by now, basically, the static sites are being built upon every deploy. This saves you from having to build the site locally and then pushing it up, which is both in-line with how git is supposed to be used (you only git push files that are generated by hand), and is in-line with the continuous deployment philosophy. Finally, we still need our requirements.txt file to be populated with whatever is needed to build the docs locally: # requirements.txt mkdocs==1.1 # put other dependencies below! Now that we're done, let's configure our remotes once again. Configure git Remote with Dokku As with the Streamlit app, go ahead and configure the git remote with Dokku: # On your local machine git remote add dokku dokku@mydomain.com:my-static-site Now, push up to Dokku! # On your local machine git push dokku master Configure Dokku As with the Streamlit app, let's now configure the domains: # On your Dokku box dokku domains:set my-static-site my-static-site.mydomain.com Unlike the app, we don't have to configure ports, because they will be mapped correctly by default. Finally, we need to configure nginx to point to the directory in which the index.html page is generated. In the case of mkdocs , the directory is in the site/ directory in the project root directory. We'll now configure it: # On your Dokku box dokku config:set my-static-site NGINX_ROOT = 'site' Warning You'll want to change site to whatever the output directory is for the static site generator you use! Alrighty, go ahead and visit your static site to confirm that it's running! Debugging As with the Streamlit app above, debugging is done in exactly the same way, using the two commands: # Inspect application logs dokku logs my_app_name -t # Inspect nginx logs dokku nginx:access-logs my_app_name -t The Framework I have a habit of categorizing things into a \"framework\" to help me anchor how I debug things, and I hope to share my framework for domains, apps, and Dokku with you. Firstly, we organized our Dokku box + domain name such that the Dokku box was referenced by the domain name, while individual apps got subdomains. We got subdomains for free by configuring a * on the DNS provider's A records, which forwarded all sub-domains to the Dokku box. Secondly, we configured each app on the Dokku box to resolve which subdomain points to it. In this way, subdomains need not be set on our DNS provider. Thirdly, we configured both static sites and dynamic data apps, using a collection of configuration files. For our data apps, it was primarily a Procfile and requirements.txt . For our static sites, it was a .buildpacks file, app.json file, and requirements.txt . Each have their purpose, but together they tell Dokku how to configure the environment in which apps are built. Cheatsheet of Commands Here's a cheatsheet of commands we used in this essay, to help you with getting set up. Domain Name Registration Register your domain. Add a custom resource record * pointing to your Dokku box's IP address Streamlit Commands # Run streamlit app streamlit run app.py Git commands # Add dokku remote git remote add dokku dokku@mydomain.com:streamlit-app # Push master branch to Dokku box git push dokku master Interacting with proxies # View port forwarding for app dokku proxy:report streamlit-app # Set port forarding for app dokku proxy:ports-set streamlit-app http:80:8501 The syntax for the ports is: <protocol>:<host port>:<container port> For port forwarding, if you follow the general framework we're using here, you should only have to configure the container port. Interacting with domains # View domains for an app dokku domains:report streamlit-app # Set domains for an app dokku domains:set streamlit-app streamlit-app.mydomain.com Again, if you follow the framework we have used here, then you should only need to configure <app-url>.mydomain.com Config Files Procfile for apps web: streamlit run app.py mkdocs.yml for MkDocs config # mkdocs.yml site_name : Marshmallow Generator Create .static for static sites: touch .static .buildpacks for static sites and multi-buildpack apps https://github.com/heroku/heroku-buildpack-python.git https://github.com/dokku/buildpack-nginx.git app.json for static sites: { \"scripts\" : { \"dokku\" : { \"predeploy\" : \"cd /app/www && mkdocs build\" } } } Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters, and shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter ! A big thank you... ...to my first Patreon supporter, Eddie Janowicz! Your support keeps me caffeinated so I can keep on making tutorials like these. I am paying for the $10/mo plan for the extra RAM seems to help. \u21a9","title":"Static Sites and Apps On Your Own Dokku Server"},{"location":"miscellaneous/static-sites-on-dokku/#static-sites-and-apps-on-your-own-dokku-server","text":"Summary: In this essay, I'm going to share with you how you can deploy your own static sites and apps on a Dokku server.","title":"Static Sites and Apps On Your Own Dokku Server"},{"location":"miscellaneous/static-sites-on-dokku/#introduction","text":"You've worked on this awesome Streamlit app, or a Panel dashboard, or a Plotly Dash web frontend for your data science work, and now you've decided to share the work. Or you've built documentation for the project, and you need to serve it up. Except, if your company doesn't have a dedicated platform for apps, you're stuck! That's because you've now got to share it from your laptop/workstation and point your colleagues to there (right... go ahead and email them a link to http://10.16.213.24:8501 right now...) and keep your computer running perpetually to serve the app in order for them to interact with it. Or, you copy/paste the docs into a separate hosted solution, and now the docs are divorced from your code, leading to documentation rot in the long-run because it's too troublesome to maintain two things simultaneously. That's much too fragile. What you need is another hosting machine that isolated from your development machine, so that you can develop in peace while the hosting machine reliably serves up the working, stable version of your work. The specific thing that you really need is a \"Platform as a Service\". There's a lot of commercial offerings, but if you're \"cheap\" and don't mind learning some new concepts to help you get around the web, then this essay is for you. Here, I'm going to show you how to configure and deploy Dokku as a personal PaaS solution that you can use at work and for hobby projects. I'm then going to show you how to deploy a static site (which can be useful for hosting blogs and documentation), and finally I'll show you how to deploy a Streamlit app, which you can use to show off a front-end to your fancy modelling work. Along the way, I hope to also point out the \"generalizable\" ideas behind the steps listed here, and give you a framework (at least, one useful one) for building things on the web.","title":"Introduction"},{"location":"miscellaneous/static-sites-on-dokku/#but-but-why","text":"","title":"But, but why?"},{"location":"miscellaneous/static-sites-on-dokku/#if-youre-part-of-a-company","text":"Your organization might not be equipped with modern PaaS tools that will enable you, a data scientist, to move quickly from local prototype to share-able prototype. If, however, you have access to bare metal cloud resources (i.e. just gimme a Linux box!), then as a hacker-type, you might be able to stand up your own PaaS and help demonstrate to your infrastructure team the value of having one.","title":"If you're part of a company..."},{"location":"miscellaneous/static-sites-on-dokku/#if-youre-doing-this-for-your-hobby-projects","text":"You might be as cheap as I am, but need a bit more beefy power than the restrictions given to you on Heroku (512MB RAM, 30 minute timeouts, and limited number of records in databases), and you don't want to pay $7/month for each project. Additionally, you might want a bit more control over your hosting options, but you don't feel completely ready to go fiddling with containers and networking without a software stack to help out just a bit .","title":"If you're doing this for your hobby projects..."},{"location":"miscellaneous/static-sites-on-dokku/#more-generally","text":"You might like the idea of containers, but find it kind of troublesome to learn yet another thing that's not trivial to configure, execute and debug (i.e. Docker). Dokku can be a bridge to get you there, as it automates much of the workflow surrounding Docker containers. It also comes with an API that both matches closely to Heroku (which is famous for being very developer-friendly) and also helps you handle proxy port mapping and domains easily. Are you ready? Let's go!","title":"More generally..."},{"location":"miscellaneous/static-sites-on-dokku/#pre-requisites","text":"I'm assuming you know how to generate and use SSH keys to remotely access another machine. This is an incredibly useful thing to know how to do, and so I'd recommend that you pick this skill up. (As usual, DigitalOcean's community pages have a great tutorial .) I am also assuming that you have access to a Linux box of some kind with an IP address that you can expose to the \"internet\". The \"internet\" in this case can mean the world wide web if you're working on a personal project, or your organization's \"intranet\" if you're planning on only letting those in your organization access the sites and apps that you will build. I'm also assuming familiarity with git , which I consider to be an indispensable tool in a data scientist's toolkit. This last point is not an assumption, but an exhortation: you should be building your data app prototype in accordance to 12-factor app principles. It'll make (most of) your Engineering colleagues delight in working with you. (Yes, there are some esoteric types that don't subscribe to 12 factor app principles...) If you've never heard of it, go check it out here . It's a wonderful read that will change how you build your data app prototypes for the better. It will also make handing over the app to Engineering much easier in the long-run, improving your relationship with the Engineering team that will take care of your app!","title":"Pre-requisites"},{"location":"miscellaneous/static-sites-on-dokku/#set-up-a-box-in-the-cloud-optional-if-you-have-one","text":"If you don't already have another computer that you have access to, or if you're curious on how to get set up in the cloud, then follow along these instructions. To make things a bit more concrete, I'm going to rent a server on DigitalOcean (DO). If you don't have a DigitalOcean account, feel free to use my referral link to sign up (and get free $100 credits that you can spend over 60 days) if you haven't got a DigitalOcean account already. (Disclaimer: I benefit too, but your support helps me make more of this kind of content!) Once you've signed up and logged into the DigitalOcean cloud dashboard, you can now set up a new Droplet. Droplets? \"Droplet\" is nothing more than DO's fancy name for a cloud server, or basically a computer that they're running. To do so, click on the big green \"Create\" button, set up a Droplet with the following settings: Ubuntu operating system \"Standard\" plan at $5/mo or $10/mo 1 Additional options: \"monitoring\" Authentication: you should use SSH keys (this is a pre-requisite for this essay). Hostname: Give it something memorable. Backups: Highly recommended. Give yourself peace of mind that you can rollback anytime. Once you're done with that, hit the big green \"Create Droplet\" button right at the bottom of the screen! Once your droplet is set up, you can go ahead and click on the \"Manage > Droplets\" left menu item, and that will bring you to a place where you can see all of your rented computers. The Cloud FYI, that's all the cloud is: renting someone else's computers. It turns out to be a pretty lucrative business! And because it's just someone else's computers, don't think of it as something magical.","title":"Set up a Box in the Cloud (optional if you have one)"},{"location":"miscellaneous/static-sites-on-dokku/#setup-dokku-on-your-shiny-rented-machine","text":"Let's now go ahead and set up Dokku on your shiny new droplet.","title":"Setup Dokku on your Shiny Rented Machine"},{"location":"miscellaneous/static-sites-on-dokku/#run-the-dokku-installation-commands","text":"Dokku installation on Ubuntu is quite easy; the following instructions are taken from the Dokku docs . SSH into your machine, then type the following: wget https://raw.githubusercontent.com/dokku/dokku/v0.20.4/bootstrap.sh ; # CHECK FOR LATEST DOKKU VERSION! # http://dokku.viewdocs.io/dokku/getting-started/installation/ sudo DOKKU_TAG = v0.20.4 bash bootstrap.sh","title":"Run the Dokku installation commands"},{"location":"miscellaneous/static-sites-on-dokku/#wrap-up-dokku-installation","text":"If you're on an Ubuntu installation, you'll want to navigate to your Linux box's IP address, and finish up the installation instructions, which involves letting your Dokku installation know about your SSH public keys. These keys are important, as the are literally the keys to letting you push your app to your Dokku box! SSH Keys and Dokku More pedantically, these SSH keys identify your machine as being \"authorized\" to push to the git server that is running on your Dokku machine. This is what allows you to git push dokku master later on! For those of you who have CentOS or other flavours of Linux, you will need to follow analogous instructions on the Dokku website. I have had experience following the CentOS instructions at work, and had to modify the installation commands a little bit to work with our internal proxies.","title":"Wrap up Dokku installation"},{"location":"miscellaneous/static-sites-on-dokku/#test-that-dokku-is-working","text":"To test that your Dokku installation is working, type the following command: dokku help That should show the Dokku help menu, and you'll know the installation has completed successfully!","title":"Test that Dokku is working"},{"location":"miscellaneous/static-sites-on-dokku/#optionally-set-proxies-for-docker","text":"Now, because Dokku builds upon Docker, if you're behind a corporate proxy, you might need to configure your Docker daemon proxies as well. You'll then want to follow instructions on the Docker daemon documentation . Those steps will generally be the same as what's in the docs, though the specifics will change (e.g. your proxy server address).","title":"Optionally set proxies for Docker"},{"location":"miscellaneous/static-sites-on-dokku/#configure-domain-names","text":"The allure of Heroku is that it gives your app a name: myapp.herokuapp.com , or myshinyapp.herokuapp.com quite automagically. With Dokku and a bit of extra legwork, we can replicate this facility. We're going to set up your Dokku box uch that its main domain will be mydomain.com , and apps will get a subdomain myapp.mydomain.com .","title":"Configure domain names"},{"location":"miscellaneous/static-sites-on-dokku/#register-a-domain-name","text":"Firstly, you'll need a domain name from a Domain Name Service (DNS) registrar. Cloudflare seems to be doing all of the right things at the moment, so their domain name registration service is something I'm not hesitant to recommend (at least as of 2020). For historical reasons, I'm currently using Google Domains. At work, we have an internal service that lets us register an internal domain name. What matters most is that you have the ability to assign a domain name that points to your Dokku machine's IP address. Go ahead and register a domain name that reflects who \"you\" are on the web. For myself, I have a personal domain name that I use. At work, I registered a name that reflects the research group that I work in. Make sure that the name \"points\"/\"forwards\" to the IP address of your Dokku box.","title":"Register a domain name"},{"location":"miscellaneous/static-sites-on-dokku/#enable-subdomains","text":"To enable the ability to use subdomains like myapp.mydomain.com for each app, you'll want to also configure the DNS settings. On your domain registrar, look for the place where you can customize \"resource records\". On Google Domains, it's under \"DNS > Custom resource records\". There, you'll want to add an \"A\" record (as opposed to other options that you might see, like \"CNAME\", \"AAAA\", and others). The \"name\" should be * , literally just an asterisk. The IPv4 address should point to your Dokku machine. This is all that is needed. What is an 'A' record? The most comprehensive explainer I've seen is at dnsimple , but the long story short is that it is an \"Address\" record. Yes, \"A\" stands for \"Address\", and it is nothing more than a pointer that maps \"string\" address to IP address. What then about the name * ? What we just did up there was to say, point everything *.mydomain.com to to the Dokku box IP address. How then do we get subdomains if we don't configure them with our DNS? Well, the secret here is that Dokku can help us with subdomains. Read on for how configuration of your Dokku box! To test whether your domain name is setup correctly, head over to the domain in your favourite browser. At this point, you should see the default NGINX landing page, as you have no apps deployed and no domains configured. How do you pronounce 'NGINX'? The official way is \"engine-X\". The wrong way is \"en-jinx\". Don't get it wrong! And what is NGINX? From Wikipedia : Nginx is a web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache. For our purposes, we treat it as a thing that routes URLs to containers.","title":"Enable subdomains!"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-a-test-app","text":"Heroku provides a \"Python getting started\" repository that we will use to check that the installation is working correctly. This one deploys reliably with all of the vanilla commands entered. Leveraging this, I will also show you how to leverage your * A record to put in nice subdomains!","title":"Deploy a test app"},{"location":"miscellaneous/static-sites-on-dokku/#clone-the-test-app","text":"Firstly, git clone Heroku's python-getting-started repository to your laptop/local machine (i.e. not your Dokku box). Next, cd into the repository: cd python-getting-started After that, add your Dokku box as a git remote to the repository: git remote add dokku dokku@your-domain-name:python-getting-started Be sure to replace your-domain-name with your newfangled domain that you registered. App name The python-getting-started after the colon is the \"app name\" that you will see at the command line when interacting with Dokku later. Now, push the app to your Dokku box! git push dokku master Unlike your usual pushes to GitHub, GitLab or Bitbucket, you'll see a series of remote outputs being beamed back to your terminal. What's happening here is the build of the app! In particular, a Docker build is happening behind-the-scenes, so your app is completely self-contained and containerized on the Dokku box! If everything went well, the last output beamed back to you should look like: ===== > Application deployed: http://mydomain.com:10161 Wonderful! Now let's go back to Dokku and configure your app.","title":"Clone the test app"},{"location":"miscellaneous/static-sites-on-dokku/#configure-the-app-domain-and-ports","text":"We're now going to configure Dokku to recognize which subdomains should point to which apps. Firstly, get familiar with the Dokku domains command: # On your Dokku box dokku domains:help That should list out all of the Dokku domains sub-commands. Usage: dokku domains [ :COMMAND ] Manage domains used by the proxy Additional commands: domains:add <app> <domain> [ <domain> ... ] Add domains to app domains:add-global <domain> [ <domain> ... ] Add global domain names domains:clear <app> Clear all domains for app domains:clear-global Clear global domain names domains:disable <app> Disable VHOST support domains:enable <app> Enable VHOST support domains:remove <app> <domain> [ <domain> ... ] Remove domains from app domains:remove-global <domain> [ <domain> ... ] Remove global domain names domains:report [ <app> | --global ] [ <flag> ] Displays a domains report for one or more apps domains:set <app> <domain> [ <domain> ... ] Set domains for app domains:set-global <domain> [ <domain> ... ] Set global domain names You can report domains used for the app, python-getting-started : # On your Dokku box dokku domains:report python-getting-started The output should look something like this: $ dokku domains:report python-getting-started ===== > python-getting-started domains information Domains app enabled: false Domains app vhosts: Domains global enabled: false Domains global vhosts: This tells us that python-getting-started has no domains configured for it. We can now set it: # On your Dokku box dokku domains:set python-getting-started python-getting-started.mydomain.com The output will look like this: -----> Added python-getting-started.mydomain.com to python-getting-started -----> Configuring python-getting-started.mydomain.com... ( using built-in template ) -----> Creating http nginx.conf Reloading nginx Now, you should be able to go to http://python-getting-started.mydomain.com , and the page that gets loaded should be the \"Getting Started with Python on Heroku\" landing page! So, what magic happened here? What's happening here is that NGINX resolving subdomains to particular containers, and mapping them to the appropriate container port that is being exposed. If everything deployed correctly up till this point, you're good to go with deploying a data app on your Dokku machine!","title":"Configure the app domain and ports"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-your-data-app","text":"Deploying the python-getting-started app should have given you: the confidence that your Dokku installation is working correctly, firsthand experience configuring Dokku, a taste of the workflow for deploying an app. Now, we're going to apply that to a Streamlit app. I've chosen Streamlit because it's got the easiest programming model amongst all of the Dashboard/app development frameworks that I've seen; in fact, I was able to stand up an explainer on the Beta distribution in under 3 hours, the bulk of which was spent on writing prose, not figuring out how to program with Streamlit.","title":"Deploy your data app"},{"location":"miscellaneous/static-sites-on-dokku/#build-a-streamlit-app-skip-if-you-already-have-an-app","text":"If you don't have a Streamlit app, here's one that you can use as a starter, which simply displays some text and a button: # app.py import streamlit as st \"\"\" # First Streamlit App! This is a dummy streamlit app. \"\"\" finished = st . button ( \"Click me!\" ) if finished : st . balloons () Save it as app.py in your project directory. Now, you can run the app with Streamlit: # On your local machine streamlit run app.py You should see the following show up in your terminal: You can now view your Streamlit app in your browser. Local URL: http://localhost:8501 Network URL: http://<your_ip_address>:8501 You can go to the local URL and confirm that the app is running correctly, and that it does exactly what's expected.","title":"Build a streamlit app (skip if you already have an app)"},{"location":"miscellaneous/static-sites-on-dokku/#add-project-specific-configuration-files-for-dokku","text":"Now, we need to add a few configuration files that Dokku will recognize.","title":"Add project-specific configuration files for Dokku"},{"location":"miscellaneous/static-sites-on-dokku/#requirementstxt","text":"Firstly, make sure you have a requirements.txt file in the project root directory, in which you specify all of the requirements for your app to run. # requirements.txt streamlit==0.57.3 # pinning version numbers is good for apps. # put more below as necessary, e.g.: numpy==0.16 With a requirements.txt file, Dokku (and Heroku) will automagically recognize that you have a Python app. Dokku will then create a Docker container equipped with Python, and install all of the dependencies in there. Declarative configuration FTW!","title":"requirements.txt"},{"location":"miscellaneous/static-sites-on-dokku/#procfile","text":"Next, you need a Procfile in the project root directory: # Procfile web: streamlit run app.py Procfile A quick note: It is Procfile , with no file extensions. Don't save it as Procfile.txt , because that will not get recognized by Dokku/Heroku. To learn more, read about it on Heroku . The general syntax in the Procfile is: <process_type>: <command> The command is always a single line, and tells Dokku/Heroku what commands to execute in order to run the app. In our case, we simply execute the same command that we used to run the app locally for development purposes. More complex Procfile commands You can have multiple bash commands in a single line, though if it gets complicated, you may want to extract the commands out into a separate bash script that you execute instead, e.g.: # dokku.sh pip install -e . export CONFIG_DIR=\"/path/to/config/files\" streamlit run app.py For the process_type , in the case of Dokku, is always \"web\". Heroku, on the other hand, can handle other process types. Since we're dealing with Dokku and not Heroku, don't bother changing process_type .","title":"Procfile"},{"location":"miscellaneous/static-sites-on-dokku/#configure-git-remote-with-dokku","text":"Now, let's configure your Dokku remote. # On your local machine git remote add dokku dokku@mydomain.com:streamlit-app Remember two points! Firstly, change mydomain.com to your domain. Secondly, you can use any app name you want, it doesn't have to be streamlit-app . A convention that has helped me is to have a 1:1 mapping between app and project repository folder name. It means one less thing to be confused about. Once you're done configuring the remote, now push it up! # On your local machine git push dokku master The same build commands will take place. While they are taking place, go ahead and open a new Terminal, and SSH into the Dokku box. We're going to configure the new app on Dokku!","title":"Configure git Remote with Dokku"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku-subdomain","text":"Let's start with the subdomain name first. For this tutorial, I'm going to use the domain name streamlit-app.mydomain.com . Let's configure the app streamlit-app with that domain name: # On Dokku box dokku domains:set streamlit-app streamlit-app.mydomain.com","title":"Configure Dokku Subdomain"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku-port-mapping","text":"Next, we have to configure the port mapping that Dokku's proxy server will recognize. By default, every container has the \"hosting box\" (i.e. the machine Dokku is running on) port 80 mapped to \"container box\" (i.e. the container the app is running on) port 5000. You can see this with: # On Dokku box dokku proxy:report streamlit-app That will give you something like: ===== > python-getting-started proxy information Proxy enabled: true Proxy port map: http:80:5000 Proxy type: nginx Now, because streamlit is going to be run on port 8501 (in the container) by default, we need to change the port mapping from http:80:5000 to http:80:8501 . To do so: # On Dokku box dokku proxy:ports-set streamlit-app http:80:8501 Putting these two configurations together, i.e. setting the subdomain and port mapping, we have now told Dokku, \"Each time you get a request from http://streamlit-app.mydomain.com , forward it to port 8501 on the streamlit-app container.\"","title":"Configure Dokku port mapping"},{"location":"miscellaneous/static-sites-on-dokku/#test-it-out","text":"Well, we now can test it out. Go ahead and head over to your app URL, and see if the app works for you!","title":"Test it out!"},{"location":"miscellaneous/static-sites-on-dokku/#debugging","text":"If things look like they're crashing, how do you debug? Well, you always should know how to look at the logs: dokku logs my_app_name -t That will keep the logs updating in the terminal as you refresh the page. Use the information in the logs to help you debug. Also, see if you can reproduce the error in the logs locally. Additionally, if you get nginx errors, you can look at the nginx logs to help you debug proxy errors as they show up: dokku nginx:access-logs my_app_name -t Look at the logs and dig through for anything that might help you with your Google searches. Follow this pattern, and soon enough, you'll become an expert at debugging your web apps!","title":"Debugging"},{"location":"miscellaneous/static-sites-on-dokku/#deploy-a-static-site","text":"Now that you've seen how to deploy an app that's powered by a container behind-the-scenes, let's now figure out how to deploy a static site that is built upon every deploy. It's essentially the same. We have configuration files (in this case, slightly different ones) that declare what kind of environment we need. We basically treat the static site generator as an \"app\" that generates the HTML pages that we serve up freshly on each build. For this example, I'm going to use mkdocs , as it is also easy to use to build sites, and can be extended with some pretty awesome templates (like mkdocs-material ) for responsive docs generated from Markdown files. If you've got another static site builder (I have used Lektor , sphinx , and Nikola before), the places where we use mkdocs commands can be easily replaced by the relevant ones for your situation.","title":"Deploy a static site"},{"location":"miscellaneous/static-sites-on-dokku/#build-a-static-site-skip-if-you-already-have-one","text":"If you don't already have a static site, then feel free to use the following example. In your project root directory, create a docs/ directory, and then place a dummy index.md in there: <!-- index.md --> # Index Page Hello world! Now, in the project root directory, create a mkdocs.yml file, in which you configure mkdocs to build the static site: # mkdocs.yml site_name : Marshmallow Generator This is a minimal mkdocs configuration. Now, make sure you have mkdocs installed in the Python environment that you're using. It's available on PyPI: # On your local machine pip install -U mkdocs Once installation has finished, you can now command mkdocs to build the static site to view locally: # On your local machine mkdocs serve If you can successfully view the static site on your local machine, i.e. you see the contents of index.md show up as a beautifully rendered HTML page, you're good to move on!","title":"Build a static site (skip if you already have one)"},{"location":"miscellaneous/static-sites-on-dokku/#add-project-specific-configuration-files-for-dokku_1","text":"We're now going to add the necessary configuration files to work with Dokku. Firstly, we have to add in a .static file in the project root directory. This file tells Dokku that the site that is going to be built is a static site. To do so in the terminal, you only have to touch the file at the command line: # On your local machine touch .static Secondly, we have to add a .buildpacks file, where we specify that we are using two \"buildpacks\": one to provide the environment to run the commands that build the site, and another to build the site and serve up the static site files. In the case of our dummy mkdocs static sites, we need in .buildpacks : https://github.com/heroku/heroku-buildpack-python.git https://github.com/dokku/buildpack-nginx.git They have to go in that order, so that the first one is used for building, and the second one is used for serving the site. What are 'buildpacks? Once again, Heroku's docs have the most comprehensive explanation , as they're the originators of the idea. The short answer is that they are pre-built and configured \"base\" environments that you can build off. It's like having an opinionated Dockerfile that you can extend, except we extend it using declared configuration files in the repository instead. Thirdly, instead of a Procfile , we add an app.json file that contains the command for building the static site. { \"scripts\" : { \"dokku\" : { \"predeploy\" : \"cd /app/www && mkdocs build\" } } } Deployment tasks If you want to read more about this file, as well as the custom \"deployment tasks\" bit of Dokku, then check out the docs pages here .) OK, we just created a bunch of files, but I haven't explained how they're interacting with Dokku. There's definitely some opinionated things that we'll have to unpack. Firstly, the Dokku buildpack-nginx buildpack makes the opinionated assumption that your repository will be copied over into the Docker container's /app/www directory. That is why we have the cd /app/www command. Then, we follow it up with a mkdocs build , which you can change depending on what static site generator you're using. Secondly, the predeploy key declares to Dokku to execute the commands in the value (i.e. cd /app/www && mkdocs build ) before starting up the nginx server that points to the static site files. As you probably can grok by now, basically, the static sites are being built upon every deploy. This saves you from having to build the site locally and then pushing it up, which is both in-line with how git is supposed to be used (you only git push files that are generated by hand), and is in-line with the continuous deployment philosophy. Finally, we still need our requirements.txt file to be populated with whatever is needed to build the docs locally: # requirements.txt mkdocs==1.1 # put other dependencies below! Now that we're done, let's configure our remotes once again.","title":"Add project-specific configuration files for Dokku"},{"location":"miscellaneous/static-sites-on-dokku/#configure-git-remote-with-dokku_1","text":"As with the Streamlit app, go ahead and configure the git remote with Dokku: # On your local machine git remote add dokku dokku@mydomain.com:my-static-site Now, push up to Dokku! # On your local machine git push dokku master","title":"Configure git Remote with Dokku"},{"location":"miscellaneous/static-sites-on-dokku/#configure-dokku","text":"As with the Streamlit app, let's now configure the domains: # On your Dokku box dokku domains:set my-static-site my-static-site.mydomain.com Unlike the app, we don't have to configure ports, because they will be mapped correctly by default. Finally, we need to configure nginx to point to the directory in which the index.html page is generated. In the case of mkdocs , the directory is in the site/ directory in the project root directory. We'll now configure it: # On your Dokku box dokku config:set my-static-site NGINX_ROOT = 'site' Warning You'll want to change site to whatever the output directory is for the static site generator you use! Alrighty, go ahead and visit your static site to confirm that it's running!","title":"Configure Dokku"},{"location":"miscellaneous/static-sites-on-dokku/#debugging_1","text":"As with the Streamlit app above, debugging is done in exactly the same way, using the two commands: # Inspect application logs dokku logs my_app_name -t # Inspect nginx logs dokku nginx:access-logs my_app_name -t","title":"Debugging"},{"location":"miscellaneous/static-sites-on-dokku/#the-framework","text":"I have a habit of categorizing things into a \"framework\" to help me anchor how I debug things, and I hope to share my framework for domains, apps, and Dokku with you. Firstly, we organized our Dokku box + domain name such that the Dokku box was referenced by the domain name, while individual apps got subdomains. We got subdomains for free by configuring a * on the DNS provider's A records, which forwarded all sub-domains to the Dokku box. Secondly, we configured each app on the Dokku box to resolve which subdomain points to it. In this way, subdomains need not be set on our DNS provider. Thirdly, we configured both static sites and dynamic data apps, using a collection of configuration files. For our data apps, it was primarily a Procfile and requirements.txt . For our static sites, it was a .buildpacks file, app.json file, and requirements.txt . Each have their purpose, but together they tell Dokku how to configure the environment in which apps are built.","title":"The Framework"},{"location":"miscellaneous/static-sites-on-dokku/#cheatsheet-of-commands","text":"Here's a cheatsheet of commands we used in this essay, to help you with getting set up.","title":"Cheatsheet of Commands"},{"location":"miscellaneous/static-sites-on-dokku/#domain-name-registration","text":"Register your domain. Add a custom resource record * pointing to your Dokku box's IP address","title":"Domain Name Registration"},{"location":"miscellaneous/static-sites-on-dokku/#streamlit-commands","text":"# Run streamlit app streamlit run app.py","title":"Streamlit Commands"},{"location":"miscellaneous/static-sites-on-dokku/#git-commands","text":"# Add dokku remote git remote add dokku dokku@mydomain.com:streamlit-app # Push master branch to Dokku box git push dokku master","title":"Git commands"},{"location":"miscellaneous/static-sites-on-dokku/#interacting-with-proxies","text":"# View port forwarding for app dokku proxy:report streamlit-app # Set port forarding for app dokku proxy:ports-set streamlit-app http:80:8501 The syntax for the ports is: <protocol>:<host port>:<container port> For port forwarding, if you follow the general framework we're using here, you should only have to configure the container port.","title":"Interacting with proxies"},{"location":"miscellaneous/static-sites-on-dokku/#interacting-with-domains","text":"# View domains for an app dokku domains:report streamlit-app # Set domains for an app dokku domains:set streamlit-app streamlit-app.mydomain.com Again, if you follow the framework we have used here, then you should only need to configure <app-url>.mydomain.com","title":"Interacting with domains"},{"location":"miscellaneous/static-sites-on-dokku/#config-files","text":"Procfile for apps web: streamlit run app.py mkdocs.yml for MkDocs config # mkdocs.yml site_name : Marshmallow Generator Create .static for static sites: touch .static .buildpacks for static sites and multi-buildpack apps https://github.com/heroku/heroku-buildpack-python.git https://github.com/dokku/buildpack-nginx.git app.json for static sites: { \"scripts\" : { \"dokku\" : { \"predeploy\" : \"cd /app/www && mkdocs build\" } } }","title":"Config Files"},{"location":"miscellaneous/static-sites-on-dokku/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters, and shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"miscellaneous/static-sites-on-dokku/#a-big-thank-you","text":"...to my first Patreon supporter, Eddie Janowicz! Your support keeps me caffeinated so I can keep on making tutorials like these. I am paying for the $10/mo plan for the extra RAM seems to help. \u21a9","title":"A big thank you..."},{"location":"newsletter/2020/05-may/","text":"Data Science Programming May 2020 Newsletter Hello fellow datanistas! Here\u2019s the May 2020 edition of my newsletter. I\u2019m trying out a slightly different formatting; as always, though, I hope you find it useful. If you have feedback, do send it my way ! Multi-Task Learning in the Wild I recently watched Andrej Karpathy\u2019s talk on multi-task learning , and I learned a ton. When you\u2019re faced with hardware constraints, how do you tweak your ML model to get better at more tasks? Check out his talk to learn more. Gaussian processes explained quite simply I\u2019ve been a fan of Gaussian Processes for non-linear predictive modeling tasks, especially when writing a neural network feels too much for the small-ish data that I have. Learning about them wasn\u2019t easy though. That said, there\u2019s a wonderful blog post from Yuge Shi, a PhD student at Oxford, explaining GPs in a pretty intuitive fashion. She put in enough pictures to accompany the math that you should find it an enjoyable read ! Preview of an exciting development using Dask If you\u2019re a Dask user, this next video preview is going to be music to your ears. Matthew Rocklin, lead developer of Dask and founder of Coiled Computing (which is providing support for Dask and more) just showed us spinning Dask clusters in the cloud from a laptop, getting us to interactive-scale compute. This is the dream: burstable, interactive-time, portable, large-scale computing from my laptop to the cloud with minimal config! Check out the screencast he made for a preview! SciPy 2020 Virtual SciPy 2020\u2019s schedule has been released! The conference has been a wonderful place to learn about the latest in data science and scientific computing tooling. Come check out the schedule here [scipy]. I will be there presenting a tutorial on Bayesian statistics; hope to see you there! Faster pandas applies with swifter While seeking out faster ways to do pandas applies, I learned about a new tool, called swifter ! It automatically finds the fastest way to apply a pandas function. Fits very nicely into the paradigm of \u201cdo one thing and only one thing well\u201d. Check out the GitHub repository and let me know what you think of it! I will be experimenting with it on pyjanitor to see whether it does a better job with speeding up some of the functions in there. From my collection Sane path management in your project directory I recently wrote a little post about how we can use Python\u2019s pathlib to make file paths a little more sane in our projects. pyprojroot , the tool I feature in the post, was developed by one of my Python conference dopplegangers, Daniel Chen , who has this wonderfully ironic habit of doing book giveaways and signings of his Pandas book at R conferences :). Updates to our network analysis tutorial! Finally, with my collaborator Mridul Seth (who runs the GSoC program with NumFOCUS), we\u2019ve been updating our Network Analysis Made Simple repository! My Patreon supporters will get early access to the tutorial repository before its public launch later in the year, so if you like it, please consider sending a cup of coffee each month ! Your support would go a long way to supporting the creation and maintenance of this teaching material! (Up next will be Bayesian content - on probabilistic programming - just in time for SciPy 2020!) Stay safe and enjoy the sunshine! Eric","title":"May"},{"location":"newsletter/2020/05-may/#data-science-programming-may-2020-newsletter","text":"Hello fellow datanistas! Here\u2019s the May 2020 edition of my newsletter. I\u2019m trying out a slightly different formatting; as always, though, I hope you find it useful. If you have feedback, do send it my way !","title":"Data Science Programming May 2020 Newsletter"},{"location":"newsletter/2020/05-may/#multi-task-learning-in-the-wild","text":"I recently watched Andrej Karpathy\u2019s talk on multi-task learning , and I learned a ton. When you\u2019re faced with hardware constraints, how do you tweak your ML model to get better at more tasks? Check out his talk to learn more.","title":"Multi-Task Learning in the Wild"},{"location":"newsletter/2020/05-may/#gaussian-processes-explained-quite-simply","text":"I\u2019ve been a fan of Gaussian Processes for non-linear predictive modeling tasks, especially when writing a neural network feels too much for the small-ish data that I have. Learning about them wasn\u2019t easy though. That said, there\u2019s a wonderful blog post from Yuge Shi, a PhD student at Oxford, explaining GPs in a pretty intuitive fashion. She put in enough pictures to accompany the math that you should find it an enjoyable read !","title":"Gaussian processes explained quite simply"},{"location":"newsletter/2020/05-may/#preview-of-an-exciting-development-using-dask","text":"If you\u2019re a Dask user, this next video preview is going to be music to your ears. Matthew Rocklin, lead developer of Dask and founder of Coiled Computing (which is providing support for Dask and more) just showed us spinning Dask clusters in the cloud from a laptop, getting us to interactive-scale compute. This is the dream: burstable, interactive-time, portable, large-scale computing from my laptop to the cloud with minimal config! Check out the screencast he made for a preview!","title":"Preview of an exciting development using Dask"},{"location":"newsletter/2020/05-may/#scipy-2020","text":"Virtual SciPy 2020\u2019s schedule has been released! The conference has been a wonderful place to learn about the latest in data science and scientific computing tooling. Come check out the schedule here [scipy]. I will be there presenting a tutorial on Bayesian statistics; hope to see you there!","title":"SciPy 2020"},{"location":"newsletter/2020/05-may/#faster-pandas-applies-with-swifter","text":"While seeking out faster ways to do pandas applies, I learned about a new tool, called swifter ! It automatically finds the fastest way to apply a pandas function. Fits very nicely into the paradigm of \u201cdo one thing and only one thing well\u201d. Check out the GitHub repository and let me know what you think of it! I will be experimenting with it on pyjanitor to see whether it does a better job with speeding up some of the functions in there.","title":"Faster pandas applies with swifter"},{"location":"newsletter/2020/05-may/#from-my-collection","text":"","title":"From my collection"},{"location":"newsletter/2020/05-may/#sane-path-management-in-your-project-directory","text":"I recently wrote a little post about how we can use Python\u2019s pathlib to make file paths a little more sane in our projects. pyprojroot , the tool I feature in the post, was developed by one of my Python conference dopplegangers, Daniel Chen , who has this wonderfully ironic habit of doing book giveaways and signings of his Pandas book at R conferences :).","title":"Sane path management in your project directory"},{"location":"newsletter/2020/05-may/#updates-to-our-network-analysis-tutorial","text":"Finally, with my collaborator Mridul Seth (who runs the GSoC program with NumFOCUS), we\u2019ve been updating our Network Analysis Made Simple repository! My Patreon supporters will get early access to the tutorial repository before its public launch later in the year, so if you like it, please consider sending a cup of coffee each month ! Your support would go a long way to supporting the creation and maintenance of this teaching material! (Up next will be Bayesian content - on probabilistic programming - just in time for SciPy 2020!) Stay safe and enjoy the sunshine! Eric","title":"Updates to our network analysis tutorial!"},{"location":"newsletter/2020/06-june/","text":"Data Science Programming June 2020 Newsletter Hello datanistas! We're back with another edition of the programmer-oriented data science newsletter. This month, I have so much I've learned and to share, so I'm thoroughly excited to be writing this newsletter edition! Python 3.9 Beta! First things first, Python 3.9's latest beta has been released! There are new language features in there, including: New dictionary operators A topological sorter class in functools A \"least common multiple\" ( lcm ) function in the math library, And the best of them all: string.removeprefix('prefix_goes_here') and string.removesuffix('suffix_goes_here') ! This is a serious convenience piece for those of us who work with files! Check out Martin Heinz' blog post on Medium to learn more! Learn Through The Data Science Design Manual During this extraordinary COVID-19 time, Springer did an extraordinary thing that I never expected: They released a whole bucketload of books for free online! One of them caught my eye: \"The Data Science Design Manual\" . Having browsed through the book PDF, I'm impressed by its coverage of the foundational topics that I think every data scientist should be equipped with: statistical inference, data wrangling, linear algebra, and machine learning. The author, Steven Skiena, also covers more in there. Go grab the PDF while it's still free! Easy matplotlib animations Recently, celluloid caught my eye: it's a package that lets you create matplotlib animations easily! If you need a dead-simple example to convince you to check it out, here's one lifted straight from the repository: from matplotlib import pyplot as plt from celluloid import Camera fig = plt . figure () camera = Camera ( fig ) for i in range ( 10 ): plt . plot ([ i ] * 10 ) camera . snap () animation = camera . animate () But seriously though, if you use the workhorse Python drawing package matplotlib for anything, this package can be considered to be one of those \"great tricks to have\" in your bag! Better Design Skills: Points of View Continuing the theme of visualization, I wanted to share with you a resource from Nature Methods that has influenced the entirety of how I approach data visualization and figure design. This is the Points of View series , written by Bang Wong and Martin Krzywinski and many other co-authors. The entire series is available online , and is a valuable resource to read. Two fun tidbits: I devoured the entire series while doing my doctoral training, eagerly awaiting each new release like a Netflix addict . And I was thoroughly thrilled when Bang decided to join the department I work in at NIBR! Imagine getting to work with your grad school hero :). Better Software Skills: Document your tests! For those of you who know me, I am a strong proponent of data scientists being equipped with good, basic software skills. When we write code in a \"basically good\" way (refactored, documented and tested), we accelerate our productivity many-fold. One of my interns reminded me of this when we realized that something that would have otherwise taken days to get right in SQL ended up being 10 minutes of work because we documented and tested our pandas DataFrame caches. (If you wish to read more about testing, I write about it on my Essays on data science .) Documenting code is important. Turns out, your test suite is also code! So in his blog post , Hyne Schlawack makes the argument that we ought to document our tests, something that has become painfully obvious in some of the projects I have worked on. His blog post, then, gets an absolute strong recommendation from me! Work Anywhere with Development Containers For those of you who, like myself, moonlight as a software engineer because you develop tools, this next piece might come as music to your ears: Visual Studio Code has superb support for developing a project inside a Docker container . If you try it out, I guarantee you the convenience of never having to get someone else set up with development instructions will be liberating. Since finding out about it on Thursday (28 May), I've enabled dev containers on my personal website , my Essays collection , and the pyjanitor project . In each case, Dockerhub automatically builds containers on every commit to the default branch, and those containers are referenced in the dev container configuration file, which means your local machine never has to build the container , you only have to pull it down! I also got everything working remotely, so my puny little 12\" MacBook now uses a remote GPU-enabled development server. Speaking of which, if you're interested in making an open source contribution, or wish to just test-drive dev containers on an actual project, check out the docs I wrote for the pyjanitor project ! Automate Workflow with Continuous X I first saw what \"Continuous X\" meant when I made my first pull requests to the matplotlib project, and was hooked ever since. Having a continuous pipeline runner like Travis or Jenkins or Azure Pipelines automatically run code and style checks on every commit takes a ton of drudgery out of guaranteeing that our software works properly. It's like having a Roomba go through your kitchen every time it knows you've finished a meal. How does \"continuous X\" apply for data science projects though? Turns out, individuals way more experienced than myself and much smarter than me have been thinking about this problem too. In particular, I want to highlight two articles, one by Danilo Sato, Arif Wider and Christoph Windheuser and one on Booklet.ai . In both cases, they raise possible ways to integrate pipeline-based automation into data projects, making them robust and reproducible. Be sure to check the articles out! From My Collection I have two articles from my own collection to share. The first one is about how to set up a personal platform as a service (PaaS) called Dokku . It's written for those who are especially cheap (like yours truly) and don't want to pay $7/month to Heroku for each project that gets hosted there. For those of you who do want to learn the basics of Heroku-based deployment, I have a class on Skillshare that you can use too, which is being used by the Insight Data Science Fellows in Boston! The second one is about a hack to speed up data loading , using a package called cachier . It's a neat hack - especially if you wrap specific data queries from a database into a Python function! Take a break, have a bit of humour Let's close with some humorous stuff, if not at least to lighten the mood in these tumultuous times. Firstly, Hossein Siamaknejad actually did it : automate a game using Python. And the hack was absolutely brilliant : \"RGB detection and programmatically controlled mouse and keyboard\". Props to you, Hossein ! Secondly, the prolifically-hilarious Kareem Carr writes about \"practicing safe.... modelling\" . Happy, ahem, modelling :) Hope you enjoyed this edition of the programmer-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. If you'd like to get early access to new written tutorials, essays, 1-on-1 consulting (I just did one session with one of my supporters!) and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"June"},{"location":"newsletter/2020/06-june/#data-science-programming-june-2020-newsletter","text":"Hello datanistas! We're back with another edition of the programmer-oriented data science newsletter. This month, I have so much I've learned and to share, so I'm thoroughly excited to be writing this newsletter edition!","title":"Data Science Programming June 2020 Newsletter"},{"location":"newsletter/2020/06-june/#python-39-beta","text":"First things first, Python 3.9's latest beta has been released! There are new language features in there, including: New dictionary operators A topological sorter class in functools A \"least common multiple\" ( lcm ) function in the math library, And the best of them all: string.removeprefix('prefix_goes_here') and string.removesuffix('suffix_goes_here') ! This is a serious convenience piece for those of us who work with files! Check out Martin Heinz' blog post on Medium to learn more!","title":"Python 3.9 Beta!"},{"location":"newsletter/2020/06-june/#learn-through-the-data-science-design-manual","text":"During this extraordinary COVID-19 time, Springer did an extraordinary thing that I never expected: They released a whole bucketload of books for free online! One of them caught my eye: \"The Data Science Design Manual\" . Having browsed through the book PDF, I'm impressed by its coverage of the foundational topics that I think every data scientist should be equipped with: statistical inference, data wrangling, linear algebra, and machine learning. The author, Steven Skiena, also covers more in there. Go grab the PDF while it's still free!","title":"Learn Through The Data Science Design Manual"},{"location":"newsletter/2020/06-june/#easy-matplotlib-animations","text":"Recently, celluloid caught my eye: it's a package that lets you create matplotlib animations easily! If you need a dead-simple example to convince you to check it out, here's one lifted straight from the repository: from matplotlib import pyplot as plt from celluloid import Camera fig = plt . figure () camera = Camera ( fig ) for i in range ( 10 ): plt . plot ([ i ] * 10 ) camera . snap () animation = camera . animate () But seriously though, if you use the workhorse Python drawing package matplotlib for anything, this package can be considered to be one of those \"great tricks to have\" in your bag!","title":"Easy matplotlib animations"},{"location":"newsletter/2020/06-june/#better-design-skills-points-of-view","text":"Continuing the theme of visualization, I wanted to share with you a resource from Nature Methods that has influenced the entirety of how I approach data visualization and figure design. This is the Points of View series , written by Bang Wong and Martin Krzywinski and many other co-authors. The entire series is available online , and is a valuable resource to read. Two fun tidbits: I devoured the entire series while doing my doctoral training, eagerly awaiting each new release like a Netflix addict . And I was thoroughly thrilled when Bang decided to join the department I work in at NIBR! Imagine getting to work with your grad school hero :).","title":"Better Design Skills: Points of View"},{"location":"newsletter/2020/06-june/#better-software-skills-document-your-tests","text":"For those of you who know me, I am a strong proponent of data scientists being equipped with good, basic software skills. When we write code in a \"basically good\" way (refactored, documented and tested), we accelerate our productivity many-fold. One of my interns reminded me of this when we realized that something that would have otherwise taken days to get right in SQL ended up being 10 minutes of work because we documented and tested our pandas DataFrame caches. (If you wish to read more about testing, I write about it on my Essays on data science .) Documenting code is important. Turns out, your test suite is also code! So in his blog post , Hyne Schlawack makes the argument that we ought to document our tests, something that has become painfully obvious in some of the projects I have worked on. His blog post, then, gets an absolute strong recommendation from me!","title":"Better Software Skills: Document your tests!"},{"location":"newsletter/2020/06-june/#work-anywhere-with-development-containers","text":"For those of you who, like myself, moonlight as a software engineer because you develop tools, this next piece might come as music to your ears: Visual Studio Code has superb support for developing a project inside a Docker container . If you try it out, I guarantee you the convenience of never having to get someone else set up with development instructions will be liberating. Since finding out about it on Thursday (28 May), I've enabled dev containers on my personal website , my Essays collection , and the pyjanitor project . In each case, Dockerhub automatically builds containers on every commit to the default branch, and those containers are referenced in the dev container configuration file, which means your local machine never has to build the container , you only have to pull it down! I also got everything working remotely, so my puny little 12\" MacBook now uses a remote GPU-enabled development server. Speaking of which, if you're interested in making an open source contribution, or wish to just test-drive dev containers on an actual project, check out the docs I wrote for the pyjanitor project !","title":"Work Anywhere with Development Containers"},{"location":"newsletter/2020/06-june/#automate-workflow-with-continuous-x","text":"I first saw what \"Continuous X\" meant when I made my first pull requests to the matplotlib project, and was hooked ever since. Having a continuous pipeline runner like Travis or Jenkins or Azure Pipelines automatically run code and style checks on every commit takes a ton of drudgery out of guaranteeing that our software works properly. It's like having a Roomba go through your kitchen every time it knows you've finished a meal. How does \"continuous X\" apply for data science projects though? Turns out, individuals way more experienced than myself and much smarter than me have been thinking about this problem too. In particular, I want to highlight two articles, one by Danilo Sato, Arif Wider and Christoph Windheuser and one on Booklet.ai . In both cases, they raise possible ways to integrate pipeline-based automation into data projects, making them robust and reproducible. Be sure to check the articles out!","title":"Automate Workflow with Continuous X"},{"location":"newsletter/2020/06-june/#from-my-collection","text":"I have two articles from my own collection to share. The first one is about how to set up a personal platform as a service (PaaS) called Dokku . It's written for those who are especially cheap (like yours truly) and don't want to pay $7/month to Heroku for each project that gets hosted there. For those of you who do want to learn the basics of Heroku-based deployment, I have a class on Skillshare that you can use too, which is being used by the Insight Data Science Fellows in Boston! The second one is about a hack to speed up data loading , using a package called cachier . It's a neat hack - especially if you wrap specific data queries from a database into a Python function!","title":"From My Collection"},{"location":"newsletter/2020/06-june/#take-a-break-have-a-bit-of-humour","text":"Let's close with some humorous stuff, if not at least to lighten the mood in these tumultuous times. Firstly, Hossein Siamaknejad actually did it : automate a game using Python. And the hack was absolutely brilliant : \"RGB detection and programmatically controlled mouse and keyboard\". Props to you, Hossein ! Secondly, the prolifically-hilarious Kareem Carr writes about \"practicing safe.... modelling\" .","title":"Take a break, have a bit of humour"},{"location":"newsletter/2020/06-june/#happy-ahem-modelling","text":"Hope you enjoyed this edition of the programmer-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. If you'd like to get early access to new written tutorials, essays, 1-on-1 consulting (I just did one session with one of my supporters!) and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"Happy, ahem, modelling :)"},{"location":"newsletter/2020/07-july/","text":"Data Science Programming July 2020 Newsletter Hello datanistas! Welcome to the July edition of the programming-oriented data science newsletter. I usually try to send the newsletter on the first Monday of the month, but this edition is a little bit later than usual, and that\u2019s because I was attending SciPy 2020\u2019s virtual conference this month! Be sure to catch the videos on Enthought\u2019s YouTube channel next week, when they are edited and uploaded! (The talks are already up, check them out!) Back to regular scheduled programming (*cough cough the SciPy puns cough*), this month\u2019s newsletter focuses on production ML systems and everything around it! On getting ML models into production Vicki Boykis has this very well-written article titled \" Getting machine learning to production \". In there, she details a lot of the struggle in getting an ML model into a production system. I found it very instructive to read. As it turns out, your ML model is kind of the least of your worries. I won\u2019t spoil it for you - take a good 10 minutes out of your day to read it! MLOps Related to ML in production is the term that is quickly becoming \"a thing\": MLOps. In the same vein as DevOps, DevSecOps etc., it\u2019s all about continuously running things to check for reproducibility of your analyses, and at least ensuring that the code continuously runs. (Checking that everything is semantically correct is still a human job that can\u2019t be eliminated.) GitHub has put together a resource to help you learn about some of the tooling to help you facilitate the automation, collaboration, and reproducibility in your ML workflows. If anything, I have found at work that continuously executed pipelines are the basic unit of engineering reliability into both my software and my models, and I\u2019d encourage you to do the same! Approach Your Data with a Product Mindset This one comes from the Harvard Business Review. Usually the HBR is a tad too suit-oriented for my tastes, but having been involved in some data products at work, this article resonated with me. Production systems usually imply something that directly impact decision-making, and \"data products\" are what help facilitate/accelerate that process. Especially if there\u2019s a focus on \"unmet needs\", that\u2019s when a data + model project can turn into something impactful. Let me not spoil the read for you, and instead come check out the article here . I hope it gives you inspiration for your work! On Technical Debt... If you\u2019ve read the paper titled \" Hidden Technical Debt in Machine Learning Systems \", then come read an article by Matthew McAteer, in which he dissects the paper and teases out which points have been made obsolete as time progressed. It\u2019s an eye-opening read ! Assortments of Goodies Some other things I have found to be important and informative include: The proposal of a DataFrame protocol for the PyData ecosystem A succinct introduction to metamorphic testing pbcopy and pbpaste , a macOS utility for copying things to the clipboard from the terminal and what I would consider to be Coiled Computing\u2019s manifesto ! (To be clear, they did not pay me to put this link in here, I\u2019m genuinely excited about what they\u2019re building!) From my collection Now for some things from my own collection that I\u2019m excited to share! Network Analysis Made Simple Each year, I submit Network Analysis Made Simple to PyCon, SciPy and PyData conferences, where they get recorded and are shared with the world for free. This year, I\u2019m super happy to announce that my co-instructor and I have revamped the website ! We spent some time restructuring the material, adding a theme that provides search, and adding a pipeline that reproducibly builds the notebook collection. For those of you who like eBook artifacts to keep, we also compiled a book! If you\u2019re interested in it, come tell us what you think the book is worth . We\u2019ll be officially launching next week, after the final chapter is added to the collection! ( Bayesian Data Science by Simulation and Probabilistic Programming is also undergoing a similar rebuild, stay tuned!) A few colleagues have also given me feedback that the Python data science ecosystem is kind of like \"the Wild Wild West\". Reflecting on my prior experience thus far, I can appreciate the sentiment, and so I sat down and wrote a long essay that tries to linearize/untangle the ecosystem for newcomers . I hope it\u2019s useful for you too :). My Patreon supporters have had early access to the article for a while, so if you appreciate the work, I\u2019d love to hear from you on Patreon! Moar Twitter Have you tried to unsubscribe from a email list and got the response that it can \"take a few days\"? Well... follow this thread to learn why! (I\u2019d love it if you\u2019d stay with this newsletter though!) Thank you for reading! Hope you enjoyed this edition of the programmer-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Next month will be a special SciPy 2020 edition, as I find time to carefully catch up and review the talks that have come by! Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"July"},{"location":"newsletter/2020/07-july/#data-science-programming-july-2020-newsletter","text":"Hello datanistas! Welcome to the July edition of the programming-oriented data science newsletter. I usually try to send the newsletter on the first Monday of the month, but this edition is a little bit later than usual, and that\u2019s because I was attending SciPy 2020\u2019s virtual conference this month! Be sure to catch the videos on Enthought\u2019s YouTube channel next week, when they are edited and uploaded! (The talks are already up, check them out!) Back to regular scheduled programming (*cough cough the SciPy puns cough*), this month\u2019s newsletter focuses on production ML systems and everything around it!","title":"Data Science Programming July 2020 Newsletter"},{"location":"newsletter/2020/07-july/#on-getting-ml-models-into-production","text":"Vicki Boykis has this very well-written article titled \" Getting machine learning to production \". In there, she details a lot of the struggle in getting an ML model into a production system. I found it very instructive to read. As it turns out, your ML model is kind of the least of your worries. I won\u2019t spoil it for you - take a good 10 minutes out of your day to read it!","title":"On getting ML models into production"},{"location":"newsletter/2020/07-july/#mlops","text":"Related to ML in production is the term that is quickly becoming \"a thing\": MLOps. In the same vein as DevOps, DevSecOps etc., it\u2019s all about continuously running things to check for reproducibility of your analyses, and at least ensuring that the code continuously runs. (Checking that everything is semantically correct is still a human job that can\u2019t be eliminated.) GitHub has put together a resource to help you learn about some of the tooling to help you facilitate the automation, collaboration, and reproducibility in your ML workflows. If anything, I have found at work that continuously executed pipelines are the basic unit of engineering reliability into both my software and my models, and I\u2019d encourage you to do the same!","title":"MLOps"},{"location":"newsletter/2020/07-july/#approach-your-data-with-a-product-mindset","text":"This one comes from the Harvard Business Review. Usually the HBR is a tad too suit-oriented for my tastes, but having been involved in some data products at work, this article resonated with me. Production systems usually imply something that directly impact decision-making, and \"data products\" are what help facilitate/accelerate that process. Especially if there\u2019s a focus on \"unmet needs\", that\u2019s when a data + model project can turn into something impactful. Let me not spoil the read for you, and instead come check out the article here . I hope it gives you inspiration for your work!","title":"Approach Your Data with a Product Mindset"},{"location":"newsletter/2020/07-july/#on-technical-debt","text":"If you\u2019ve read the paper titled \" Hidden Technical Debt in Machine Learning Systems \", then come read an article by Matthew McAteer, in which he dissects the paper and teases out which points have been made obsolete as time progressed. It\u2019s an eye-opening read !","title":"On Technical Debt..."},{"location":"newsletter/2020/07-july/#assortments-of-goodies","text":"Some other things I have found to be important and informative include: The proposal of a DataFrame protocol for the PyData ecosystem A succinct introduction to metamorphic testing pbcopy and pbpaste , a macOS utility for copying things to the clipboard from the terminal and what I would consider to be Coiled Computing\u2019s manifesto ! (To be clear, they did not pay me to put this link in here, I\u2019m genuinely excited about what they\u2019re building!)","title":"Assortments of Goodies"},{"location":"newsletter/2020/07-july/#from-my-collection","text":"Now for some things from my own collection that I\u2019m excited to share!","title":"From my collection"},{"location":"newsletter/2020/07-july/#network-analysis-made-simple","text":"Each year, I submit Network Analysis Made Simple to PyCon, SciPy and PyData conferences, where they get recorded and are shared with the world for free. This year, I\u2019m super happy to announce that my co-instructor and I have revamped the website ! We spent some time restructuring the material, adding a theme that provides search, and adding a pipeline that reproducibly builds the notebook collection. For those of you who like eBook artifacts to keep, we also compiled a book! If you\u2019re interested in it, come tell us what you think the book is worth . We\u2019ll be officially launching next week, after the final chapter is added to the collection! ( Bayesian Data Science by Simulation and Probabilistic Programming is also undergoing a similar rebuild, stay tuned!) A few colleagues have also given me feedback that the Python data science ecosystem is kind of like \"the Wild Wild West\". Reflecting on my prior experience thus far, I can appreciate the sentiment, and so I sat down and wrote a long essay that tries to linearize/untangle the ecosystem for newcomers . I hope it\u2019s useful for you too :). My Patreon supporters have had early access to the article for a while, so if you appreciate the work, I\u2019d love to hear from you on Patreon!","title":"Network Analysis Made Simple"},{"location":"newsletter/2020/07-july/#moar-twitter","text":"Have you tried to unsubscribe from a email list and got the response that it can \"take a few days\"? Well... follow this thread to learn why! (I\u2019d love it if you\u2019d stay with this newsletter though!)","title":"Moar Twitter"},{"location":"newsletter/2020/07-july/#thank-you-for-reading","text":"Hope you enjoyed this edition of the programmer-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Next month will be a special SciPy 2020 edition, as I find time to carefully catch up and review the talks that have come by! Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"Thank you for reading!"},{"location":"newsletter/2020/08-august/","text":"Data Science Programming August 2020 Newsletter Hello fellow datanistas! Welcome to the August edition of the programming-oriented data science newsletter! This edition of the newsletter has a large dose of SciPy 2020 inside it. I participated in the conference as a tutorial instructor (and as one of the Financial Aid chairs), though I did also miss Austin TX food! (Seriously, Texan BBQ is one of the best!) If you're interested in seeing the whole playlist, check it out on YouTube ! If not, come check out some of those that I've watched and liked, my subset curated for you! Frictionless Data for Reproducible Biology by Lilly Winfree The reason I like this talk is primarily because of the idea of \"Data Packages\", where raw data and its metadata are packaged in a machine-readable format. In my mind, I'm contrasting this idea against the large-scale data collection efforts; in biosciences, many datasets are small and designed for one question, but may be useful for other problems by providing, for example, useful priors on parameters. Here, a data package helps users ship and distribute a self-contained unit of data that others can build on top of. I'm imagining many cool use cases, both in public-facing research and in internal-facing workflows! Continuous Integration for Scientific Python Projects by Stanley Seibert In this talk, Stan Seibert (one of the Numba core developers) speaks about the advantages of standing up a continuous integration pipeline for your code, as well as challenges that you'll encounter along the way. I find this to be a useful video for data scientists, because in it Stan gives a good overview of what to look out for. Awkward Array: Manipulating JSON like Data with NumPy like Idioms by Jim Pivarski This one has to be one of my favourite talks, because the package featured in there has an awesome name, brings over NumPy idioms and semantics into world of nested and \"awkwardly\"-structured data. JAX: Accelerated Machine Learning Research by Jake Vanderplas I'm a fan of the NumPy API because it's the array-computing lingua franca of the Python world, and I strongly believe that targeting a common API (and evolving it in a community-oriented fashion) is the right way to build the PyData ecosystem. JAX does this by making array-oriented automatic differentiation, GPU/TPU acceleration, just-in-time compilation, and vectorized mapping all first-class citizens alongside the idiomatic NumPy API. I love it and totally dig it! And I use it for research and production at work. I'd encourage you to try it out too! matplotlib Data Model by Hannah Aizenman If you use matplotlib , then this Maintainer's track talk by Hannah Aizenman is going to make your eyes light up! In here, she talks about CZI-funded work to refactor the data model underneath matplotlib , which will enable a ton of really cool things downstream. I'm not going to spoil it for you; check it out! (And also check out the other cool talks by the other maintainers!) Interactive Supercomputing with Jupyter at NERSC by Rollin Thomas I think this is a great case study talk that shows how JupyterHub is used at a research institution to help facilitate computational research. If your organization is thinking about setting something up, I think this talk will give you valuable insights and lessons! Tutorials If I really wanted to, I would have listed all 10 tutorials down here amongst my recommendations, but I know you came for a curation. Here's the two that I think are most generally useful for data scientists: Introduction to Conda for (Data) Scientists : This being such a foundational tool for distributing data science packages, I think it's work getting our mental models straightened out! Jupyter Interactive Widget Ecosystem : With Jupyter notebooks being so idiomatic, and with widgets being so useful for dashboarding, pedagogy and more, this one is an easy recommendation! But seriously, check out all 10 of them! From my collection Here's a few snippets of my participation this year at SciPy! Call prediction, prediction, not inference! (My ~~rant~~ lightning talk at SciPy.) Bayesian Data Science by Simulation (tutorial I led, based on material I co-developed with Hugo Bowne-Anderson!) In some other news, the Network Analysis Made Simple eBook has launched ! In line with my personal philosophy of democratizing access to learning material, everything is freely available online , but if you'd like to support us (mostly by keeping us caffeinated) or would like an offline copy to keep that will be kept up-to-date for life, please consider purchasing a copy! Thank you for reading! Alrighty, I shan't toot my own horn anymore. I hope you enjoyed this special SciPy curation edition of the programming-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Next month, we resume regular scheduled, ahem, programming! Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"August"},{"location":"newsletter/2020/08-august/#data-science-programming-august-2020-newsletter","text":"Hello fellow datanistas! Welcome to the August edition of the programming-oriented data science newsletter! This edition of the newsletter has a large dose of SciPy 2020 inside it. I participated in the conference as a tutorial instructor (and as one of the Financial Aid chairs), though I did also miss Austin TX food! (Seriously, Texan BBQ is one of the best!) If you're interested in seeing the whole playlist, check it out on YouTube ! If not, come check out some of those that I've watched and liked, my subset curated for you!","title":"Data Science Programming August 2020 Newsletter"},{"location":"newsletter/2020/08-august/#frictionless-data-for-reproducible-biology-by-lilly-winfree","text":"The reason I like this talk is primarily because of the idea of \"Data Packages\", where raw data and its metadata are packaged in a machine-readable format. In my mind, I'm contrasting this idea against the large-scale data collection efforts; in biosciences, many datasets are small and designed for one question, but may be useful for other problems by providing, for example, useful priors on parameters. Here, a data package helps users ship and distribute a self-contained unit of data that others can build on top of. I'm imagining many cool use cases, both in public-facing research and in internal-facing workflows!","title":"Frictionless Data for Reproducible Biology by Lilly Winfree"},{"location":"newsletter/2020/08-august/#continuous-integration-for-scientific-python-projects-by-stanley-seibert","text":"In this talk, Stan Seibert (one of the Numba core developers) speaks about the advantages of standing up a continuous integration pipeline for your code, as well as challenges that you'll encounter along the way. I find this to be a useful video for data scientists, because in it Stan gives a good overview of what to look out for.","title":"Continuous Integration for Scientific Python Projects by Stanley Seibert"},{"location":"newsletter/2020/08-august/#awkward-array-manipulating-json-like-data-with-numpy-like-idioms-by-jim-pivarski","text":"This one has to be one of my favourite talks, because the package featured in there has an awesome name, brings over NumPy idioms and semantics into world of nested and \"awkwardly\"-structured data.","title":"Awkward Array: Manipulating JSON like Data with NumPy like Idioms by Jim Pivarski"},{"location":"newsletter/2020/08-august/#jax-accelerated-machine-learning-research-by-jake-vanderplas","text":"I'm a fan of the NumPy API because it's the array-computing lingua franca of the Python world, and I strongly believe that targeting a common API (and evolving it in a community-oriented fashion) is the right way to build the PyData ecosystem. JAX does this by making array-oriented automatic differentiation, GPU/TPU acceleration, just-in-time compilation, and vectorized mapping all first-class citizens alongside the idiomatic NumPy API. I love it and totally dig it! And I use it for research and production at work. I'd encourage you to try it out too!","title":"JAX: Accelerated Machine Learning Research by Jake Vanderplas"},{"location":"newsletter/2020/08-august/#matplotlib-data-model-by-hannah-aizenman","text":"If you use matplotlib , then this Maintainer's track talk by Hannah Aizenman is going to make your eyes light up! In here, she talks about CZI-funded work to refactor the data model underneath matplotlib , which will enable a ton of really cool things downstream. I'm not going to spoil it for you; check it out! (And also check out the other cool talks by the other maintainers!)","title":"matplotlib Data Model by Hannah Aizenman"},{"location":"newsletter/2020/08-august/#interactive-supercomputing-with-jupyter-at-nersc-by-rollin-thomas","text":"I think this is a great case study talk that shows how JupyterHub is used at a research institution to help facilitate computational research. If your organization is thinking about setting something up, I think this talk will give you valuable insights and lessons!","title":"Interactive Supercomputing with Jupyter at NERSC by Rollin Thomas"},{"location":"newsletter/2020/08-august/#tutorials","text":"If I really wanted to, I would have listed all 10 tutorials down here amongst my recommendations, but I know you came for a curation. Here's the two that I think are most generally useful for data scientists: Introduction to Conda for (Data) Scientists : This being such a foundational tool for distributing data science packages, I think it's work getting our mental models straightened out! Jupyter Interactive Widget Ecosystem : With Jupyter notebooks being so idiomatic, and with widgets being so useful for dashboarding, pedagogy and more, this one is an easy recommendation! But seriously, check out all 10 of them!","title":"Tutorials"},{"location":"newsletter/2020/08-august/#from-my-collection","text":"Here's a few snippets of my participation this year at SciPy! Call prediction, prediction, not inference! (My ~~rant~~ lightning talk at SciPy.) Bayesian Data Science by Simulation (tutorial I led, based on material I co-developed with Hugo Bowne-Anderson!) In some other news, the Network Analysis Made Simple eBook has launched ! In line with my personal philosophy of democratizing access to learning material, everything is freely available online , but if you'd like to support us (mostly by keeping us caffeinated) or would like an offline copy to keep that will be kept up-to-date for life, please consider purchasing a copy!","title":"From my collection"},{"location":"newsletter/2020/08-august/#thank-you-for-reading","text":"Alrighty, I shan't toot my own horn anymore. I hope you enjoyed this special SciPy curation edition of the programming-oriented data science newsletter! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Next month, we resume regular scheduled, ahem, programming! Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"Thank you for reading!"},{"location":"newsletter/2020/09-september/","text":"Data Science Programming September 2020 Newsletter Hello fellow datanistas! Welcome to the September edition of the programming-oriented data science newsletter. I hope you've all been staying safe amid the COVID-19 outbreak. There's no special theme this month, just a smattering of cool tools and articles that I think will improve your productivity! Setting up VSCode for Python Development like RStudio Firstly, a blog post by Steven Mortimer on how to set up VSCode, which is a really awesome IDE, in such a way that it behaves like RStudio. For R users who have to transition over to Python (e.g. for work, or for personal interest), this should help bridge the gap a bit! Pylance in VSCode Speaking of VSCode, I have been test-driving Pylance in my workflow at work, and it's blazing fast and performant for code checking! As I was writing my code, the Pylance VSCode extension continually checked my code, helping me to catch execution errors before I even executed the code. Amazing stuff, Microsoft, I like what you've become now :). ECDFs are in Seaborn! Since learning about ECDFs a few years ago, I have advocated for visualizing distributions of data using ECDFs rather than histograms . Well, nothing beats having best practices available conveniently, so I'm super happy to see ECDFs conveniently available in seaborn! Stupid Simple Kubernetes From experience at work, I can vouch for the idea that it's completely worthwhile for a data scientist to learn the ideas around containers, Kubernetes included. To help get up to speed, my colleague Zach Barry found an awesome article to help, titled \" Stupid Simple Kubernetes \". Lots of terms in the K8 world get clarified in that article. I hope you enjoy it! Learn in Public This is an article that resonated deeply with me. Learning in public has been, for me, the biggest career hack that I have experienced. Now, Shawn Wang has articulated clearly the benefits of doing so! The biggest is being able to build a public-facing portfolio that you can point to that demonstrates your skill set. From my collection Some things I recently wrote about: Software skills are important, for it helps us data scientists think clearly . Some early thoughts test-driving pandera for data validation. .also() , which comes from the Kotlin programming language, proposed in pyjanitor as a new feature - I'm excited to see where this one goes! I'll be speaking at JupyterCon 2020 this year! Super excited to release a talk on how we compiled Network Analysis Made Simple into our eBook and website ! A plug for an awesome open source contributor The final thing I'd like to include in this newsletter is a completely unsolicited but heartfelt advertisement for Samuel Oranyeli . He's been a consistent contributor to the pyjanitor project, and I have witnessed his skills growth over the past few months of contribution. The most important quality he possesses is consistent learning! If you're hiring for a Python developer in the Sydney, Australia area or remotely, do consider him on your list! Thank you for reading! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"September"},{"location":"newsletter/2020/09-september/#data-science-programming-september-2020-newsletter","text":"Hello fellow datanistas! Welcome to the September edition of the programming-oriented data science newsletter. I hope you've all been staying safe amid the COVID-19 outbreak. There's no special theme this month, just a smattering of cool tools and articles that I think will improve your productivity!","title":"Data Science Programming September 2020 Newsletter"},{"location":"newsletter/2020/09-september/#setting-up-vscode-for-python-development-like-rstudio","text":"Firstly, a blog post by Steven Mortimer on how to set up VSCode, which is a really awesome IDE, in such a way that it behaves like RStudio. For R users who have to transition over to Python (e.g. for work, or for personal interest), this should help bridge the gap a bit!","title":"Setting up VSCode for Python Development like RStudio"},{"location":"newsletter/2020/09-september/#pylance-in-vscode","text":"Speaking of VSCode, I have been test-driving Pylance in my workflow at work, and it's blazing fast and performant for code checking! As I was writing my code, the Pylance VSCode extension continually checked my code, helping me to catch execution errors before I even executed the code. Amazing stuff, Microsoft, I like what you've become now :).","title":"Pylance in VSCode"},{"location":"newsletter/2020/09-september/#ecdfs-are-in-seaborn","text":"Since learning about ECDFs a few years ago, I have advocated for visualizing distributions of data using ECDFs rather than histograms . Well, nothing beats having best practices available conveniently, so I'm super happy to see ECDFs conveniently available in seaborn!","title":"ECDFs are in Seaborn!"},{"location":"newsletter/2020/09-september/#stupid-simple-kubernetes","text":"From experience at work, I can vouch for the idea that it's completely worthwhile for a data scientist to learn the ideas around containers, Kubernetes included. To help get up to speed, my colleague Zach Barry found an awesome article to help, titled \" Stupid Simple Kubernetes \". Lots of terms in the K8 world get clarified in that article. I hope you enjoy it!","title":"Stupid Simple Kubernetes"},{"location":"newsletter/2020/09-september/#learn-in-public","text":"This is an article that resonated deeply with me. Learning in public has been, for me, the biggest career hack that I have experienced. Now, Shawn Wang has articulated clearly the benefits of doing so! The biggest is being able to build a public-facing portfolio that you can point to that demonstrates your skill set.","title":"Learn in Public"},{"location":"newsletter/2020/09-september/#from-my-collection","text":"Some things I recently wrote about: Software skills are important, for it helps us data scientists think clearly . Some early thoughts test-driving pandera for data validation. .also() , which comes from the Kotlin programming language, proposed in pyjanitor as a new feature - I'm excited to see where this one goes! I'll be speaking at JupyterCon 2020 this year! Super excited to release a talk on how we compiled Network Analysis Made Simple into our eBook and website !","title":"From my collection"},{"location":"newsletter/2020/09-september/#a-plug-for-an-awesome-open-source-contributor","text":"The final thing I'd like to include in this newsletter is a completely unsolicited but heartfelt advertisement for Samuel Oranyeli . He's been a consistent contributor to the pyjanitor project, and I have witnessed his skills growth over the past few months of contribution. The most important quality he possesses is consistent learning! If you're hiring for a Python developer in the Sydney, Australia area or remotely, do consider him on your list!","title":"A plug for an awesome open source contributor"},{"location":"newsletter/2020/09-september/#thank-you-for-reading","text":"As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay indoors, and keep hacking! Cheers, Eric","title":"Thank you for reading!"},{"location":"newsletter/2020/10-october/","text":"Data Science Programming October 2020 Newsletter Hello fellow datanistas! Welcome to the October edition of the programming-oriented data science newsletter. As the weather chills down, hope you all are staying warm indoors, and safe both indoors and outdoors! This edition of the Data Science Programming Newsletter has a particular focus on machine learning engineering, which is a discipline that is evolving out of the old \"data science\" umbrella into its own. Effective testing for machine learning systems Recently at work, I've been building some bespoke machine learning models (autoregressive hidden Markov models and graph neural networks) for scientific problems that we encounter. In building those bespoke models, because we aren't using standard reference libraries, we have to build the model code from scratch. Since it's software, it needs tests, and Jeremy Jordan has a great blog post on how to effectively test ML systems. Definitely worth a read! Software engineering fundamentals for Data Scientists In his Medium article, Gonzalo Ferreiro Volpi shares some fundamentals software skills for data scientists. For those of you who want to invest in levelling up your code-writing skills to reap multiplicative dividends in time saved, frustrations avoided, and happiness multiplied, come check it out! Reflecting on a year of making machine learning actually useful In her blog post, Shreya Shankar has some extremely valuable insights into the practice of making ML useful in the real world, which I absolutely agree with. One, in particular, being the quote: Outside of ML classes and research, I learned that often the most reliable way to get performance improvements is to find another piece of data which gives insight into a completely new aspect of the problem, rather than to add a tweak to the loss. Whenever model performance is bad, we (scientists and practitioners) shouldn\u2019t only resort to investigating model architecture and parameters. We should also be thinking about \u201cculprits\u201d of bad performance in the data. Reminds me of the power of finding \"the invariants\" of a problem. With that little teaser, I hope this gives you enough impetus to read it! The Multiplicative Power of Masks This article is one that is topical and relevant. I also appreciated the illustrations put in there! Also, it's a blog post that highlights a really powerful model -- where powerful doesn't mean millions of parameters, but rather conceptually simple, easy to communicate, broadly applicable, and intensely relevant for the times. Aatish Bhatia has done a tremendously wonderful job here with this explanation. It's a technical masterpiece. From my collection Some colleagues had questions about environment variables, so I decided to surface up an old post on the topic and spruce it up with more information on my essays collection. I moved data across work sites securely and as fast a commercial tools using nothing but free and open source tooling. Come read how! I also recently figured out how to directly open a Jupyter notebook in a Binder session. The hack is really cool! Finally, some more Twitter humour from the ever on fire Kareem Carr :). Thank you for reading! As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay hacking :) Cheers, Eric","title":"Data Science Programming October 2020 Newsletter"},{"location":"newsletter/2020/10-october/#data-science-programming-october-2020-newsletter","text":"Hello fellow datanistas! Welcome to the October edition of the programming-oriented data science newsletter. As the weather chills down, hope you all are staying warm indoors, and safe both indoors and outdoors! This edition of the Data Science Programming Newsletter has a particular focus on machine learning engineering, which is a discipline that is evolving out of the old \"data science\" umbrella into its own.","title":"Data Science Programming October 2020 Newsletter"},{"location":"newsletter/2020/10-october/#effective-testing-for-machine-learning-systems","text":"Recently at work, I've been building some bespoke machine learning models (autoregressive hidden Markov models and graph neural networks) for scientific problems that we encounter. In building those bespoke models, because we aren't using standard reference libraries, we have to build the model code from scratch. Since it's software, it needs tests, and Jeremy Jordan has a great blog post on how to effectively test ML systems. Definitely worth a read!","title":"Effective testing for machine learning systems"},{"location":"newsletter/2020/10-october/#software-engineering-fundamentals-for-data-scientists","text":"In his Medium article, Gonzalo Ferreiro Volpi shares some fundamentals software skills for data scientists. For those of you who want to invest in levelling up your code-writing skills to reap multiplicative dividends in time saved, frustrations avoided, and happiness multiplied, come check it out!","title":"Software engineering fundamentals for Data Scientists"},{"location":"newsletter/2020/10-october/#reflecting-on-a-year-of-making-machine-learning-actually-useful","text":"In her blog post, Shreya Shankar has some extremely valuable insights into the practice of making ML useful in the real world, which I absolutely agree with. One, in particular, being the quote: Outside of ML classes and research, I learned that often the most reliable way to get performance improvements is to find another piece of data which gives insight into a completely new aspect of the problem, rather than to add a tweak to the loss. Whenever model performance is bad, we (scientists and practitioners) shouldn\u2019t only resort to investigating model architecture and parameters. We should also be thinking about \u201cculprits\u201d of bad performance in the data. Reminds me of the power of finding \"the invariants\" of a problem. With that little teaser, I hope this gives you enough impetus to read it!","title":"Reflecting on a year of making machine learning actually useful"},{"location":"newsletter/2020/10-october/#the-multiplicative-power-of-masks","text":"This article is one that is topical and relevant. I also appreciated the illustrations put in there! Also, it's a blog post that highlights a really powerful model -- where powerful doesn't mean millions of parameters, but rather conceptually simple, easy to communicate, broadly applicable, and intensely relevant for the times. Aatish Bhatia has done a tremendously wonderful job here with this explanation. It's a technical masterpiece.","title":"The Multiplicative Power of Masks"},{"location":"newsletter/2020/10-october/#from-my-collection","text":"Some colleagues had questions about environment variables, so I decided to surface up an old post on the topic and spruce it up with more information on my essays collection. I moved data across work sites securely and as fast a commercial tools using nothing but free and open source tooling. Come read how! I also recently figured out how to directly open a Jupyter notebook in a Binder session. The hack is really cool! Finally, some more Twitter humour from the ever on fire Kareem Carr :).","title":"From my collection"},{"location":"newsletter/2020/10-october/#thank-you-for-reading","text":"As always, let me know on Twitter if you've enjoyed the newsletter, and I'm always open to hearing about the new things you've learned from it. Meanwhile, if you'd like to get early access to new written tutorials, essays, 1-on-1 consulting and complimentary access to the Skillshare workshops that I make, I'd appreciate your support on Patreon ! Stay safe, stay hacking :) Cheers, Eric","title":"Thank you for reading!"},{"location":"software-skills/","text":"Software Skills Because our day-to-day involves writing code, I am convinced that we data scientists need to be equipped with basic software engineering skills. Being equipped with these skills will help us write code that is, in the long-run, easy to recap, remember, reference, review, and rewrite. In this collection of short essays, I will highlight the basic software skills that, if we master, will increase our efficiency and effectiveness in the long-run. Common Objections If you have heard these suggestions before, then you might have also heard some of the common objections to learning these software practices. I wish to address them here in bulk, so I do not have to address them in-depth in the individual essays. I have not enough time This objection is one I am sympathetic to, as I operate under time constraints myself. This is the nature of code: written once, used many times. Hence, the best response that I can give is that time taken cutting corners now yields multiples of others' (including your future self's) time wasted navigating an undocumented, spaghetti-code codebase, that is not well-structured either. Cutting out these software practices now makes things much more difficult to maintain and improve code when it goes into production. My code is only going to be written and read by myself At some point, though, there is a high probability that you will end up writing code that someone else has to read and use. The time invested in making the code read well now , even on code that does not have to be read by others, will reduce the learning curve pain when you eventually do have to write code for others. You might as well invest the time now while there's less formal scrutiny to practice your software skills. When the stakes are higher, being ready can only be helpful. I don't know how to get started, there are so many places to begin Pick any one skill, say, refactoring, and work on it first. You can always add on more skills into your toolkit as you go along. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Importance"},{"location":"software-skills/#software-skills","text":"Because our day-to-day involves writing code, I am convinced that we data scientists need to be equipped with basic software engineering skills. Being equipped with these skills will help us write code that is, in the long-run, easy to recap, remember, reference, review, and rewrite. In this collection of short essays, I will highlight the basic software skills that, if we master, will increase our efficiency and effectiveness in the long-run.","title":"Software Skills"},{"location":"software-skills/#common-objections","text":"If you have heard these suggestions before, then you might have also heard some of the common objections to learning these software practices. I wish to address them here in bulk, so I do not have to address them in-depth in the individual essays.","title":"Common Objections"},{"location":"software-skills/#i-have-not-enough-time","text":"This objection is one I am sympathetic to, as I operate under time constraints myself. This is the nature of code: written once, used many times. Hence, the best response that I can give is that time taken cutting corners now yields multiples of others' (including your future self's) time wasted navigating an undocumented, spaghetti-code codebase, that is not well-structured either. Cutting out these software practices now makes things much more difficult to maintain and improve code when it goes into production.","title":"I have not enough time"},{"location":"software-skills/#my-code-is-only-going-to-be-written-and-read-by-myself","text":"At some point, though, there is a high probability that you will end up writing code that someone else has to read and use. The time invested in making the code read well now , even on code that does not have to be read by others, will reduce the learning curve pain when you eventually do have to write code for others. You might as well invest the time now while there's less formal scrutiny to practice your software skills. When the stakes are higher, being ready can only be helpful.","title":"My code is only going to be written and read by myself"},{"location":"software-skills/#i-dont-know-how-to-get-started-there-are-so-many-places-to-begin","text":"Pick any one skill, say, refactoring, and work on it first. You can always add on more skills into your toolkit as you go along.","title":"I don't know how to get started, there are so many places to begin"},{"location":"software-skills/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"software-skills/code-formatting/","text":"Formatting your code One key insight from the Python programming language is that code is read more often than it is written. Hence, writing code in a fashion that makes it easy to read is something that can only be beneficial. But formatting code is a nit-picky and tedious matter, isn't it? Moreover, code style is one of those things that are not substantive enough to engage in flame wars. It really is one of those things we should just get over with, right? Yes, and it is possible to be \"just over and done with it\" if we use automation tools to help us take care of code formatting so that we don't have to think about it. Introducing black black is an opinionated code formatter for the Python programming language. It comes with sane defaults, and produces consistently formatted code with a single command at the terminal. Installing black To install it, we can either use pip or conda : # for pip users pip install black # for conda users conda install black Using black We can use black directly at the command line in our project directory, with configurations called at the command line for convenience. # Format all .py files within and underneath current working directory. black -l 79 . Introducing isort isort is a package for sorting your imports in a source .py file. Once again, this is the sort of thing you definitely don't want to do by hand. Installing isort isort is also conda- and pip-installable. # pip users pip install isort # conda users conda install isort Using isort Just like with black, we can use isort to automagically sort our imports. As an example we will call it at the command line with certain options enabled. # -r: recurses down below the current working directory. # -y: automatically overwrite original source file with sorted imports. isort -r -y . Building automation for code formatting Automatically executing automagic commands is pretty awesome. Let's see how we can enable this. Makefiles I also place black as part of a series of commands used in code style checking in a Makefile, to run all of those commands together. format : isort -r -y . black -l 79 . With that Makefile command, we can now execute all code formatting commands with a single call. Side note: I usually do isort first because black will make detect isort -ed code as not properly formatted, hence I defer to black to make the final changes. Pre-commit hooks We can also use pre-commit hooks to catch non-properly-formatted code, and run the code formatters over the code, preventing them from being merged if any formatting has to take place. This ensures thatwe never commit code that is incorrectly formatted. Getting set up with pre-commit hooks is another topic, but there are already great resources that can be searched for online on how to get setup. Concluding words I hope this short essay gives you an overview of the tools that you can use to format your code automatically. Code formatting is important for readability, but isn't worth the tedium. Letting automation save your time is the wise thing to do. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Formatting your code"},{"location":"software-skills/code-formatting/#formatting-your-code","text":"One key insight from the Python programming language is that code is read more often than it is written. Hence, writing code in a fashion that makes it easy to read is something that can only be beneficial. But formatting code is a nit-picky and tedious matter, isn't it? Moreover, code style is one of those things that are not substantive enough to engage in flame wars. It really is one of those things we should just get over with, right? Yes, and it is possible to be \"just over and done with it\" if we use automation tools to help us take care of code formatting so that we don't have to think about it.","title":"Formatting your code"},{"location":"software-skills/code-formatting/#introducing-black","text":"black is an opinionated code formatter for the Python programming language. It comes with sane defaults, and produces consistently formatted code with a single command at the terminal.","title":"Introducing black"},{"location":"software-skills/code-formatting/#installing-black","text":"To install it, we can either use pip or conda : # for pip users pip install black # for conda users conda install black","title":"Installing black"},{"location":"software-skills/code-formatting/#using-black","text":"We can use black directly at the command line in our project directory, with configurations called at the command line for convenience. # Format all .py files within and underneath current working directory. black -l 79 .","title":"Using black"},{"location":"software-skills/code-formatting/#introducing-isort","text":"isort is a package for sorting your imports in a source .py file. Once again, this is the sort of thing you definitely don't want to do by hand.","title":"Introducing isort"},{"location":"software-skills/code-formatting/#installing-isort","text":"isort is also conda- and pip-installable. # pip users pip install isort # conda users conda install isort","title":"Installing isort"},{"location":"software-skills/code-formatting/#using-isort","text":"Just like with black, we can use isort to automagically sort our imports. As an example we will call it at the command line with certain options enabled. # -r: recurses down below the current working directory. # -y: automatically overwrite original source file with sorted imports. isort -r -y .","title":"Using isort"},{"location":"software-skills/code-formatting/#building-automation-for-code-formatting","text":"Automatically executing automagic commands is pretty awesome. Let's see how we can enable this.","title":"Building automation for code formatting"},{"location":"software-skills/code-formatting/#makefiles","text":"I also place black as part of a series of commands used in code style checking in a Makefile, to run all of those commands together. format : isort -r -y . black -l 79 . With that Makefile command, we can now execute all code formatting commands with a single call. Side note: I usually do isort first because black will make detect isort -ed code as not properly formatted, hence I defer to black to make the final changes.","title":"Makefiles"},{"location":"software-skills/code-formatting/#pre-commit-hooks","text":"We can also use pre-commit hooks to catch non-properly-formatted code, and run the code formatters over the code, preventing them from being merged if any formatting has to take place. This ensures thatwe never commit code that is incorrectly formatted. Getting set up with pre-commit hooks is another topic, but there are already great resources that can be searched for online on how to get setup.","title":"Pre-commit hooks"},{"location":"software-skills/code-formatting/#concluding-words","text":"I hope this short essay gives you an overview of the tools that you can use to format your code automatically. Code formatting is important for readability, but isn't worth the tedium. Letting automation save your time is the wise thing to do.","title":"Concluding words"},{"location":"software-skills/code-formatting/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"software-skills/documentation/","text":"Documenting your code Writing lightweight documentation is a practice that I found sorely lacking in data science practice. In this essay, I will show you how to introduce lightweight documentation into your code. Why document your code There are a few good reasons to document your code. Firstly, your future self will thank you for having a plain English translation of what you intended to do with that block of code. Oftentimes, the intent behind the code is lost in the translation from our heads to actual code. Secondly, other readers of your code will also thank you. Thirdly, by clarifying what exactly you intended to accomplish with a block of code, as well as the major steps taken towards accomplishing those goals, you often will end up with a much cleaner implementation in the end. When to document your code A pragmatic choice would be once you find yourself accomplishing a logical chunk of work. I usually do it as soon as I define a Python function. Where your code documentation should go As a general rule of thumb, having code documentation as close to the actual source code is probably the best way to approach this. For Python programmers, this would imply taking advantage of docstrings ! Docstrings occur in the following places: Right after a function or class method definition. Right inside a class definition. Right at the top of a .py module. An anti-pattern here would be writing your documentation in an external system, such as a Wiki. (Woe betide the code developer who writes code docs in Confluence...) This is because the documentation is not proximal to the source code. I have found myself forgetting to update the docstrings after updating the source code. If it's easy to forget to update the docs when the docs are right next to the source, imagine how much easier it is to forget to update external docs! Where, then, would documentation on how the code is organized live then? I would argue it should be pushed as close to the source code as possible. For example, we can use the .py module docstrings to describe the intent behind why certain entire modules exist. An example Here is a skeleton to follow: \"\"\" This module houses all functions that cannot be neatly categorized in other places. \"\"\" def my_function ( arg1 , arg2 ): \"\"\" Calculates something based on arg1 and arg2. This calculated thing is intended to be used by `this_other_function`, so the return type should not be changed. :param arg1: Describe arg1 :param arg2: Describe arg2 :returns: ``the_thing_being_returned``, a pandas DataFrame (for example). \"\"\" the_thing_being_returned = ... # implement the function return the_thing_being_returned Now, let's see this in action with a function that returns a snake-cased version of a string with all punctuation also removed. (This is a simplified implementation of what is implemented in pyjanitor 's clean_names function.) import string def clean_string ( s ): \"\"\" Remove all punctuation from string, and convert to lower_snake_case. An example of the input and output: \"My string!\" -> \"my_string\" :param s: String to clean. \"\"\" s = s . replace ( string . punctuation , \"_\" ) . replace ( \" \" , \"_\" ) . strip ( \"_\" ) . lower () return s You may notice that the docstring is longer than the implementation. Frequently (though not always), I have found that when docstring length exceeds implementation length, it is a sign that the author(s) of the code have been thoughtful about its implementation. This bodes well for working in a team, especially when a data scientist hands over a prototype to the engineering team. Addressing objections The main objections to injecting \"basic software engineering\" into a data scientist's workflow usually center around not having enough time. As always, I am sympathetic to this objection, because I also operate under time constraints. One thing I will offer is that docs are an investment of time for the team, rather than for the individual. We save multiples of time downstream when we write good docs. One way to conceptualize this is the number of person-hours saved down the road by oneself and one's teammates when good docs exist. We minimize the amount of time spent reading code to grok what it is about. At the same time, the practice of clarifying what we intend to accomplish with the function can help bring clarity to the implementation. This I have mentioned above. Having a clean implementation makes things easier to maintain later on. Hence, time invested now on good docs also helps us later on. As with other software engineering skills, this is a skill that can be picked up, refined, and honed. We get more efficient at writing docs the more we do it. Parting words I hope this essay has helped you get a feel for how you can write well-documented code. At the same time, I hope that by showing you a simple anchoring example that you will be able to replicate the pattern in your own work. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Documenting your code"},{"location":"software-skills/documentation/#documenting-your-code","text":"Writing lightweight documentation is a practice that I found sorely lacking in data science practice. In this essay, I will show you how to introduce lightweight documentation into your code.","title":"Documenting your code"},{"location":"software-skills/documentation/#why-document-your-code","text":"There are a few good reasons to document your code. Firstly, your future self will thank you for having a plain English translation of what you intended to do with that block of code. Oftentimes, the intent behind the code is lost in the translation from our heads to actual code. Secondly, other readers of your code will also thank you. Thirdly, by clarifying what exactly you intended to accomplish with a block of code, as well as the major steps taken towards accomplishing those goals, you often will end up with a much cleaner implementation in the end.","title":"Why document your code"},{"location":"software-skills/documentation/#when-to-document-your-code","text":"A pragmatic choice would be once you find yourself accomplishing a logical chunk of work. I usually do it as soon as I define a Python function.","title":"When to document your code"},{"location":"software-skills/documentation/#where-your-code-documentation-should-go","text":"As a general rule of thumb, having code documentation as close to the actual source code is probably the best way to approach this. For Python programmers, this would imply taking advantage of docstrings ! Docstrings occur in the following places: Right after a function or class method definition. Right inside a class definition. Right at the top of a .py module. An anti-pattern here would be writing your documentation in an external system, such as a Wiki. (Woe betide the code developer who writes code docs in Confluence...) This is because the documentation is not proximal to the source code. I have found myself forgetting to update the docstrings after updating the source code. If it's easy to forget to update the docs when the docs are right next to the source, imagine how much easier it is to forget to update external docs! Where, then, would documentation on how the code is organized live then? I would argue it should be pushed as close to the source code as possible. For example, we can use the .py module docstrings to describe the intent behind why certain entire modules exist.","title":"Where your code documentation should go"},{"location":"software-skills/documentation/#an-example","text":"Here is a skeleton to follow: \"\"\" This module houses all functions that cannot be neatly categorized in other places. \"\"\" def my_function ( arg1 , arg2 ): \"\"\" Calculates something based on arg1 and arg2. This calculated thing is intended to be used by `this_other_function`, so the return type should not be changed. :param arg1: Describe arg1 :param arg2: Describe arg2 :returns: ``the_thing_being_returned``, a pandas DataFrame (for example). \"\"\" the_thing_being_returned = ... # implement the function return the_thing_being_returned Now, let's see this in action with a function that returns a snake-cased version of a string with all punctuation also removed. (This is a simplified implementation of what is implemented in pyjanitor 's clean_names function.) import string def clean_string ( s ): \"\"\" Remove all punctuation from string, and convert to lower_snake_case. An example of the input and output: \"My string!\" -> \"my_string\" :param s: String to clean. \"\"\" s = s . replace ( string . punctuation , \"_\" ) . replace ( \" \" , \"_\" ) . strip ( \"_\" ) . lower () return s You may notice that the docstring is longer than the implementation. Frequently (though not always), I have found that when docstring length exceeds implementation length, it is a sign that the author(s) of the code have been thoughtful about its implementation. This bodes well for working in a team, especially when a data scientist hands over a prototype to the engineering team.","title":"An example"},{"location":"software-skills/documentation/#addressing-objections","text":"The main objections to injecting \"basic software engineering\" into a data scientist's workflow usually center around not having enough time. As always, I am sympathetic to this objection, because I also operate under time constraints. One thing I will offer is that docs are an investment of time for the team, rather than for the individual. We save multiples of time downstream when we write good docs. One way to conceptualize this is the number of person-hours saved down the road by oneself and one's teammates when good docs exist. We minimize the amount of time spent reading code to grok what it is about. At the same time, the practice of clarifying what we intend to accomplish with the function can help bring clarity to the implementation. This I have mentioned above. Having a clean implementation makes things easier to maintain later on. Hence, time invested now on good docs also helps us later on. As with other software engineering skills, this is a skill that can be picked up, refined, and honed. We get more efficient at writing docs the more we do it.","title":"Addressing objections"},{"location":"software-skills/documentation/#parting-words","text":"I hope this essay has helped you get a feel for how you can write well-documented code. At the same time, I hope that by showing you a simple anchoring example that you will be able to replicate the pattern in your own work.","title":"Parting words"},{"location":"software-skills/documentation/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"software-skills/environment-variables/","text":"A Data Scientist's Guide to Environment Variables You might have encountered a piece of software asking you for permission to modify your PATH variable, or another program's installation instructions cryptically telling you that you have to \"set your LD_LIBRARY_PATH variable correctly\". As a data scientist, you might encounter other environment variable issues when interacting with your compute stack (particularly if you don't have full control over it, like I do). This post is meant to demystify what an environment variable is, and how it gets used in a data science context. What Is An Environment Variable? First off, let me explain what an environment variable is, by going in-depth into the PATH environment variable. I'd encourage you to execute the commands here inside your bash terminal (with appropriate modifications -- read the text to figure out what I'm doing!). When you log into your computer system, say, your local computer\u2019s terminal or your remote server via SSH, your bash interpreter needs to know where to look for particular programs, such as nano (the text editor), or git (your version control software), or your Python executable. This is controlled by your PATH variable. It specifies the paths to folders where your executable programs are found. By historical convention, command line programs, such as nano , which , and top , are found in the directory /usr/bin . By historical convention, the /bin folder is for software binaries, which is why they are named /bin . These are the ones that are bundled with your operating system, and as such, need special permissions to upgrade. Try it out in your terminal: $ which which /usr/bin/which $ which top /usr/bin/top Other programs are installed (for whatever reason) into /bin instead. ls is one example: $ which ls /bin/ls Yet other programs might be installed in other special directories: $ which nano /usr/local/bin/nano How does your Bash terminal figure out where to go to look for stuff? It uses the PATH environment variable. It looks something like this: $ echo $PATH /usr/bin:/bin:/usr/local/bin The most important thing to remember about the PATH variable is that it is \"colon-delimited\". That is, each directory path is separated by the next using a \"colon\" ( : ) character. The order in which your bash terminal is looking for programs goes from left to right: /usr/bin /bin /usr/local/bin On my particular computer, when I type in ls , my bash interpreter will look inside the /usr/bin directory first. It'll find that ls doesn't exist in /usr/bin , and so it'll move to the next directory, /bin . Since my ls exists under /bin , it'll execute the ls program from there. You can see, then, that this is simultaneously super flexible for customizing your compute environment, yet also potentially super frustrating if a program modified your PATH variable without you knowing. Wait, you can actually modify your PATH variable? Yep, and there's a few ways to do this. How To Modify the PATH variable Using a Bash Session The first way is transient, or temporary, and only occurs for your particular bash session. You can make a folder have higher priority than the existing paths by \"pre-pending\" it to the PATH variable: $ export PATH=/path/to/my/folder:$PATH $ echo $PATH /path/to/my/folder:/usr/bin:/bin:/usr/local/bin Or I can make it have a lower priority than existing paths by \"appending\" it to the PATH variable: $ export PATH=$PATH:/path/to/my/folder $ echo $PATH /usr/bin:/bin:/usr/local/bin:/path/to/my/folder The reason this is temporary is because I only export it during my current bash session. bashrc or .bash_profile File If I wanted to make my changes somewhat more permanent, then I would include inside my .bashrc or .bash_profile file. (I recommend using the .bashrc file.) The .bashrc / .bash_profile file lives inside your home directory (your $HOME environment variable specifies this), and is a file that your bash interpreter will execute first load. It will execute all of the commands inside there. This means, you can change your PATH variable by simply putting inside your .bashrc : ...other stuff above... # Make /path/to/folder have higher priority export PATH=/path/to/folder:$PATH # Make /path/to/other/folder have lower priority export PATH=$PATH:/path/to/folder ...other stuff below... Data Science and the PATH environment variable Now, how is this relevant to data scientists? Well, if you're a data scientist, chances are that you use Python, and that your Python interpreter comes from the Anaconda Python distribution (a seriously awesome thing, go get it!). What the Anaconda Python installer does is prioritize the /path/to/anaconda/bin folder in the PATH environment variable. You might have other Python interpreters installed on your system (that is, Apple ships its own). However, this PATH modification ensures that each time you type python into your Bash terminal, ou execute the Python interpreter shipped with the Anaconda Python distribution. In my case, after installing the Anaconda Python distribution, my PATH looks like: $ echo $PATH /Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin Even better, what conda environments do is prepend the path to the conda environment binaries folder while the environment is activated. For example, with my blog, I keep it in an environment named lektor . Thus... $ echo $PATH /Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin $ which python /Users/ericmjl/anaconda/bin/python $ source activate lektor $ echo $PATH /Users/ericmjl/anaconda/envs/lektor/bin:/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin $ which python /Users/ericmjl/anaconda/envs/lektor/bin/python Notice how the bash terminal now preferentially picks the Python inside the higher-priority lektor environment. If you've gotten to this point, then you'll hopefully realize there's a few important concepts listed here. Let's recap them: PATH is an environment variable stored as a plain text string used by the bash interpreter to figure out where to find executable programs. PATH is colon-delimited; higher priority directories are to the left of the string, while lower priority directories are to the right of the string. PATH can be modified by prepending or appending directories to the environment variable. It can be done transiently inside a bash session by running the export command at the command prompt, or it can be done permanently across bash sessions by adding an export line inside your .bashrc or .bash_profile . Other Environment Variables of Interest Now, what other environment variables might a data scientist encounter? These are a sampling of them that you might see, and might have to fix, especially in contexts where your system administrators are off on vacation (or taking too long to respond). General Use For general use**, you'll definitely want to know where your HOME folder is -- on Linux systems, it's often /home/username , while on macOS systems, it's often /Users/username . You can figure out what HOME is by doing: $ echo $HOME /Users/ericmjl Python If you're a Python user , then the PYTHONPATH is one variable that might be useful. It is used by the Python interpreter, and specifies where to find Python modules/packages. C++ libraries If you have to deal with C++ libraries , then knowing your LD_LIBRARY_PATH environment variable is going to be very important. I'm not well-versed enough in this to espouse on it intelligently, so I would defer to this website for more information on best practices for using the LD_LIBRARY_PATH variable. Spark If you're working with Spark , then the PYSPARK_PYTHON environment variable would be of interest. This essentially tells Spark which Python to use for both its driver and its workers; you can also set the PYSPARK_DRIVER_PYTHON to be separate from the PYSPARK_PYTHON environment variable, if needed. Data science apps If you're developing data science apps , then according to the 12 factor app development principles , your credentials to databases and other sensitive information are securely stored and dynamically loaded into the environment at runtime. How then do you mimic this in a \"local\" environment (i.e. your computer) without hard-coding sensitive information in your source .py files? One way to handle this situation is as follows: Firstly, create a .env file in your home directory. In there, store your credentials: SOME_PASSWORD = \"put_your_pw_here\" SOME_USERNAME = \"put_your_username_here\" Next, add it to your .gitignore , so you never add it to your version control system. # other things .env Finally, in your source .py files, use python-dotenv to load the environment variables at runtime. from dotenv import load_dotenv load_dotenv () import os username = os . getenv ( \"SOME_USERNAME\" ) password = os . getenv ( \"SOME_PASSWORD\" ) Hack Your Environment Variables This is where the most fun happens! Follow along for some stuff you might be able to do by hacking your environment variables. Hack #1: Enable access to PyPy. I occasionally keep up with the development of PyPy, but because PyPy is not yet the default Python interpreter, and is not yet conda install -able, I have to put it in its own $HOME/pypy/bin directory. To enable access to the PyPy interpreter, I have to make sure that my /path/to/pypy is present in the PATH environment variable, but at a lower priority than my regular CPython interpreter. Hack #2: Enable access to other language interpreters/compilers. This is analogous to PyPy. I once was trying out Lua's JIT interpreter to use Torch for deep learning, and needed to add a path to there in my .bashrc . Hack #3: Install Python packages to your home directory. On shared Linux compute systems that use the modules system rather than conda environments, a modulefile that you load might be configured with a virtual environment that you don't have permissions to modify . If you need to install a Python package, you might want to pip install --user my_pkg_name . This will install it to $HOME/.local/lib/python-[version]/site-packages/ . Ensuring that your PYTHONPATH includes $HOME/.local/lib/python-[version]/site-packages at a high enough priority is going to be important in this case. Hack 4: Debugging when things go wrong. In case something throws an error, or you have unexpected behaviour -- something I encountered before was my Python interpreter not being found correctly after loading all of my Linux modules -- then a way to debug is to temporarily set your PATH environment variable to some sensible \"defaults\" and sourcing that, effectively \"resetting\" your PATH variable, so that you can manually prepend/append while debugging. To do this, place the following line inside a file named .path_default , inside your home directory: export PATH=\"\" # resets PATH to an empty string. export PATH=/usr/bin:/bin:/usr/local/bin:$PATH # this is a sensible default; customize as needed. After something goes wrong, you can reset your PATH environment variable by using the \"source\" command: $ echo $PATH /some/complicated/path:/more/complicated/paths:/really/complicated/paths $ source ~/.path_default $ echo $PATH /usr/bin:/bin:/usr/local/bin Note - you can also execute the exact same commands inside your bash session; the interactivity may also be helpful. Conclusion I hope you enjoyed this article, and that it'll give you a, ahem, path forward whenever you encounter these environment variables! Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"A Data Scientist's Guide to Environment Variables"},{"location":"software-skills/environment-variables/#a-data-scientists-guide-to-environment-variables","text":"You might have encountered a piece of software asking you for permission to modify your PATH variable, or another program's installation instructions cryptically telling you that you have to \"set your LD_LIBRARY_PATH variable correctly\". As a data scientist, you might encounter other environment variable issues when interacting with your compute stack (particularly if you don't have full control over it, like I do). This post is meant to demystify what an environment variable is, and how it gets used in a data science context.","title":"A Data Scientist's Guide to Environment Variables"},{"location":"software-skills/environment-variables/#what-is-an-environment-variable","text":"First off, let me explain what an environment variable is, by going in-depth into the PATH environment variable. I'd encourage you to execute the commands here inside your bash terminal (with appropriate modifications -- read the text to figure out what I'm doing!). When you log into your computer system, say, your local computer\u2019s terminal or your remote server via SSH, your bash interpreter needs to know where to look for particular programs, such as nano (the text editor), or git (your version control software), or your Python executable. This is controlled by your PATH variable. It specifies the paths to folders where your executable programs are found. By historical convention, command line programs, such as nano , which , and top , are found in the directory /usr/bin . By historical convention, the /bin folder is for software binaries, which is why they are named /bin . These are the ones that are bundled with your operating system, and as such, need special permissions to upgrade. Try it out in your terminal: $ which which /usr/bin/which $ which top /usr/bin/top Other programs are installed (for whatever reason) into /bin instead. ls is one example: $ which ls /bin/ls Yet other programs might be installed in other special directories: $ which nano /usr/local/bin/nano How does your Bash terminal figure out where to go to look for stuff? It uses the PATH environment variable. It looks something like this: $ echo $PATH /usr/bin:/bin:/usr/local/bin The most important thing to remember about the PATH variable is that it is \"colon-delimited\". That is, each directory path is separated by the next using a \"colon\" ( : ) character. The order in which your bash terminal is looking for programs goes from left to right: /usr/bin /bin /usr/local/bin On my particular computer, when I type in ls , my bash interpreter will look inside the /usr/bin directory first. It'll find that ls doesn't exist in /usr/bin , and so it'll move to the next directory, /bin . Since my ls exists under /bin , it'll execute the ls program from there. You can see, then, that this is simultaneously super flexible for customizing your compute environment, yet also potentially super frustrating if a program modified your PATH variable without you knowing. Wait, you can actually modify your PATH variable? Yep, and there's a few ways to do this.","title":"What Is An Environment Variable?"},{"location":"software-skills/environment-variables/#how-to-modify-the-path-variable","text":"","title":"How To Modify the PATH variable"},{"location":"software-skills/environment-variables/#using-a-bash-session","text":"The first way is transient, or temporary, and only occurs for your particular bash session. You can make a folder have higher priority than the existing paths by \"pre-pending\" it to the PATH variable: $ export PATH=/path/to/my/folder:$PATH $ echo $PATH /path/to/my/folder:/usr/bin:/bin:/usr/local/bin Or I can make it have a lower priority than existing paths by \"appending\" it to the PATH variable: $ export PATH=$PATH:/path/to/my/folder $ echo $PATH /usr/bin:/bin:/usr/local/bin:/path/to/my/folder The reason this is temporary is because I only export it during my current bash session.","title":"Using a Bash Session"},{"location":"software-skills/environment-variables/#bashrc-or-bash_profile-file","text":"If I wanted to make my changes somewhat more permanent, then I would include inside my .bashrc or .bash_profile file. (I recommend using the .bashrc file.) The .bashrc / .bash_profile file lives inside your home directory (your $HOME environment variable specifies this), and is a file that your bash interpreter will execute first load. It will execute all of the commands inside there. This means, you can change your PATH variable by simply putting inside your .bashrc : ...other stuff above... # Make /path/to/folder have higher priority export PATH=/path/to/folder:$PATH # Make /path/to/other/folder have lower priority export PATH=$PATH:/path/to/folder ...other stuff below...","title":"bashrc or .bash_profile File"},{"location":"software-skills/environment-variables/#data-science-and-the-path-environment-variable","text":"Now, how is this relevant to data scientists? Well, if you're a data scientist, chances are that you use Python, and that your Python interpreter comes from the Anaconda Python distribution (a seriously awesome thing, go get it!). What the Anaconda Python installer does is prioritize the /path/to/anaconda/bin folder in the PATH environment variable. You might have other Python interpreters installed on your system (that is, Apple ships its own). However, this PATH modification ensures that each time you type python into your Bash terminal, ou execute the Python interpreter shipped with the Anaconda Python distribution. In my case, after installing the Anaconda Python distribution, my PATH looks like: $ echo $PATH /Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin Even better, what conda environments do is prepend the path to the conda environment binaries folder while the environment is activated. For example, with my blog, I keep it in an environment named lektor . Thus... $ echo $PATH /Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin $ which python /Users/ericmjl/anaconda/bin/python $ source activate lektor $ echo $PATH /Users/ericmjl/anaconda/envs/lektor/bin:/Users/ericmjl/anaconda/bin:/usr/bin:/bin:/usr/local/bin $ which python /Users/ericmjl/anaconda/envs/lektor/bin/python Notice how the bash terminal now preferentially picks the Python inside the higher-priority lektor environment. If you've gotten to this point, then you'll hopefully realize there's a few important concepts listed here. Let's recap them: PATH is an environment variable stored as a plain text string used by the bash interpreter to figure out where to find executable programs. PATH is colon-delimited; higher priority directories are to the left of the string, while lower priority directories are to the right of the string. PATH can be modified by prepending or appending directories to the environment variable. It can be done transiently inside a bash session by running the export command at the command prompt, or it can be done permanently across bash sessions by adding an export line inside your .bashrc or .bash_profile .","title":"Data Science and the PATH environment variable"},{"location":"software-skills/environment-variables/#other-environment-variables-of-interest","text":"Now, what other environment variables might a data scientist encounter? These are a sampling of them that you might see, and might have to fix, especially in contexts where your system administrators are off on vacation (or taking too long to respond).","title":"Other Environment Variables of Interest"},{"location":"software-skills/environment-variables/#general-use","text":"For general use**, you'll definitely want to know where your HOME folder is -- on Linux systems, it's often /home/username , while on macOS systems, it's often /Users/username . You can figure out what HOME is by doing: $ echo $HOME /Users/ericmjl","title":"General Use"},{"location":"software-skills/environment-variables/#python","text":"If you're a Python user , then the PYTHONPATH is one variable that might be useful. It is used by the Python interpreter, and specifies where to find Python modules/packages.","title":"Python"},{"location":"software-skills/environment-variables/#c-libraries","text":"If you have to deal with C++ libraries , then knowing your LD_LIBRARY_PATH environment variable is going to be very important. I'm not well-versed enough in this to espouse on it intelligently, so I would defer to this website for more information on best practices for using the LD_LIBRARY_PATH variable.","title":"C++ libraries"},{"location":"software-skills/environment-variables/#spark","text":"If you're working with Spark , then the PYSPARK_PYTHON environment variable would be of interest. This essentially tells Spark which Python to use for both its driver and its workers; you can also set the PYSPARK_DRIVER_PYTHON to be separate from the PYSPARK_PYTHON environment variable, if needed.","title":"Spark"},{"location":"software-skills/environment-variables/#data-science-apps","text":"If you're developing data science apps , then according to the 12 factor app development principles , your credentials to databases and other sensitive information are securely stored and dynamically loaded into the environment at runtime. How then do you mimic this in a \"local\" environment (i.e. your computer) without hard-coding sensitive information in your source .py files? One way to handle this situation is as follows: Firstly, create a .env file in your home directory. In there, store your credentials: SOME_PASSWORD = \"put_your_pw_here\" SOME_USERNAME = \"put_your_username_here\" Next, add it to your .gitignore , so you never add it to your version control system. # other things .env Finally, in your source .py files, use python-dotenv to load the environment variables at runtime. from dotenv import load_dotenv load_dotenv () import os username = os . getenv ( \"SOME_USERNAME\" ) password = os . getenv ( \"SOME_PASSWORD\" )","title":"Data science apps"},{"location":"software-skills/environment-variables/#hack-your-environment-variables","text":"This is where the most fun happens! Follow along for some stuff you might be able to do by hacking your environment variables.","title":"Hack Your Environment Variables"},{"location":"software-skills/environment-variables/#hack-1-enable-access-to-pypy","text":"I occasionally keep up with the development of PyPy, but because PyPy is not yet the default Python interpreter, and is not yet conda install -able, I have to put it in its own $HOME/pypy/bin directory. To enable access to the PyPy interpreter, I have to make sure that my /path/to/pypy is present in the PATH environment variable, but at a lower priority than my regular CPython interpreter.","title":"Hack #1: Enable access to PyPy."},{"location":"software-skills/environment-variables/#hack-2-enable-access-to-other-language-interpreterscompilers","text":"This is analogous to PyPy. I once was trying out Lua's JIT interpreter to use Torch for deep learning, and needed to add a path to there in my .bashrc .","title":"Hack #2: Enable access to other language interpreters/compilers."},{"location":"software-skills/environment-variables/#hack-3-install-python-packages-to-your-home-directory","text":"On shared Linux compute systems that use the modules system rather than conda environments, a modulefile that you load might be configured with a virtual environment that you don't have permissions to modify . If you need to install a Python package, you might want to pip install --user my_pkg_name . This will install it to $HOME/.local/lib/python-[version]/site-packages/ . Ensuring that your PYTHONPATH includes $HOME/.local/lib/python-[version]/site-packages at a high enough priority is going to be important in this case.","title":"Hack #3: Install Python packages to your home directory."},{"location":"software-skills/environment-variables/#hack-4-debugging-when-things-go-wrong","text":"In case something throws an error, or you have unexpected behaviour -- something I encountered before was my Python interpreter not being found correctly after loading all of my Linux modules -- then a way to debug is to temporarily set your PATH environment variable to some sensible \"defaults\" and sourcing that, effectively \"resetting\" your PATH variable, so that you can manually prepend/append while debugging. To do this, place the following line inside a file named .path_default , inside your home directory: export PATH=\"\" # resets PATH to an empty string. export PATH=/usr/bin:/bin:/usr/local/bin:$PATH # this is a sensible default; customize as needed. After something goes wrong, you can reset your PATH environment variable by using the \"source\" command: $ echo $PATH /some/complicated/path:/more/complicated/paths:/really/complicated/paths $ source ~/.path_default $ echo $PATH /usr/bin:/bin:/usr/local/bin Note - you can also execute the exact same commands inside your bash session; the interactivity may also be helpful.","title":"Hack 4: Debugging when things go wrong."},{"location":"software-skills/environment-variables/#conclusion","text":"I hope you enjoyed this article, and that it'll give you a, ahem, path forward whenever you encounter these environment variables!","title":"Conclusion"},{"location":"software-skills/environment-variables/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"software-skills/refactoring/","text":"Refactoring your code How many times have you found yourself copy/pasting code from one notebook to another? If it the answer is \"many\", then this essay probably has something for you. We're going to look at the practice of \"refactoring\" code, and how it applies in a data science context. Why refactor When writing code, we intend to have a block of code do one thing. As such, its multiple application should have a single source of truth. However, the practice of copying and pasting code gives us multiple sources of truth. Refactoring code, thus, gives us a way of establishing a single source of truth for our functions, which can be called on in multiple situations. When to refactor The short answer is \"basically whenever you find yourself hitting copy+paste\" on your keyboard. How do we refactor The steps involved are as follows. Wrap the semi-complex block of code in a function. Identify what you would consider to be an \"input\" and \"output\" for the function. Take specific variable names and give them more general names. An example Let's take the example of a chunk of code that takes a protein sequence, compares it to a reference sequence, and returns all of the mutations that it has. (We will only implemenet a naive version for the sake of pedagogy.) sequence1 = ... sequence2 = ... mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( sequence1 , sequence2 )): mutations . append ( f \" { letter1 }{ i + 1 }{ letter2 } \" ) mutations = \"; \" . join ( m for m in mutations ) This more or less should accomplish what we want. Let's now apply the ideas behind refactoring to this code block. def mutation_string ( reference , sequence , sep = \"; \" ): mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( reference , sequence )): mutations . append ( f \" { letter1 }{ i + 1 }{ letter2 } \" ) return f \" { sep } \" . join ( m for m in mutations ) You'll notice the three steps coming into play. Firstly , we simply shifted the main logic of the code into a function definition. Secondly , we then generalized the function a bit, by renaming sequence1 and sequence2 to what we usually intend for it to be, a sequence of interest and a reference sequence. Finally , we defined those two as inputs, alongside a keyword argument called sep , which defines the separator between each mutation. Bonus On the basis of this function definition, we can do some additional neat things! For example, in protein sequence analysis, our reference sequence is usually kept constant. Hence, we can actually create a custom mutation_string for our reference sequence using functools.partial by fixing reference to a particular value, thus eliminating the need to repetitively pass in the same reference string. from functools import partial protein1 = ... # define the string here. prot1_mut_string = partial ( mutation_string , reference = protein1 ) protein2 = ... # define the string here. mutstring = prot1_mut_string ( sequence = protein2 ) Where should this function be refactored to You can choose to keep it in the notebook, and that would be fine if the function was used only in a single notebook. If you find yourself needing to call on that same function from another notebook, do the right thing and create a utils.py (or analogous) Python module that lives in the same directory as the notebook. Then, import the refactored function from utils.py . If you feel sophisticated, you can also create a custom Python library for your project. I will address this in a separate essay. An anti-pattern, though, would be to attempt to treat the notebook as source code and import the function from one notebook into another. Notebooks are great for one thing: weaving functions together into an integrarted analysis. I'm of the opinion that we should use a tool the way it was intended, and bring in other tools to do what we need. In this respect, I think that DataBricks notebooks does the wrong thing by bowing to bad human first instincts rather than encouraging productive behaviours. Where do we find time to do this I hear this concern, as I went through the same concerns myself. Isn't it faster to just copy/paste the code? What if I don't end up reusing the code elsewhere? Isn't the time then wasted? In thinking back to my own habits, I realized early on that doing this was not a matter of technical ability but rather a matter of mindset. Investing the time into doing simple refactoring alongside my analyses does take immediate time away from the analysis. However, the deliberate practice of refactoring early on earns back multiples of the time spent as the project progresses. Moreover, if and when the project gets handed over \"in production\", or at least shared with others to use, our colleagues can spend less time is spent navigating a spaghetti-like codebase, and more time can be spent building a proper mental model of the codebase to build on top of. On the possiblity of not reusing the code elsewhere, I would strongly disagree. Refactoring is not a common skill, while copy/pasting code is. Every chance we get to refactor code is practicing the skill, which only gets sharper and more refined as we do it more. Hence, even for the sake of getting more practice makes it worthwhile to do refactoring at every chance. Concluding words I hope this mini-essay demystifies the practice of code refactoring, and gives you some ideas on how to make it part of your workflow. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Refactoring your code"},{"location":"software-skills/refactoring/#refactoring-your-code","text":"How many times have you found yourself copy/pasting code from one notebook to another? If it the answer is \"many\", then this essay probably has something for you. We're going to look at the practice of \"refactoring\" code, and how it applies in a data science context.","title":"Refactoring your code"},{"location":"software-skills/refactoring/#why-refactor","text":"When writing code, we intend to have a block of code do one thing. As such, its multiple application should have a single source of truth. However, the practice of copying and pasting code gives us multiple sources of truth. Refactoring code, thus, gives us a way of establishing a single source of truth for our functions, which can be called on in multiple situations.","title":"Why refactor"},{"location":"software-skills/refactoring/#when-to-refactor","text":"The short answer is \"basically whenever you find yourself hitting copy+paste\" on your keyboard.","title":"When to refactor"},{"location":"software-skills/refactoring/#how-do-we-refactor","text":"The steps involved are as follows. Wrap the semi-complex block of code in a function. Identify what you would consider to be an \"input\" and \"output\" for the function. Take specific variable names and give them more general names.","title":"How do we refactor"},{"location":"software-skills/refactoring/#an-example","text":"Let's take the example of a chunk of code that takes a protein sequence, compares it to a reference sequence, and returns all of the mutations that it has. (We will only implemenet a naive version for the sake of pedagogy.) sequence1 = ... sequence2 = ... mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( sequence1 , sequence2 )): mutations . append ( f \" { letter1 }{ i + 1 }{ letter2 } \" ) mutations = \"; \" . join ( m for m in mutations ) This more or less should accomplish what we want. Let's now apply the ideas behind refactoring to this code block. def mutation_string ( reference , sequence , sep = \"; \" ): mutations = [] for i , ( letter1 , letter2 ) in enumerate ( zip ( reference , sequence )): mutations . append ( f \" { letter1 }{ i + 1 }{ letter2 } \" ) return f \" { sep } \" . join ( m for m in mutations ) You'll notice the three steps coming into play. Firstly , we simply shifted the main logic of the code into a function definition. Secondly , we then generalized the function a bit, by renaming sequence1 and sequence2 to what we usually intend for it to be, a sequence of interest and a reference sequence. Finally , we defined those two as inputs, alongside a keyword argument called sep , which defines the separator between each mutation.","title":"An example"},{"location":"software-skills/refactoring/#bonus","text":"On the basis of this function definition, we can do some additional neat things! For example, in protein sequence analysis, our reference sequence is usually kept constant. Hence, we can actually create a custom mutation_string for our reference sequence using functools.partial by fixing reference to a particular value, thus eliminating the need to repetitively pass in the same reference string. from functools import partial protein1 = ... # define the string here. prot1_mut_string = partial ( mutation_string , reference = protein1 ) protein2 = ... # define the string here. mutstring = prot1_mut_string ( sequence = protein2 )","title":"Bonus"},{"location":"software-skills/refactoring/#where-should-this-function-be-refactored-to","text":"You can choose to keep it in the notebook, and that would be fine if the function was used only in a single notebook. If you find yourself needing to call on that same function from another notebook, do the right thing and create a utils.py (or analogous) Python module that lives in the same directory as the notebook. Then, import the refactored function from utils.py . If you feel sophisticated, you can also create a custom Python library for your project. I will address this in a separate essay. An anti-pattern, though, would be to attempt to treat the notebook as source code and import the function from one notebook into another. Notebooks are great for one thing: weaving functions together into an integrarted analysis. I'm of the opinion that we should use a tool the way it was intended, and bring in other tools to do what we need. In this respect, I think that DataBricks notebooks does the wrong thing by bowing to bad human first instincts rather than encouraging productive behaviours.","title":"Where should this function be refactored to"},{"location":"software-skills/refactoring/#where-do-we-find-time-to-do-this","text":"I hear this concern, as I went through the same concerns myself. Isn't it faster to just copy/paste the code? What if I don't end up reusing the code elsewhere? Isn't the time then wasted? In thinking back to my own habits, I realized early on that doing this was not a matter of technical ability but rather a matter of mindset. Investing the time into doing simple refactoring alongside my analyses does take immediate time away from the analysis. However, the deliberate practice of refactoring early on earns back multiples of the time spent as the project progresses. Moreover, if and when the project gets handed over \"in production\", or at least shared with others to use, our colleagues can spend less time is spent navigating a spaghetti-like codebase, and more time can be spent building a proper mental model of the codebase to build on top of. On the possiblity of not reusing the code elsewhere, I would strongly disagree. Refactoring is not a common skill, while copy/pasting code is. Every chance we get to refactor code is practicing the skill, which only gets sharper and more refined as we do it more. Hence, even for the sake of getting more practice makes it worthwhile to do refactoring at every chance.","title":"Where do we find time to do this"},{"location":"software-skills/refactoring/#concluding-words","text":"I hope this mini-essay demystifies the practice of code refactoring, and gives you some ideas on how to make it part of your workflow.","title":"Concluding words"},{"location":"software-skills/refactoring/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"software-skills/testing/","text":"Testing your code Writing tests for code is a basic software skill. Writing tests helps build confidence in the stability of our code. When to write tests There are two \"time scales\" at which I think this question can be answered. The first time scale is \"short-term\". As soon as we finish up a function, that first test should be written. Doing so lets us immediately sanity-check our intuition about the newly-written fuction. The second time scale is \"longer-term\". As soon as we discover bugs, new tests should be added to the test suite. Those new tests should either cover that exact bug, or cover the class of bugs together. A general rule-of-thumb that has proven reliable is to write an automated test for anything function you come to rely on. How to get setup In a Python project, first ensure that you have pytest installed. If you follow recommended practice and have one conda environment per project, then you should be able to install pytest using conda : # if you use conda: conda install pytest # if you use pip: pip install pytest The anatomy of a test When using pytest , your tests take on the function name: from custom_library import my_function def test_my_function (): \"\"\"Test for my_function.\"\"\" # set up test here. assert some_condition We can then execute the test from the command line: pytest . Voila! The tests will be executed, and you will see them run one by one. The kinds of tests you could write Let's go through the kinds of tests you might want to write. Execution tests I started with this kind of test because these are the simplest to understand: we simply execute a function to make sure that it runs without breaking. from custom_lib import my_function def test_my_function (): \"\"\"Execution test for my_function.\"\"\" my_function () This kind of test is useful when your function is not parameterized, and simply calls on other functions inside your library. It is also incredibly useful as a starter test when you cannot think of a better test to write. One place where I have used this test pattern is when we built a project dashboard using Panel. The dashboard is made from many complex layers of function calls, involving database queries, data preprocessing, cached results, and more. Sporadically, something would break, and it was something difficult to debug. By wrapping the dashboard execution inside a Python function and executing it by simply calling dashboard() , we could discover bugs as soon as they showed up, rather than so-called \"in production\". Example-based test An example-based test looks basically like this: from custom_lib import another_function def test_another_function (): arg1 = ... arg2 = ... result = another_function ( arg1 , arg2 ) expected_result = ... assert result == expected_result Basically, we set up the test with an example, and check that when given a set of pre-specified inputs, a particular expected result is returned. When writing code in the notebook, I find myself writing example-based tests informally all the time. They are those \"sanity-checks\" function calls where I manually check that the result looks correct. I am sure you do too. So rather than rely on manually checking, it makes perfect sense to simply copy and paste the code into a test function and execute them. Advanced Testing The above I consider to be basic, bare minimum testing that a data scientist can do. Of course, there are more complex forms of testing that a QA engineer would engage in, and I find it useful to know at least what they are and what tools we have to do these forms of testing in the Python ecosystem: Parameterized tests: pytest has these capabilities . Property-based tests: hypothesis gives us these capabilities . Tests for Data Data are notoriously difficult to test, because it is a snapshot of the stochastic state of the world. Nonetheless, if we impose prior knowledge on our testing, we can ensure that certain errors in our data never show up. Nullity Tests For example, if we subject a SQL query to a series of transforms that are supposed to guarantee a densely populated DataFrame, then we can write a nullity test . def test_dataframe_function (): \"\"\"Ensures that there are no null values in the dataframe function.\"\"\" df = dataframe_function ( * args , ** kwargs ) assert pd . isnull ( df ) . sum () . sum () == 0 dtype Tests We can also check that the dtypes of the dataframe are correct. def test_dataframe_dtypes (): \"\"\"Checks that the dtypes of the dataframe are correct.\"\"\" dtypes = { \"col1\" : float32 , \"col2\" : int , \"col3\" : object , } df = dataframe_function ( * args , ** kwargs ) for col , dtype in dtypes . items (): assert df [ col ] . dtype == dtype Bounds Tests We can also check to make sure that our dataframe-returning function yields data in the correct bounds for each column. def test_dataframe_bounds (): \"\"\"Checks that the bounds of datsa are correct.\"\"\" df = dataframe_function ( * args , ** kwargs ) # For a column that can be greater than or equal to zero. assert df [ \"column1\" ] . min () >= 0 # For a column that can only be non-zero positive. assert df [ \"column2\" ] . min () > 0 # For a column that can only be non-zero negative. assert df [ \"column3\" ] . max () < 0 DataFrame tests are a special one for data scientists, because the dataframe is the idiomatic data structure that we engage with on an almost daily basis. Column Name Tests Having stable and consistent column names in the dataframes that we use is extremely important; the column names are like our API to the data. Hence, checking that a suite of expected column names exist in the dataframe can be very useful. def test_dataframe_names (): \"\"\"Checks that dataframe column names are correct.\"\"\" expected_column_names = [ \"col1\" , \"col2\" , \"col3\" ] df = dataframe_function ( * args , ** kwargs ) # Check that each of those column names are present for c in expected_column_names : assert c in df . columns # (Optional) check that _only_ those columns are present. assert set ( df . columns ) == set ( expected_column_names ) Other statistical property tests Testing the mean, median, and mode are difficult, but under some circumstances, such as when we know that the data are drawn from some distribution, we might be able to write a test for the central tendencies of the data. Placing an automated test that checks whether the data matches a particular parameterized distribution with some probability value is generally not a good idea, because it can give a false sense of security . However, if this is a key modelling assumption and you need to keep an automated, rolling check on your data, then having it as a test can help you catch failures in downstream modelling early. In practice, I rarely use this because the speed at which data come in are slow relative to the time I need to check assumptions. Additionally, the stochastic nature of data means that this test would be a flaky one, which is an undesirable property for tests. Parting words I hope this essay gives you some ideas for implementing testing in your data science workflow. As with other software skills, these are skills that become muscle memory over time, hence taking the time from our daily hustle to practice them makes us more efficient in the long-run. In particular, the consistent practice of testing builds confidence in our codebase, not just for my future self, but also for other colleagues who might end up using the codebase too. A Glossary of Testing in Data Science Manual testing : Basically where we use a Jupyter notebook and manually inspect that the function works to how we\u2019re expecting. Automated testing : Where we provide a test suite and use a test runner (e.g. pytest ) to automatically execute all of the tests in the suite. Example-based testing : Where we provide one or more hard-coded examples in our test suite, and test that our function works on those examples. Parameterized testing : Where we provide examples as parameters to our test functions, helping us reduce code duplication in our test functions. Not necessarily something distinct from example-based testing. Auto-manual testing : A not-so-tongue-in-cheek way of describing automated testing using hard-coded examples. Property-based testing : Where we use an automatic generator of examples that fulfill certain \u201cproperties\u201d. For example, numbers with range constraints, or strings generated from an alphabet of a certain length or less. Property-based testing builds on top of parameterized testing. Data testing : Where we test the \u201ccorrectness\u201d of our data. Property-based testing can be used here, or we can hard-code checks on our data that we know should be invariant over time. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Testing your code"},{"location":"software-skills/testing/#testing-your-code","text":"Writing tests for code is a basic software skill. Writing tests helps build confidence in the stability of our code.","title":"Testing your code"},{"location":"software-skills/testing/#when-to-write-tests","text":"There are two \"time scales\" at which I think this question can be answered. The first time scale is \"short-term\". As soon as we finish up a function, that first test should be written. Doing so lets us immediately sanity-check our intuition about the newly-written fuction. The second time scale is \"longer-term\". As soon as we discover bugs, new tests should be added to the test suite. Those new tests should either cover that exact bug, or cover the class of bugs together. A general rule-of-thumb that has proven reliable is to write an automated test for anything function you come to rely on.","title":"When to write tests"},{"location":"software-skills/testing/#how-to-get-setup","text":"In a Python project, first ensure that you have pytest installed. If you follow recommended practice and have one conda environment per project, then you should be able to install pytest using conda : # if you use conda: conda install pytest # if you use pip: pip install pytest","title":"How to get setup"},{"location":"software-skills/testing/#the-anatomy-of-a-test","text":"When using pytest , your tests take on the function name: from custom_library import my_function def test_my_function (): \"\"\"Test for my_function.\"\"\" # set up test here. assert some_condition We can then execute the test from the command line: pytest . Voila! The tests will be executed, and you will see them run one by one.","title":"The anatomy of a test"},{"location":"software-skills/testing/#the-kinds-of-tests-you-could-write","text":"Let's go through the kinds of tests you might want to write.","title":"The kinds of tests you could write"},{"location":"software-skills/testing/#execution-tests","text":"I started with this kind of test because these are the simplest to understand: we simply execute a function to make sure that it runs without breaking. from custom_lib import my_function def test_my_function (): \"\"\"Execution test for my_function.\"\"\" my_function () This kind of test is useful when your function is not parameterized, and simply calls on other functions inside your library. It is also incredibly useful as a starter test when you cannot think of a better test to write. One place where I have used this test pattern is when we built a project dashboard using Panel. The dashboard is made from many complex layers of function calls, involving database queries, data preprocessing, cached results, and more. Sporadically, something would break, and it was something difficult to debug. By wrapping the dashboard execution inside a Python function and executing it by simply calling dashboard() , we could discover bugs as soon as they showed up, rather than so-called \"in production\".","title":"Execution tests"},{"location":"software-skills/testing/#example-based-test","text":"An example-based test looks basically like this: from custom_lib import another_function def test_another_function (): arg1 = ... arg2 = ... result = another_function ( arg1 , arg2 ) expected_result = ... assert result == expected_result Basically, we set up the test with an example, and check that when given a set of pre-specified inputs, a particular expected result is returned. When writing code in the notebook, I find myself writing example-based tests informally all the time. They are those \"sanity-checks\" function calls where I manually check that the result looks correct. I am sure you do too. So rather than rely on manually checking, it makes perfect sense to simply copy and paste the code into a test function and execute them.","title":"Example-based test"},{"location":"software-skills/testing/#advanced-testing","text":"The above I consider to be basic, bare minimum testing that a data scientist can do. Of course, there are more complex forms of testing that a QA engineer would engage in, and I find it useful to know at least what they are and what tools we have to do these forms of testing in the Python ecosystem: Parameterized tests: pytest has these capabilities . Property-based tests: hypothesis gives us these capabilities .","title":"Advanced Testing"},{"location":"software-skills/testing/#tests-for-data","text":"Data are notoriously difficult to test, because it is a snapshot of the stochastic state of the world. Nonetheless, if we impose prior knowledge on our testing, we can ensure that certain errors in our data never show up.","title":"Tests for Data"},{"location":"software-skills/testing/#nullity-tests","text":"For example, if we subject a SQL query to a series of transforms that are supposed to guarantee a densely populated DataFrame, then we can write a nullity test . def test_dataframe_function (): \"\"\"Ensures that there are no null values in the dataframe function.\"\"\" df = dataframe_function ( * args , ** kwargs ) assert pd . isnull ( df ) . sum () . sum () == 0","title":"Nullity Tests"},{"location":"software-skills/testing/#dtype-tests","text":"We can also check that the dtypes of the dataframe are correct. def test_dataframe_dtypes (): \"\"\"Checks that the dtypes of the dataframe are correct.\"\"\" dtypes = { \"col1\" : float32 , \"col2\" : int , \"col3\" : object , } df = dataframe_function ( * args , ** kwargs ) for col , dtype in dtypes . items (): assert df [ col ] . dtype == dtype","title":"dtype Tests"},{"location":"software-skills/testing/#bounds-tests","text":"We can also check to make sure that our dataframe-returning function yields data in the correct bounds for each column. def test_dataframe_bounds (): \"\"\"Checks that the bounds of datsa are correct.\"\"\" df = dataframe_function ( * args , ** kwargs ) # For a column that can be greater than or equal to zero. assert df [ \"column1\" ] . min () >= 0 # For a column that can only be non-zero positive. assert df [ \"column2\" ] . min () > 0 # For a column that can only be non-zero negative. assert df [ \"column3\" ] . max () < 0 DataFrame tests are a special one for data scientists, because the dataframe is the idiomatic data structure that we engage with on an almost daily basis.","title":"Bounds Tests"},{"location":"software-skills/testing/#column-name-tests","text":"Having stable and consistent column names in the dataframes that we use is extremely important; the column names are like our API to the data. Hence, checking that a suite of expected column names exist in the dataframe can be very useful. def test_dataframe_names (): \"\"\"Checks that dataframe column names are correct.\"\"\" expected_column_names = [ \"col1\" , \"col2\" , \"col3\" ] df = dataframe_function ( * args , ** kwargs ) # Check that each of those column names are present for c in expected_column_names : assert c in df . columns # (Optional) check that _only_ those columns are present. assert set ( df . columns ) == set ( expected_column_names )","title":"Column Name Tests"},{"location":"software-skills/testing/#other-statistical-property-tests","text":"Testing the mean, median, and mode are difficult, but under some circumstances, such as when we know that the data are drawn from some distribution, we might be able to write a test for the central tendencies of the data. Placing an automated test that checks whether the data matches a particular parameterized distribution with some probability value is generally not a good idea, because it can give a false sense of security . However, if this is a key modelling assumption and you need to keep an automated, rolling check on your data, then having it as a test can help you catch failures in downstream modelling early. In practice, I rarely use this because the speed at which data come in are slow relative to the time I need to check assumptions. Additionally, the stochastic nature of data means that this test would be a flaky one, which is an undesirable property for tests.","title":"Other statistical property tests"},{"location":"software-skills/testing/#parting-words","text":"I hope this essay gives you some ideas for implementing testing in your data science workflow. As with other software skills, these are skills that become muscle memory over time, hence taking the time from our daily hustle to practice them makes us more efficient in the long-run. In particular, the consistent practice of testing builds confidence in our codebase, not just for my future self, but also for other colleagues who might end up using the codebase too.","title":"Parting words"},{"location":"software-skills/testing/#a-glossary-of-testing-in-data-science","text":"Manual testing : Basically where we use a Jupyter notebook and manually inspect that the function works to how we\u2019re expecting. Automated testing : Where we provide a test suite and use a test runner (e.g. pytest ) to automatically execute all of the tests in the suite. Example-based testing : Where we provide one or more hard-coded examples in our test suite, and test that our function works on those examples. Parameterized testing : Where we provide examples as parameters to our test functions, helping us reduce code duplication in our test functions. Not necessarily something distinct from example-based testing. Auto-manual testing : A not-so-tongue-in-cheek way of describing automated testing using hard-coded examples. Property-based testing : Where we use an automatic generator of examples that fulfill certain \u201cproperties\u201d. For example, numbers with range constraints, or strings generated from an alphabet of a certain length or less. Property-based testing builds on top of parameterized testing. Data testing : Where we test the \u201ccorrectness\u201d of our data. Property-based testing can be used here, or we can hard-code checks on our data that we know should be invariant over time.","title":"A Glossary of Testing in Data Science"},{"location":"software-skills/testing/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"terminal/cli-tools/","text":"Tools and Upgrades for your CLI In this short essay, I would like to introduce you to a list of awesome command-line tools that I have found on the internet. Most of the tools listed here do one thing really well: they add visual clarity to the text that we are looking at. This is mostly done by colorizing the terminal with syntax highlighting. Without further ado, let's get started listing them. exa exa is a favourite of mine, because it is an almost drop-in replacement for ls , except with saner defaults. It also comes with a saner set of defaults for the tree command. After installing, you can replace ls and tree with exa by aliasing: alias ls = 'exa --long --git -a --header --group' alias tree = 'exa --tree --level=2 --long -a --header --git' tmux tmux is another daily driver of mine. I use it to keep remote terminal sessions persistent, and use it effectively as a workspace manager between projects. nanorc If you're like me, and are accustomed to the nano text editor rather than vim or emacs , then nanorc , a set of syntax highlighting configurations provided by Anthony Scopatz is an awesome addition to your nano toolkit. (For what it's worth, I wrote this short essay in nano , and nanorc played no small role in making the text readable!) diff-so-fancy diff-so-fancy is a drop-in replacement for diff , and makes it so much easier read diffs between two files. After installation, you can easily replace diff with diff-so-fancy through aliasing: alias diff = \"diff-so-fancy\" bat bat is another one of those instant favourites. I use cat and less often to look through files, but bat takes things to another level. It is basically a mash-up between cat and less , allowing you to scroll through your files in a less -like scrolling fashion, while also providing syntax highlighting for the files you open. At the same time, it'll let you concatenate two files together (just like cat ) and display them to the screen. After installing, you can replace cat with bat by aliasing as well: alias cat = \"bat\" fd fd is another tool that provides saner syntax than the default find . After installing, you can replace find with fd by aliasing: alias find = \"fd\" ripgrep ripgrep is a tool that will let you search directories recursively for a particular pattern. This can help you quickly find text inside a file inside the file tree easily. References Vim From Scratch introduced many of the tools shown here, and I want to make sure that the author gets credit for finding and sharing these awesome tools! James Weis introduced me to tmux while in grad school, and I've been hooked ever since. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Tools and Upgrades for your CLI"},{"location":"terminal/cli-tools/#tools-and-upgrades-for-your-cli","text":"In this short essay, I would like to introduce you to a list of awesome command-line tools that I have found on the internet. Most of the tools listed here do one thing really well: they add visual clarity to the text that we are looking at. This is mostly done by colorizing the terminal with syntax highlighting. Without further ado, let's get started listing them.","title":"Tools and Upgrades for your CLI"},{"location":"terminal/cli-tools/#exa","text":"exa is a favourite of mine, because it is an almost drop-in replacement for ls , except with saner defaults. It also comes with a saner set of defaults for the tree command. After installing, you can replace ls and tree with exa by aliasing: alias ls = 'exa --long --git -a --header --group' alias tree = 'exa --tree --level=2 --long -a --header --git'","title":"exa"},{"location":"terminal/cli-tools/#tmux","text":"tmux is another daily driver of mine. I use it to keep remote terminal sessions persistent, and use it effectively as a workspace manager between projects.","title":"tmux"},{"location":"terminal/cli-tools/#nanorc","text":"If you're like me, and are accustomed to the nano text editor rather than vim or emacs , then nanorc , a set of syntax highlighting configurations provided by Anthony Scopatz is an awesome addition to your nano toolkit. (For what it's worth, I wrote this short essay in nano , and nanorc played no small role in making the text readable!)","title":"nanorc"},{"location":"terminal/cli-tools/#diff-so-fancy","text":"diff-so-fancy is a drop-in replacement for diff , and makes it so much easier read diffs between two files. After installation, you can easily replace diff with diff-so-fancy through aliasing: alias diff = \"diff-so-fancy\"","title":"diff-so-fancy"},{"location":"terminal/cli-tools/#bat","text":"bat is another one of those instant favourites. I use cat and less often to look through files, but bat takes things to another level. It is basically a mash-up between cat and less , allowing you to scroll through your files in a less -like scrolling fashion, while also providing syntax highlighting for the files you open. At the same time, it'll let you concatenate two files together (just like cat ) and display them to the screen. After installing, you can replace cat with bat by aliasing as well: alias cat = \"bat\"","title":"bat"},{"location":"terminal/cli-tools/#fd","text":"fd is another tool that provides saner syntax than the default find . After installing, you can replace find with fd by aliasing: alias find = \"fd\"","title":"fd"},{"location":"terminal/cli-tools/#ripgrep","text":"ripgrep is a tool that will let you search directories recursively for a particular pattern. This can help you quickly find text inside a file inside the file tree easily.","title":"ripgrep"},{"location":"terminal/cli-tools/#references","text":"Vim From Scratch introduced many of the tools shown here, and I want to make sure that the author gets credit for finding and sharing these awesome tools! James Weis introduced me to tmux while in grad school, and I've been hooked ever since.","title":"References"},{"location":"terminal/cli-tools/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"terminal/pre-commits/","text":"Using pre-commit git hooks to automate code checks Git hooks are an awesome way to automate checks on your codebase locally before committing them to your code repository. That said, setting them up involves digging into the .git folder of your repository, and can feel intimidating to set up and replicate across multiple local clones of repositories. Thankfully, there is an easier way about. The developers of the pre-commit framework have given us a wonderful tool to standardize and automate the replication of pre-commit git hooks. What git hooks are Git hooks are basically commands that are run just before or after git commands are executed. In this essay's context, I basically consider it a great way to run automated checks on our code before we commit them. Getting started with pre-commit First off, you should follow the pre-commit instructions for getting setup. These instructions are availble on the pre-commit website. For those of you who know what you are doing and just want something to copy/paste: conda install -c conda-forge pre-commit pre-commit sample-config > .pre-commit-config.yaml pre-commit install pre-commit run --all-files Configuring your pre-commit While the default set is nice, you might want to install other hooks. For example, a Python project might want to default to using black as the code formatter. To enable automatic black formatting and checking before committing code, we need to add black to the configuration file that was produced ( .pre-commit-config.yaml ). - repo : https://github.com/psf/black rev : 19.3b0 hooks : - id : black A classic mistake that I made was to add black directly underneath the default: # THIS IS WRONG!!! - repo : https://github.com/pre-commit/pre-commit-hooks rev : v2.3.0 hooks : - id : check-yaml - id : end-of-file-fixer - id : trailing-whitespace - id : black # THIS IS WRONG!!! You will get an error if you do this. Be forewarned! Updating your pre-commit after updating .pre-commit-config.yaml If you forgot to add a hook but have just edited the YAML file to do so, you will need to run the command to install the hooks. pre-commit install-hooks # Optional pre-commit run --all-files Now, the new hooks will be installed. What happens when you use pre-commit As soon as you write your commit your source files, just before the commit happens, your installed pre-commit hooks execute. If the hooks modify any files, then the commit is halted, and the files that were modified will show up as being \"modified\" or \"untracked\" in your git status. At this point, add the files that were modified by your pre-commit hooks, commit those files, and re-enter your commit message. In this way, you will prevent yourself from committing code that does not pass your code checks. Good pre-commit hooks for Python projects My opinionated list of nice hooks to have can be found below. black pydocstyle isort Benefits of setting up pre-commit (and hooks) By setting up a standard configuration that gets checked into source control, we are setting our team up for success working together. Opinionated checks are now delegated to automated machinery rather than requiring human intervention, hence freeing us up to discuss higher order issues rather than nitpicking on code style. Moreover, by using the pre-commit framework, we take a lot of tedium out in setting up the pre-commit git hooks correctly. I've tried to do that before, and found writing the bash script to be a fragile task to execute. It's fragile because I'm not very proficient in Bash, and I have no other way of testing the git pre-commit hooks apart from actually making a commit. Yet, it seems like we should be able to modularize our hooks, such that they are distributed, installed, and executed in a standard fashion. This is what the pre-commit framework gives us. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Using pre-commit git hooks to automate code checks"},{"location":"terminal/pre-commits/#using-pre-commit-git-hooks-to-automate-code-checks","text":"Git hooks are an awesome way to automate checks on your codebase locally before committing them to your code repository. That said, setting them up involves digging into the .git folder of your repository, and can feel intimidating to set up and replicate across multiple local clones of repositories. Thankfully, there is an easier way about. The developers of the pre-commit framework have given us a wonderful tool to standardize and automate the replication of pre-commit git hooks.","title":"Using pre-commit git hooks to automate code checks"},{"location":"terminal/pre-commits/#what-git-hooks-are","text":"Git hooks are basically commands that are run just before or after git commands are executed. In this essay's context, I basically consider it a great way to run automated checks on our code before we commit them.","title":"What git hooks are"},{"location":"terminal/pre-commits/#getting-started-with-pre-commit","text":"First off, you should follow the pre-commit instructions for getting setup. These instructions are availble on the pre-commit website. For those of you who know what you are doing and just want something to copy/paste: conda install -c conda-forge pre-commit pre-commit sample-config > .pre-commit-config.yaml pre-commit install pre-commit run --all-files","title":"Getting started with pre-commit"},{"location":"terminal/pre-commits/#configuring-your-pre-commit","text":"While the default set is nice, you might want to install other hooks. For example, a Python project might want to default to using black as the code formatter. To enable automatic black formatting and checking before committing code, we need to add black to the configuration file that was produced ( .pre-commit-config.yaml ). - repo : https://github.com/psf/black rev : 19.3b0 hooks : - id : black A classic mistake that I made was to add black directly underneath the default: # THIS IS WRONG!!! - repo : https://github.com/pre-commit/pre-commit-hooks rev : v2.3.0 hooks : - id : check-yaml - id : end-of-file-fixer - id : trailing-whitespace - id : black # THIS IS WRONG!!! You will get an error if you do this. Be forewarned!","title":"Configuring your pre-commit"},{"location":"terminal/pre-commits/#updating-your-pre-commit-after-updating-pre-commit-configyaml","text":"If you forgot to add a hook but have just edited the YAML file to do so, you will need to run the command to install the hooks. pre-commit install-hooks # Optional pre-commit run --all-files Now, the new hooks will be installed.","title":"Updating your pre-commit after updating .pre-commit-config.yaml"},{"location":"terminal/pre-commits/#what-happens-when-you-use-pre-commit","text":"As soon as you write your commit your source files, just before the commit happens, your installed pre-commit hooks execute. If the hooks modify any files, then the commit is halted, and the files that were modified will show up as being \"modified\" or \"untracked\" in your git status. At this point, add the files that were modified by your pre-commit hooks, commit those files, and re-enter your commit message. In this way, you will prevent yourself from committing code that does not pass your code checks.","title":"What happens when you use pre-commit"},{"location":"terminal/pre-commits/#good-pre-commit-hooks-for-python-projects","text":"My opinionated list of nice hooks to have can be found below. black pydocstyle isort","title":"Good pre-commit hooks for Python projects"},{"location":"terminal/pre-commits/#benefits-of-setting-up-pre-commit-and-hooks","text":"By setting up a standard configuration that gets checked into source control, we are setting our team up for success working together. Opinionated checks are now delegated to automated machinery rather than requiring human intervention, hence freeing us up to discuss higher order issues rather than nitpicking on code style. Moreover, by using the pre-commit framework, we take a lot of tedium out in setting up the pre-commit git hooks correctly. I've tried to do that before, and found writing the bash script to be a fragile task to execute. It's fragile because I'm not very proficient in Bash, and I have no other way of testing the git pre-commit hooks apart from actually making a commit. Yet, it seems like we should be able to modularize our hooks, such that they are distributed, installed, and executed in a standard fashion. This is what the pre-commit framework gives us.","title":"Benefits of setting up pre-commit (and hooks)"},{"location":"terminal/pre-commits/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"workflow/code-review/","text":"Practicing Code Review The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well. What code review is Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they understand the code and are able to follow the logic, find potential flaws in the newly contributed code, identify poorly documented code and confusing use of variable names, raise constructive questions and provide constructive feedback on the codebase. If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead. What code review isn't Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa. Why data scientists should do code review Reason 1: Sharing Knowledge The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.) In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model. Reason 2: Catching Mistakes The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified. Reason 3: Social Networking If your team is remote, then code review can be an incredibly powerful way of interacting with one another in a professional and constructive fashion. Because of code review, even in the absence of in-person chats, we still know someone else is looking at the product of our work. The constructive feedback and the mark of approval at the end of the code review session are little plus points that add up to a great working relationship in the long-run, and reduce the sense of loneliness in working remotely. What code review can be Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion. How code review happens Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context. As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction. Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues. Parting words If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Practicing Code Review"},{"location":"workflow/code-review/#practicing-code-review","text":"The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well.","title":"Practicing Code Review"},{"location":"workflow/code-review/#what-code-review-is","text":"Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they understand the code and are able to follow the logic, find potential flaws in the newly contributed code, identify poorly documented code and confusing use of variable names, raise constructive questions and provide constructive feedback on the codebase. If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead.","title":"What code review is"},{"location":"workflow/code-review/#what-code-review-isnt","text":"Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa.","title":"What code review isn't"},{"location":"workflow/code-review/#why-data-scientists-should-do-code-review","text":"","title":"Why data scientists should do code review"},{"location":"workflow/code-review/#reason-1-sharing-knowledge","text":"The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.) In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model.","title":"Reason 1: Sharing Knowledge"},{"location":"workflow/code-review/#reason-2-catching-mistakes","text":"The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified.","title":"Reason 2: Catching Mistakes"},{"location":"workflow/code-review/#reason-3-social-networking","text":"If your team is remote, then code review can be an incredibly powerful way of interacting with one another in a professional and constructive fashion. Because of code review, even in the absence of in-person chats, we still know someone else is looking at the product of our work. The constructive feedback and the mark of approval at the end of the code review session are little plus points that add up to a great working relationship in the long-run, and reduce the sense of loneliness in working remotely.","title":"Reason 3: Social Networking"},{"location":"workflow/code-review/#what-code-review-can-be","text":"Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion.","title":"What code review can be"},{"location":"workflow/code-review/#how-code-review-happens","text":"Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context. As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction. Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues.","title":"How code review happens"},{"location":"workflow/code-review/#parting-words","text":"If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try.","title":"Parting words"},{"location":"workflow/code-review/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"workflow/effective-commit-messages/","text":"Effective Git Commits in Data Science Continuing on the theme of the use of Git in data science, I thought I would write about how to use git commits effectively in our day-to-day data science work. How git commits are intended to be used Git commits are intended to be used as a running log of what gets checked into a code repository. In software engineering, each commit is intended to be a \u201clogical unit of work\u201d. One intent behind defining a commit as a \u201clogical unit of work\u201d is that in case that logical unit of work turned out to be faulty, we can revert that unit of work and only that unit of work without touching other units of work. Git commits can also help us track who made contributions to a repository, as each commit also contains information about the committer (e.g. name and email address). We can view the commit history at the terminal by typing the following incantation: git log --decorate --graph That will give us an interface to the commit log. It will show a running log of the commits to the project, as well as every commit message that was put in. Writing commit messages as if we're going to read them at a later date in reverse sequential order can help us write better commit messages. git commits in analysis-heavy projects In the software world, git commits are a logical way to work. By comparison, in data analysis-heavy work, it is seemingly more difficult to define a \u201clogical unit of work\u201d thank we might in software engineering. After all, what exactly constitutes a \u201clogical unit\u201d of work in data analytics? Is it the answering of a question? That might yield commits/changes that are very large. Is it a software change? That might yield commits/changes that are too small. Admittedly, there is a bit of an art to getting this right. Here, I think treating git commits more as a \"log of work done\" and less of \"report of work done\" might be helpful in adapting git as a lab notebook-style log book. Effective git commits But before we describe how, a few preliminaries are in order. Let\u2019s take a look at what effective and informative commit messages accomplish: Firstly , if we are committing something that is work-in-progress (and yes, this should be permitted, because end-of-day always rolls by), a commit message can mark the fact that there is still work to be done, and describe enough prose to resume context the next day. Secondly , when used in tandem with a timeline, an informative commit message lets us quickly isolate when work was done, thus allowing us to retrace the progression of the project. Finally , good commit messages allow others we collaborate with to get a handle on the work that was already done. Well-written git commit messages can help colleagues that review our work get quickly up-to-speed on what was done, and what to review. In other words, effective commit messages act like documentation for our future selves and for others. Once again, the \u201csocial coding\u201d paradigm comes back. Social coding? Social coding: where we aren\u2019t programming something alone, but rather writing code in collaboration with others\u2019 input. OSS development is a wonderful example of this. git commit messages: examples in data science contexts Let\u2019s see a few examples in action. The Trivial Change Message If we applied trivial changes, such as code formatting, rather than writing a message that read: Don't do this black Perhaps a a more informative message might be: Do this Applied code formatting (make format). We don\u2019t need an extended message (unlike those we might see later), because it is a trivial change. Now, I have been guilty of just writing black as the commit message, but usually that is in the context where I am working on my own project alone. Keeping in mind that commit messages are intended to be read by others, the more informative version is clearer to read and only takes practice to become second nature. The Work-In-Progress (WIP) Message Sometimes, the end of the day rolls by just like that, or we realize we have a mid-afternoon meeting to attend (these are, the wurst sausages!). In those scenarios, putting in a WIP commit may be helpful. So instead of writing a commit message that reads: Don't do this WIP loaded data We instead can write a commit message that reads: Do this WIP finished code that loads data into memory We still need to do the following: - Check statistical covariation between columns and remove correlated features. - Identify the best predictors. Now, when we look at the git log , we will see something that looks like this right at the top of our development branch: * commit abe3d2e8ed55711a57835d96e67207aa2f07f383 (HEAD -> feature-branch) | Author: Me <abc@xyz.com> | Date: Fri Nov 15 14:01:13 2019 -0500 | | WIP finished code that loads data into memory | | We still need to do the following: | | - Check statistical covariation between columns and remove correlated features. | - Identify the best predictors. | * commit ... In this way, the git commit log gives us a way to use it as a \u201clab notebook\u201d-style running log of what\u2019s we have done. The Report on Progress Pedantically, this is distinguished from the WIP message described above by being a \u201cfinal\u201d (but not necessarily binding) message in the work log. An uninformative commit message for this would look like: Don't do this Finally done with model building By contrast, an informative one might look something like this: Do this Model building (Issue #34) ready for review Finished: - Pipeline taking data from input (strings) to activity prediction. - Custom code for data pipeline has been stored in custom package. Tests and docs written. - Notebooks documenting work are also written. Static HTML version for archival also generated. Not done: - Hyperparameter selection. This is the logical next step, and as agreed at last meeting, of highest priority. Admittedly, it can be tough to know when to write this one, and I think it\u2019s because it feels like we might want to be sure that this is absolutely the place that we actually want to write such a message. To this, I would suggest simply commit (pun intended) to writing it when appropriate, and worry about minor things in later commits. Squashed commits If we squash commits in our git workflow (e.g. when merging branches), then writing such detailed commit messages might seem unnecessary. To which my response is, yes indeed! In the case of using squashed commits really only the final commit message ends up being stored in the running log of what gets done. Hence, it makes perfect sense to focus writing good commit messages only at the merge stage, rather than at every single commit. Intentional adoption of better commit messages As I have observed with my own and colleagues\u2019 workflows, we do not regularly write informative commit messages because we don\u2019t read the git log. Then again, we don\u2019t read the git log because it doesn\u2019t contain a lot of information. Hold on, that sounds kind of circular, doesn\u2019t it? I think the chicken-and-egg cycle at some point has to be broken. By starting at some point, we break a vicious cycle of uninformative logging, and allow us to break into a virtuous cycle of good record-keeping. And that really is what this essay is trying to encourage: better record-keeping! Further Reading How to Write a Git Commit Message by Chris Beams . A note to Chris Thank you for writing a wonderful article. I'll be praying for a speedy recovery, Chris. Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Effective Git Commits in Data Science"},{"location":"workflow/effective-commit-messages/#effective-git-commits-in-data-science","text":"Continuing on the theme of the use of Git in data science, I thought I would write about how to use git commits effectively in our day-to-day data science work.","title":"Effective Git Commits in Data Science"},{"location":"workflow/effective-commit-messages/#how-git-commits-are-intended-to-be-used","text":"Git commits are intended to be used as a running log of what gets checked into a code repository. In software engineering, each commit is intended to be a \u201clogical unit of work\u201d. One intent behind defining a commit as a \u201clogical unit of work\u201d is that in case that logical unit of work turned out to be faulty, we can revert that unit of work and only that unit of work without touching other units of work. Git commits can also help us track who made contributions to a repository, as each commit also contains information about the committer (e.g. name and email address). We can view the commit history at the terminal by typing the following incantation: git log --decorate --graph That will give us an interface to the commit log. It will show a running log of the commits to the project, as well as every commit message that was put in. Writing commit messages as if we're going to read them at a later date in reverse sequential order can help us write better commit messages.","title":"How git commits are intended to be used"},{"location":"workflow/effective-commit-messages/#git-commits-in-analysis-heavy-projects","text":"In the software world, git commits are a logical way to work. By comparison, in data analysis-heavy work, it is seemingly more difficult to define a \u201clogical unit of work\u201d thank we might in software engineering. After all, what exactly constitutes a \u201clogical unit\u201d of work in data analytics? Is it the answering of a question? That might yield commits/changes that are very large. Is it a software change? That might yield commits/changes that are too small. Admittedly, there is a bit of an art to getting this right. Here, I think treating git commits more as a \"log of work done\" and less of \"report of work done\" might be helpful in adapting git as a lab notebook-style log book.","title":"git commits in analysis-heavy projects"},{"location":"workflow/effective-commit-messages/#effective-git-commits","text":"But before we describe how, a few preliminaries are in order. Let\u2019s take a look at what effective and informative commit messages accomplish: Firstly , if we are committing something that is work-in-progress (and yes, this should be permitted, because end-of-day always rolls by), a commit message can mark the fact that there is still work to be done, and describe enough prose to resume context the next day. Secondly , when used in tandem with a timeline, an informative commit message lets us quickly isolate when work was done, thus allowing us to retrace the progression of the project. Finally , good commit messages allow others we collaborate with to get a handle on the work that was already done. Well-written git commit messages can help colleagues that review our work get quickly up-to-speed on what was done, and what to review. In other words, effective commit messages act like documentation for our future selves and for others. Once again, the \u201csocial coding\u201d paradigm comes back. Social coding? Social coding: where we aren\u2019t programming something alone, but rather writing code in collaboration with others\u2019 input. OSS development is a wonderful example of this.","title":"Effective git commits"},{"location":"workflow/effective-commit-messages/#git-commit-messages-examples-in-data-science-contexts","text":"Let\u2019s see a few examples in action.","title":"git commit messages: examples in data science contexts"},{"location":"workflow/effective-commit-messages/#the-trivial-change-message","text":"If we applied trivial changes, such as code formatting, rather than writing a message that read: Don't do this black Perhaps a a more informative message might be: Do this Applied code formatting (make format). We don\u2019t need an extended message (unlike those we might see later), because it is a trivial change. Now, I have been guilty of just writing black as the commit message, but usually that is in the context where I am working on my own project alone. Keeping in mind that commit messages are intended to be read by others, the more informative version is clearer to read and only takes practice to become second nature.","title":"The Trivial Change Message"},{"location":"workflow/effective-commit-messages/#the-work-in-progress-wip-message","text":"Sometimes, the end of the day rolls by just like that, or we realize we have a mid-afternoon meeting to attend (these are, the wurst sausages!). In those scenarios, putting in a WIP commit may be helpful. So instead of writing a commit message that reads: Don't do this WIP loaded data We instead can write a commit message that reads: Do this WIP finished code that loads data into memory We still need to do the following: - Check statistical covariation between columns and remove correlated features. - Identify the best predictors. Now, when we look at the git log , we will see something that looks like this right at the top of our development branch: * commit abe3d2e8ed55711a57835d96e67207aa2f07f383 (HEAD -> feature-branch) | Author: Me <abc@xyz.com> | Date: Fri Nov 15 14:01:13 2019 -0500 | | WIP finished code that loads data into memory | | We still need to do the following: | | - Check statistical covariation between columns and remove correlated features. | - Identify the best predictors. | * commit ... In this way, the git commit log gives us a way to use it as a \u201clab notebook\u201d-style running log of what\u2019s we have done.","title":"The Work-In-Progress (WIP) Message"},{"location":"workflow/effective-commit-messages/#the-report-on-progress","text":"Pedantically, this is distinguished from the WIP message described above by being a \u201cfinal\u201d (but not necessarily binding) message in the work log. An uninformative commit message for this would look like: Don't do this Finally done with model building By contrast, an informative one might look something like this: Do this Model building (Issue #34) ready for review Finished: - Pipeline taking data from input (strings) to activity prediction. - Custom code for data pipeline has been stored in custom package. Tests and docs written. - Notebooks documenting work are also written. Static HTML version for archival also generated. Not done: - Hyperparameter selection. This is the logical next step, and as agreed at last meeting, of highest priority. Admittedly, it can be tough to know when to write this one, and I think it\u2019s because it feels like we might want to be sure that this is absolutely the place that we actually want to write such a message. To this, I would suggest simply commit (pun intended) to writing it when appropriate, and worry about minor things in later commits.","title":"The Report on Progress"},{"location":"workflow/effective-commit-messages/#squashed-commits","text":"If we squash commits in our git workflow (e.g. when merging branches), then writing such detailed commit messages might seem unnecessary. To which my response is, yes indeed! In the case of using squashed commits really only the final commit message ends up being stored in the running log of what gets done. Hence, it makes perfect sense to focus writing good commit messages only at the merge stage, rather than at every single commit.","title":"Squashed commits"},{"location":"workflow/effective-commit-messages/#intentional-adoption-of-better-commit-messages","text":"As I have observed with my own and colleagues\u2019 workflows, we do not regularly write informative commit messages because we don\u2019t read the git log. Then again, we don\u2019t read the git log because it doesn\u2019t contain a lot of information. Hold on, that sounds kind of circular, doesn\u2019t it? I think the chicken-and-egg cycle at some point has to be broken. By starting at some point, we break a vicious cycle of uninformative logging, and allow us to break into a virtuous cycle of good record-keeping. And that really is what this essay is trying to encourage: better record-keeping!","title":"Intentional adoption of better commit messages"},{"location":"workflow/effective-commit-messages/#further-reading","text":"How to Write a Git Commit Message by Chris Beams . A note to Chris Thank you for writing a wonderful article. I'll be praying for a speedy recovery, Chris.","title":"Further Reading"},{"location":"workflow/effective-commit-messages/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"},{"location":"workflow/gitflow/","text":"Principled Git-based Workflow in Collaborative Data Science Projects GitFlow is an incredible branching model for working with code. In this essay, I would like to introduce it to you, the data scientist, and show how it might be useful in your context, especially for working with multiple colleagues on the same project. What GitFlow is GitFlow is a way of working with multiple collaborators on a git repository. It originated in the software development world, and gives software developers a way of keeping new development work isolated from reviewed, documented, and stable code. At its core, we have a \"source of truth\" branch called master , from which we make branches on which development work happens. Development work basically means new code, added documentation, more tests, etc. When the new code, documentation, tests, and more are reviewed, a pull request is made to merge the new code back into the master branch. Usually, the act of making a branch is paired with raising an issue on an issue tracker, in which the problem and proposed solution are written down. (In other words, the deliverables are explicitly sketched out.) Merging into master is paired with a code review session, in which another colleague (or the tech lead) reviews the code to be merged, and approves (or denies) code merger based on whether the issue raised in the issue tracker has been resolved. From my time experimenting with GitFlow at work, I think that when paired with other principled workflows that doen't directly interact with Git, can I think be of great utility to data scientists. It does, however, involve a bit of change in the common mode of working that data scientists use. Is GitFlow still confusing for you? If so, please check out this article on GitFlow. It includes the appropriate graphics that will make it much clearer. I felt that a detailed explanation here would be rather out of scope. That said, nothing beats trying it out to get a feel for it, so if you're willing to pick it up, I would encourage you to find a software developer in your organization who has experience with GitFlow and ask them to guide you on it. GitFlow in a data science project Here is how I think GitFlow can be successfully deployed in a data science project. Everything starts with the unit of analysis that we are trying to perform. We start by defining the question that we are trying to answer. We then proceed forward by sketching out an analysis plan (let's call this an analysis sketch ), which outlines the data sources that we need, the strategy for analyzing the data (roughly including: models we think might be relevant to the scale of the problem, the plots we think might be relevant to make, and where we think, future directions might lie). None of this is binding, which makes the analysis sketch less like a formal pre-registered analysis plan, and more like a tool to be more thoughtful of what we want to do when analyzing our data. After all, one of the myths of data science is that we can \"stir the pile until the data start looking right\" . About stirring the pot... If you didn't click the URL to go to XKCD, here's the cartoon embedded below: Once we are done with defining the analysis sketch in an issue, we follow the rest of GitFlow-based workflow: We create a branch off from master , execute on our work, and submit a pull request with everything that we have done. We then invite a colleague to review our work, in which the colleague is explicitly checking that we have delivered on our analysis sketch, or if we have changed course, to discuss the analysis with us in a formal setting. Ideally this is done in-person, but by submitting a formal pull request, our colleague can pull down our code and check that things have been done correctly on their computer. Code review If you want to know more about code review, please check out another essay in this collection. If your team has access to a Binder -like service, then review can be done in an even simpler fashion: simply create a Binder session for the colleague's fork, and explore the analyses there in a temporary session. Once the formal review has finished and both colleagues are on the same page with the analysis, the analysis is merged back into the master branch, and considered done. Both parties can now move onto the next analysis. Mindset changes needed to make GitFlow work In this section, I am going to describe some common mindsets that prevent successful adoption of GitFlow that data scientists might employ, and ways to adapt those mindsets to work with GitFlow. Jumping straight into exploratory data analysis (EDA) This is a common one that even I have done before. The refrain in our mind is, \"Just give me the CSV file! I will figure something out.\" Famous last words, once we come to terms with the horror that we experience in looking through the data. It seems, though, that we shouldn't be able to sketch an analysis plan for EDA, right? I think that mode of thinking might be a tad pessimistic. What we are trying to accomplish with exploratory data analysis is to establish our own working knowledge on: The bounds of the data, The types of the data (ordinal, categorical, numeric), The possible definitions of a single sample in the dataset, Covariation between columns of data, Whether or not the data can answer our questions, and Further questions that come up while looking at the data. Hence, a good analysis sketch to raise for exploratory data analysis would be to write a Jupyter notebook that simply documents all of the above, and then have a colleague review it. Endless modelling experiments This is another one of those trops that I fall into often, so I am sympathetic towards others who might do the same. Scientists (of any type, not just data sciensists) usually come with an obsessive streak, and the way it manifests in data science is usually the quest for the best-performing model. However, in most data science settings, the goal we are trying to accomplish requires first proving out the value of our work using some form of prototype, so we cannot afford to chase performance rabbits down their hole. One way to get around this is to think about the problem in two phases. The first phase is model prototyping . As such, in the analysis sketch, we define a deliverable that is \"a machine learning model that predicts Y from X\", leaving out the performance metric for now . In other words, we are establishing a baseline model, and building out the analysis framework for evaluating how good the model is in the larger applied context. We do this in a quick and dirty fashion, and invite a colleague to review our work to ensure that we have not made any elementary statistical errors, and that the framework is correct with respect to the applied problem that we are tackling. (See note below for more detail.) Note: statistical errors For example, we need to get splitting done correctly in a time series setting, which does not have i.i.d. samples, compared to most other ML problems. And in a cheminformatics setting, random splits tend to over-estimate model performance when compared to a real-world setting where new molecules are often out-of-distribution. If we focused on getting a good model right from the get-go, we may end up missing out on elementary details such as these. Once we are done with this, we embark on the second phase, which is model improvement . Here, we define another analysis sketch where we outline the models that we intend to try, and for which the deliverable is now a Jupyter notebook documenting the modelling experiments we tried. As usual, once we are done, we invite a colleague to review the work to make sure that we have conducted it correctly. A key here is to define the task in as neutral and relevant terms as possible. For example, nobody can guarantee an improved model. However, we can promise a comprehensive, if not exhaustive, search through model and parameter space. We can also guarantee delivering recommendations for improvement regardless of what model performance looks like. Note: Neutral forms of goals As expressed on Twitter before, \"the most scary scientist is one with a hypothesis to prove\". A data scientist who declares that a high-performing model will be the goal is probably being delusional. I wish I knew where exactly I saw the quote, and hence will not take credit for that. Endless ideation prototyping Another trap I have fallen into involves endless ideation prototyping, which is very similar to the \"endless modelling experiments\" problem described above. My proposal here, then, is two-fold. Firstly, rather than running down rabbit holes endlessly, we trust our instincts in evaluating the maturity of an idea. Secondly, we ought also to define \"kill/stop criteria\" ahead-of-time, and move as quickly as possible to kill the idea while also documenting it in a Jupyter notebook. If made part of an analysis sketch that is raised on the issue tracker, then we can be kept accountable by our colleagues. Benefits of adopting GitFlow and associated practices At its core, adopting a workflow as described above is really about intentionally slowing down our work a little bit so that we are more thoughtful about the work we want to finish. In work with my colleagues, I have found this to be incredibly useful. GitFlow and its associated practices bring a suite of benefits to our projects, and I think it is easy to see how. By spending a bit more time on thought and on execution, we cut down on wasted hours exploring unproductive analysis avenues. By pre-defining deliverables expressed in a neutral form, we reduce stress and pressure on data scientists, We also prevent endless rabbit-hole hacking to achieve those non-neutrally-expressed goals. We also receive a less biased analysis, which I believe can only help with making better decisions. Finally, by inviting colleagues to review our work, we also prevent the silo-ing of knowledge on one person, and instead distribute expertise and knowledge. How to gradually adopt GitFlow in your data science teams I know that not every single data science team will have adopted GitFlow from the get-go, and so there will have to be some form of ramp-up to get it going productively. Because this is a collaborative workflow, and because adoption is usually done only in the presence of incentives, I think that in order for GitFlow and associated practices to be adopted, one or more champions for using GitFlow needs to be empowered with the authority to use this workflow on any project they embark on. They also have to be sufficiently unpressured to deliver, so that time and performance pressures do not compromise on adoption. Finally, they have to be able to teach git newcomers and debug problems that show up in git branching, and be able to handle the git workflow for colleagues who might not have the time to pick it up. Tooling also has to be present. A modern version control system and associated hosting software, such as BitBucket, GitHub and GitLab, are necessary. Issue trackers also need to be present for each repository (or project, more generally). At my workplace, I have been fortunate to initiate two projects on which we practice GitFlow, bringing along an intern and a colleague one rank above me who were willing to try this out. This has led to much better sharing of the coding and knowledge load, and has also allowed us to cover for one another much more effectively. While above I may have sounded as if there is resistance to adoption, in practice I know that most data scientists instinctively know that proper workflows are going to be highly beneficial, but lack the time/space and incentives to introduce them in, yet would jump at the chance to do so if properly incentivized and given the time and space to do so. Concluding words I hope that I have convinced you that learning GitFlow, and its associated practices, can be incredibly useful for the long-term health and productivity of your data science team(s). Thank you for reading! If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Principled Git-based Workflow in Collaborative Data Science Projects"},{"location":"workflow/gitflow/#principled-git-based-workflow-in-collaborative-data-science-projects","text":"GitFlow is an incredible branching model for working with code. In this essay, I would like to introduce it to you, the data scientist, and show how it might be useful in your context, especially for working with multiple colleagues on the same project.","title":"Principled Git-based Workflow in Collaborative Data Science Projects"},{"location":"workflow/gitflow/#what-gitflow-is","text":"GitFlow is a way of working with multiple collaborators on a git repository. It originated in the software development world, and gives software developers a way of keeping new development work isolated from reviewed, documented, and stable code. At its core, we have a \"source of truth\" branch called master , from which we make branches on which development work happens. Development work basically means new code, added documentation, more tests, etc. When the new code, documentation, tests, and more are reviewed, a pull request is made to merge the new code back into the master branch. Usually, the act of making a branch is paired with raising an issue on an issue tracker, in which the problem and proposed solution are written down. (In other words, the deliverables are explicitly sketched out.) Merging into master is paired with a code review session, in which another colleague (or the tech lead) reviews the code to be merged, and approves (or denies) code merger based on whether the issue raised in the issue tracker has been resolved. From my time experimenting with GitFlow at work, I think that when paired with other principled workflows that doen't directly interact with Git, can I think be of great utility to data scientists. It does, however, involve a bit of change in the common mode of working that data scientists use. Is GitFlow still confusing for you? If so, please check out this article on GitFlow. It includes the appropriate graphics that will make it much clearer. I felt that a detailed explanation here would be rather out of scope. That said, nothing beats trying it out to get a feel for it, so if you're willing to pick it up, I would encourage you to find a software developer in your organization who has experience with GitFlow and ask them to guide you on it.","title":"What GitFlow is"},{"location":"workflow/gitflow/#gitflow-in-a-data-science-project","text":"Here is how I think GitFlow can be successfully deployed in a data science project. Everything starts with the unit of analysis that we are trying to perform. We start by defining the question that we are trying to answer. We then proceed forward by sketching out an analysis plan (let's call this an analysis sketch ), which outlines the data sources that we need, the strategy for analyzing the data (roughly including: models we think might be relevant to the scale of the problem, the plots we think might be relevant to make, and where we think, future directions might lie). None of this is binding, which makes the analysis sketch less like a formal pre-registered analysis plan, and more like a tool to be more thoughtful of what we want to do when analyzing our data. After all, one of the myths of data science is that we can \"stir the pile until the data start looking right\" . About stirring the pot... If you didn't click the URL to go to XKCD, here's the cartoon embedded below: Once we are done with defining the analysis sketch in an issue, we follow the rest of GitFlow-based workflow: We create a branch off from master , execute on our work, and submit a pull request with everything that we have done. We then invite a colleague to review our work, in which the colleague is explicitly checking that we have delivered on our analysis sketch, or if we have changed course, to discuss the analysis with us in a formal setting. Ideally this is done in-person, but by submitting a formal pull request, our colleague can pull down our code and check that things have been done correctly on their computer. Code review If you want to know more about code review, please check out another essay in this collection. If your team has access to a Binder -like service, then review can be done in an even simpler fashion: simply create a Binder session for the colleague's fork, and explore the analyses there in a temporary session. Once the formal review has finished and both colleagues are on the same page with the analysis, the analysis is merged back into the master branch, and considered done. Both parties can now move onto the next analysis.","title":"GitFlow in a data science project"},{"location":"workflow/gitflow/#mindset-changes-needed-to-make-gitflow-work","text":"In this section, I am going to describe some common mindsets that prevent successful adoption of GitFlow that data scientists might employ, and ways to adapt those mindsets to work with GitFlow.","title":"Mindset changes needed to make GitFlow work"},{"location":"workflow/gitflow/#jumping-straight-into-exploratory-data-analysis-eda","text":"This is a common one that even I have done before. The refrain in our mind is, \"Just give me the CSV file! I will figure something out.\" Famous last words, once we come to terms with the horror that we experience in looking through the data. It seems, though, that we shouldn't be able to sketch an analysis plan for EDA, right? I think that mode of thinking might be a tad pessimistic. What we are trying to accomplish with exploratory data analysis is to establish our own working knowledge on: The bounds of the data, The types of the data (ordinal, categorical, numeric), The possible definitions of a single sample in the dataset, Covariation between columns of data, Whether or not the data can answer our questions, and Further questions that come up while looking at the data. Hence, a good analysis sketch to raise for exploratory data analysis would be to write a Jupyter notebook that simply documents all of the above, and then have a colleague review it.","title":"Jumping straight into exploratory data analysis (EDA)"},{"location":"workflow/gitflow/#endless-modelling-experiments","text":"This is another one of those trops that I fall into often, so I am sympathetic towards others who might do the same. Scientists (of any type, not just data sciensists) usually come with an obsessive streak, and the way it manifests in data science is usually the quest for the best-performing model. However, in most data science settings, the goal we are trying to accomplish requires first proving out the value of our work using some form of prototype, so we cannot afford to chase performance rabbits down their hole. One way to get around this is to think about the problem in two phases. The first phase is model prototyping . As such, in the analysis sketch, we define a deliverable that is \"a machine learning model that predicts Y from X\", leaving out the performance metric for now . In other words, we are establishing a baseline model, and building out the analysis framework for evaluating how good the model is in the larger applied context. We do this in a quick and dirty fashion, and invite a colleague to review our work to ensure that we have not made any elementary statistical errors, and that the framework is correct with respect to the applied problem that we are tackling. (See note below for more detail.) Note: statistical errors For example, we need to get splitting done correctly in a time series setting, which does not have i.i.d. samples, compared to most other ML problems. And in a cheminformatics setting, random splits tend to over-estimate model performance when compared to a real-world setting where new molecules are often out-of-distribution. If we focused on getting a good model right from the get-go, we may end up missing out on elementary details such as these. Once we are done with this, we embark on the second phase, which is model improvement . Here, we define another analysis sketch where we outline the models that we intend to try, and for which the deliverable is now a Jupyter notebook documenting the modelling experiments we tried. As usual, once we are done, we invite a colleague to review the work to make sure that we have conducted it correctly. A key here is to define the task in as neutral and relevant terms as possible. For example, nobody can guarantee an improved model. However, we can promise a comprehensive, if not exhaustive, search through model and parameter space. We can also guarantee delivering recommendations for improvement regardless of what model performance looks like. Note: Neutral forms of goals As expressed on Twitter before, \"the most scary scientist is one with a hypothesis to prove\". A data scientist who declares that a high-performing model will be the goal is probably being delusional. I wish I knew where exactly I saw the quote, and hence will not take credit for that.","title":"Endless modelling experiments"},{"location":"workflow/gitflow/#endless-ideation-prototyping","text":"Another trap I have fallen into involves endless ideation prototyping, which is very similar to the \"endless modelling experiments\" problem described above. My proposal here, then, is two-fold. Firstly, rather than running down rabbit holes endlessly, we trust our instincts in evaluating the maturity of an idea. Secondly, we ought also to define \"kill/stop criteria\" ahead-of-time, and move as quickly as possible to kill the idea while also documenting it in a Jupyter notebook. If made part of an analysis sketch that is raised on the issue tracker, then we can be kept accountable by our colleagues.","title":"Endless ideation prototyping"},{"location":"workflow/gitflow/#benefits-of-adopting-gitflow-and-associated-practices","text":"At its core, adopting a workflow as described above is really about intentionally slowing down our work a little bit so that we are more thoughtful about the work we want to finish. In work with my colleagues, I have found this to be incredibly useful. GitFlow and its associated practices bring a suite of benefits to our projects, and I think it is easy to see how. By spending a bit more time on thought and on execution, we cut down on wasted hours exploring unproductive analysis avenues. By pre-defining deliverables expressed in a neutral form, we reduce stress and pressure on data scientists, We also prevent endless rabbit-hole hacking to achieve those non-neutrally-expressed goals. We also receive a less biased analysis, which I believe can only help with making better decisions. Finally, by inviting colleagues to review our work, we also prevent the silo-ing of knowledge on one person, and instead distribute expertise and knowledge.","title":"Benefits of adopting GitFlow and associated practices"},{"location":"workflow/gitflow/#how-to-gradually-adopt-gitflow-in-your-data-science-teams","text":"I know that not every single data science team will have adopted GitFlow from the get-go, and so there will have to be some form of ramp-up to get it going productively. Because this is a collaborative workflow, and because adoption is usually done only in the presence of incentives, I think that in order for GitFlow and associated practices to be adopted, one or more champions for using GitFlow needs to be empowered with the authority to use this workflow on any project they embark on. They also have to be sufficiently unpressured to deliver, so that time and performance pressures do not compromise on adoption. Finally, they have to be able to teach git newcomers and debug problems that show up in git branching, and be able to handle the git workflow for colleagues who might not have the time to pick it up. Tooling also has to be present. A modern version control system and associated hosting software, such as BitBucket, GitHub and GitLab, are necessary. Issue trackers also need to be present for each repository (or project, more generally). At my workplace, I have been fortunate to initiate two projects on which we practice GitFlow, bringing along an intern and a colleague one rank above me who were willing to try this out. This has led to much better sharing of the coding and knowledge load, and has also allowed us to cover for one another much more effectively. While above I may have sounded as if there is resistance to adoption, in practice I know that most data scientists instinctively know that proper workflows are going to be highly beneficial, but lack the time/space and incentives to introduce them in, yet would jump at the chance to do so if properly incentivized and given the time and space to do so.","title":"How to gradually adopt GitFlow in your data science teams"},{"location":"workflow/gitflow/#concluding-words","text":"I hope that I have convinced you that learning GitFlow, and its associated practices, can be incredibly useful for the long-term health and productivity of your data science team(s).","title":"Concluding words"},{"location":"workflow/gitflow/#thank-you-for-reading","text":"If you enjoyed this essay and would like to receive early-bird access to more, please support me on Patreon ! A coffee a month sent my way gets you early access to my essays on a private URL exclusively for my supporters as well as shoutouts on every single essay that I put out. Also, I have a free monthly newsletter that I use as an outlet to share programming-oriented data science tips and tools. If you'd like to receive it, sign up on TinyLetter !","title":"Thank you for reading!"}]}