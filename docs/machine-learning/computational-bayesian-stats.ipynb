{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# An Introduction to Probability and Computational Bayesian Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In Bayesian statistics, we often say that we are \"sampling\" from a posterior distribution to estimate what parameters could be, given a model structure and data. What exactly is happening here?\n",
    "\n",
    "Examples that I have seen on \"how sampling happens\" tends to focus on an overly-simple example of sampling from a single distribution with known parameters. I was wondering if I could challenge myself to come up with a \"simplest complex example\" that would illuminate ideas that were obscure to me before. In this essay, I would like to share that knowledge with you, and hopefully build up your intuition behind what is happening in computational Bayesian inference."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Probability Distributions\n",
    "\n",
    "We do need to have a working understanding of what a probability distribution is before we can go on. Without going down deep technical and philosophical rabbit holes (I hear they are deep), I'll start by proposing that \"a probability distribution is a Python object that has a math function that allocates credibility points onto the number line\".\n",
    "\n",
    "Because we'll be using the normal distribution extensively in this essay, we'll start off by examining that definition in the context of the standard normal distribution.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Base Object Implementation\n",
    "\n",
    "Since the normal distribution is an object, I'm implying here that it can hold state. What might that state be? Well, we know from math that probability distributions have parameters, and that the normal distribution has the \"mean\" and \"variance\" parameters defined. In Python code, we might write it as:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma"
   ]
  },
  {
   "source": [
    "### Probability Density Function\n",
    "\n",
    "Now, I also stated that the normal distribution has a math function that we can use to allocate credibility points to the number line. This function also has a name, called a \"probability density function\", or the \"PDF\". Using this, we may then extend extend this object with a method called `.pdf(x)`, that returns a number giving the number of credibility points assigned to the value of `x` passed in.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def pdf(self, x):\n",
    "        return (\n",
    "            1 / np.sqrt(2 * self.sigma ** 2 * np.pi)\n",
    "            * np.exp(\n",
    "                - (x - self.mu) ** 2\n",
    "                / 2 * self.sigma ** 2\n",
    "            ))"
   ]
  },
  {
   "source": [
    "If we pass in a number `x` from the number line,\n",
    "we will get back another number that tells us\n",
    "the number of credibility points given to that value `x`,\n",
    "under the state of the normal distribution instantiated.\n",
    "We'll call this $P(x)$.\n",
    "\n",
    "To simplify the implementation used here,\n",
    "we are going to borrow some machinery already available to us\n",
    "in the Python scientific computing ecosystem,\n",
    "particularly from the SciPy stats module,\n",
    "which gives us reference implementations of probability distributions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)"
   ]
  },
  {
   "source": [
    "### Log Probability\n",
    "\n",
    "A common task in Bayesian inference is computing the likelihood of data.\n",
    "Let's assume that the data ${X_1, X_2, ... X_i}$ generated\n",
    "are independent and identically distributed,\n",
    "(the famous _i.i.d._ term comes from this).\n",
    "This means, then, that the joint probability of the data that was generated\n",
    "is equivalent to the product of the individual probabilities of each datum:\n",
    "\n",
    "$$P(X_1, X_2, ... X_i) = P(X_1) P(X_2) ... P(X_i)$$\n",
    "\n",
    "(We have to know the rules of probability to know this result;\n",
    "it is a topic for a different essay.)\n",
    "\n",
    "If you remember the notation above,\n",
    "each $P(X_i)$ is an evaluation of $X_i$\n",
    "on the distribution's probability density function.\n",
    "It being a probability value means it is bound between 0 and 1.\n",
    "However, multiplying many probabilities together\n",
    "usually will result in issues with underflow computationally,\n",
    "so in evaluating likelihoods,\n",
    "we usually stick with log-likelihoods instead.\n",
    "By the usual rules of math, then:\n",
    "\n",
    "$$\\log P(X_1, X_2, ..., X_i) = \\sum_{j=1}^{i}\\log P(X_i)$$\n",
    "\n",
    "To our normal distribution class,\n",
    "we can now add in another class method\n",
    "that computes the sum of log likelihoods\n",
    "evaluated at a bunch of i.i.d. data points.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return self.dist.logpdf(x)"
   ]
  },
  {
   "source": [
    "## Random Variables\n",
    "\n",
    "### Definition\n",
    "\n",
    "Informally, a \"random variable\" is nothing more than\n",
    "a variable whose quantity is non-deterministic (hence random)\n",
    "but whose probability of taking on a certain value\n",
    "can be described by a probability distribution.\n",
    "\n",
    "According to the Wikipedia definition of a [random variable][rv]:\n",
    "\n",
    "> A random variable has a probability distribution, which specifies the probability of its values.\n",
    "\n",
    "[rv]: https://en.wikipedia.org/wiki/Random_variable\n",
    "\n",
    "As such, it may be tempting to conceive of a random variable\n",
    "as an object that has a probability distribution attribute attached to it.\n",
    "\n",
    "### Realizations of a Random Variable\n",
    "\n",
    "On the other hand, it can also be convenient to invert that relationship,\n",
    "and claim that a probability distribution\n",
    "can generate realizations of a random variable.\n",
    "The latter is exactly how SciPy distributions are implemented:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-0.0658959 , -0.64361796,  0.30558603,  1.17337056, -0.01598192,\n",
       "       -0.81016368,  1.97911392, -0.52308103, -0.65893989,  0.35481343])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Normal distribution can generate realizations of an RV\n",
    "# The following returns a NumPy array of 10 draws\n",
    "# from a standard normal distribution.\n",
    "norm(loc=0, scale=1).rvs(10)"
   ]
  },
  {
   "source": [
    "\n",
    "??? note \"Realizations of a Random Variable\"\n",
    "\n",
    "    A \"realization\" of a random variable is nothing more than\n",
    "    generating a random number\n",
    "    whose probability of being generated\n",
    "    is defined by the random variable's probability density function.\n",
    "\n",
    "Because the generation of realizations of a random variable\n",
    "is equivalent to sampling from a probability distribution,\n",
    "we can extend our probability distribution definition\n",
    "to include a `.sample(n)` method:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "class Normal:\n",
    "    def __init__(self, mu, sigma):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = norm(loc=mu, scale=sigma)\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return self.dist.logpdf(x)\n",
    "\n",
    "    def sample(self, n):\n",
    "        return self.dist.rvs(n)"
   ]
  },
  {
   "source": [
    "Now, if we draw 10 realizations of a normally distributed random variable,\n",
    "and the drawing of each realization has no dependence of any kind\n",
    "on the previous draw,\n",
    "then we can claim that each draw is **independent**\n",
    "and **identically distributed**.\n",
    "This is where the fabled \"_iid_\" term in undergraduate statistics classes\n",
    "comes from.\n",
    "\n",
    "## Data Generating Process\n",
    "\n",
    "Now that we have covered what probability distributions are,\n",
    "we can now move on to other concepts\n",
    "that are important in Bayesian statistical modelling.\n",
    "\n",
    "Realizations of a random variable,\n",
    "or draws from its probability distribution,\n",
    "are how a Bayesian assumes data are generated.\n",
    "Describing how data are generated using probability distributions,\n",
    "or in other words, writing down the \"data generating process\",\n",
    "is a core activity in Bayesian statistical modelling.\n",
    "\n",
    "Viewed this way, data values generated by a random process\n",
    "depend on the underlying random variable's probability distribution.\n",
    "In other words, the random variable realizations are known,\n",
    "given the probability distribution used to model it.\n",
    "Keep this idea in mind:\n",
    "it is going to be important shortly.\n",
    "\n",
    "## Bayes' Rule\n",
    "\n",
    "Now that we've covered probability distributions,\n",
    "we can move on to Bayes' rule.\n",
    "You probably have seen the following equation:\n",
    "\n",
    "$$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$$\n",
    "\n",
    "Bayes' rule states nothing more than the fact that\n",
    "the conditional probability of B given A is equal to\n",
    "the conditional probability of A given B\n",
    "times the probability of B\n",
    "divided by the probability of A.\n",
    "\n",
    "When doing Bayesian statistical inference,\n",
    "we commonly take a related but distinct interpretation:\n",
    "\n",
    "$$P(H|D) = \\frac{P(D|H)P(H)}{P(D)}$$\n",
    "\n",
    "It may look weird,\n",
    "but didn't we say before that data are realizations from a random variable?\n",
    "Why are we now treating data as a random variable?\n",
    "Here, we are doing not-so-intuitive but technically correct step\n",
    "of treating the data $D$ as being part of this probabilistic model\n",
    "(hence it \"looks\" like a random variable),\n",
    "alongside our model parameters $H$.\n",
    "There's a lot of measure theory that goes into this interpretation,\n",
    "which at this point I have not yet mastered,\n",
    "and so will wave my hands in great arcs\n",
    "and propose that this interpretation be accepted for now and move on.\n",
    "\n",
    "??? note \"Data are random variables?\"\n",
    "\n",
    "    Notes from a chat with Colin gave me a lot to chew on, as usual:\n",
    "\n",
    "    > The answer is in how you define \"event\" as\n",
    "    > \"an element of a sigma algebra\".\n",
    "    > intuitively, an \"event\" is just an abstraction,\n",
    "    > so one event might be \"the coin is heads\",\n",
    "    > or in another context the event might be\n",
    "    > \"the parameters are [0.2, 0.1, 0.2]\".\n",
    "    > And so analogously, \"the data were configured as [0, 5, 2, 3]\".\n",
    "    > Notice also that the events are different\n",
    "    > if the data being ordered vs unordered are different!\n",
    "\n",
    "    This was a logical leap that I had been asked about before,\n",
    "    but did not previously have the knowledge to respond to.\n",
    "    Thanks to Colin, I now do.\n",
    "\n",
    "\n",
    "[colin]: https://colindcarroll.com/\n",
    "\n",
    "With the data + hypothesis interpretation of Bayes' rule in hand,\n",
    "the next question arises:\n",
    "What math happens when we calculate posterior densities?\n",
    "\n",
    "## Translating Bayes' Math to Python\n",
    "\n",
    "### Defining Posterior Log-Likelihood\n",
    "\n",
    "To understand this, let's look at the simplest complex example\n",
    "that I could think of:\n",
    "Estimating the $\\mu$ and $\\sigma$ parameters\n",
    "of a normal distribution\n",
    "conditioned on observing data points $y$.\n",
    "\n",
    "If we assume a data generating process that looks like the following\n",
    "(with no probability distributions specified yet):\n",
    "\n",
    "```mermaid\n",
    "graph TD;\n",
    "    μ((μ)) --> y(y);\n",
    "    σ((σ)) --> y(y);\n",
    "```\n",
    "\n",
    "We can write out the following probabilistic model\n",
    "(now explicitly specifying probability distributions):\n",
    "\n",
    "$$\\mu \\sim Normal(0, 10)$$\n",
    "\n",
    "$$\\sigma \\sim Exponential(1)$$\n",
    "\n",
    "$$y \\sim Normal(\\mu, \\sigma)$$\n",
    "\n",
    "Let's now map the symbols onto Bayes' rule.\n",
    "\n",
    "- $H$ are the parameters, which are $\\mu$ and $\\sigma$ here.\n",
    "- $D$ is the data that I will observe\n",
    "- $P(H|D)$ is the posterior, which we would like to compute.\n",
    "- $P(D|H)$ is the likelihood,\n",
    "and is given by $y$'s probability distribution $Normal(\\mu, \\sigma)$,\n",
    "or in probability notation, $P(y|\\mu, \\sigma)$.\n",
    "- $P(H)$ is the the prior, and is given by $P(\\mu, \\sigma)$.\n",
    "- $P(D)$ is a hard quantity to calculate, so we sort of cheat and don't use it,\n",
    "and merely claim that the posterior is proportional to likelihood times prior.\n",
    "\n",
    "If we look at the probability symbols again,\n",
    "we should notice that $P(\\mu, \\sigma)$\n",
    "is the joint distribution between $\\mu$ and $\\sigma$.\n",
    "However, from observing the graphical diagram,\n",
    "we'll notice that $\\mu$ and $\\sigma$ have no bearing on one another:\n",
    "we do not need to know $\\mu$ to know the value of $\\sigma$,\n",
    "and vice versa.\n",
    "Hence, they are independent of one another,\n",
    "and so by the rules of probability,\n",
    "\n",
    "$$P(\\mu, \\sigma) = P(\\mu | \\sigma)P(\\sigma) = P(\\mu)P(\\sigma) = P(H)$$\n",
    "\n",
    "Now, by simply moving symbols around:\n",
    "\n",
    "$$P(H|D) = P(D|H)P(H)$$\n",
    "\n",
    "$$ = P(y|\\mu,\\sigma)P(\\mu, \\sigma)$$\n",
    "\n",
    "$$ = P(y|\\mu, \\sigma)P(\\mu)P(\\sigma)$$\n",
    "\n",
    "This translates directly into Python code!\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "class Exponential:\n",
    "    def __init__(self, x):\n",
    "        self.x = x\n",
    "\n",
    "        # We instantiate the distribution object here.\n",
    "        self.dist = expon()\n",
    "\n",
    "    def pdf(self, x):\n",
    "        # Now, our PDF class method is simplified to be just a wrapper.\n",
    "        return self.dist.pdf(x)\n",
    "\n",
    "    def logpdf(self, x):\n",
    "        return self.dist.logpdf(x)\n",
    "\n",
    "    def sample(self, n):\n",
    "        return self.dist.rvs(n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prob(mu, sigma, y):\n",
    "    # Probability of mu under prior.\n",
    "    normal_prior = Normal(0, 10)\n",
    "    mu_prob = normal_prior.pdf(mu)\n",
    "\n",
    "    # Probability of sigma under prior.\n",
    "    sigma_prior = Exponential(1)\n",
    "    sigma_prob = sigma_prior.pdf(sigma)\n",
    "\n",
    "    # Likelihood of data given mu and sigma\n",
    "    likelihood = Normal(mu, sigma)\n",
    "    likelihood_prob = likelihood.pdf(y).prod()\n",
    "\n",
    "    # Joint likelihood\n",
    "    return mu_prob * sigma_prob * likelihood_prob"
   ]
  },
  {
   "source": [
    "If you remember, multiplying so many probability distributions together\n",
    "can give us underflow issues when computing,\n",
    "so it is common to take the log of both sides.\n",
    "\n",
    "$$\\log(P(H|D)) = log(P(y|\\mu, \\sigma)) + log(P(\\mu)) + log(P(\\sigma))$$\n",
    "\n",
    "This also translates directly into Python code!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_log_prob(mu, sigma, y):\n",
    "    # log-probability of mu under prior.\n",
    "    normal_prior = Normal(0, 10)\n",
    "    mu_log_prob = normal_prior.logpdf(mu)\n",
    "\n",
    "    # log-probability of sigma under prior.\n",
    "    sigma_prior = Exponential(1)\n",
    "    sigma_log_prob = sigma_prior.logpdf(sigma)\n",
    "\n",
    "    # log-likelihood given priors and data\n",
    "    likelihood = Normal(mu, sigma)\n",
    "    likelihood_log_prob = likelihood.logpdf(y).sum()\n",
    "\n",
    "    # Joint log-likelihood\n",
    "    return mu_log_prob + sigma_log_prob + likelihood_log_prob"
   ]
  },
  {
   "source": [
    "## Computing the Posterior with Sampling\n",
    "\n",
    "To identify what the values of $\\mu$ and $\\sigma$\n",
    "should take on given the data and priors,\n",
    "we can turn to sampling to help us.\n",
    "I am intentionally skipping over integrals\n",
    "which are used to compute expectations,\n",
    "which is what sampling is replacing.\n",
    "\n",
    "### Metropolis-Hastings Sampling\n",
    "\n",
    "An easy-to-understand sampler that we can start with\n",
    "is the Metropolis-Hastings sampler.\n",
    "I first learned it in a grad-level computational biology class,\n",
    "but I expect most statistics undergrads should have\n",
    "a good working knowledge of the algorithm.\n",
    "\n",
    "For the rest of us, check out the note below on how the algorithm works.\n",
    "\n",
    "???+ note \"The Metropolis-Hastings Algorithm\"\n",
    "\n",
    "    Shamelessly copied (and modified)\n",
    "    from the [Wikipedia article]():\n",
    "\n",
    "    - For each parameter $p$, do the following.\n",
    "    - Initialize an arbitrary point for the parameter (this is $p_t$, or $p$ at step $t$).\n",
    "    - Define a probability density $P(p_t)$, for which we will draw new values of the parameters. Here, we will use $P(p) = Normal(p_{t-1}, 1)$.\n",
    "    - For each iteration:\n",
    "        - Generate candidate new candidate $p_t$ drawn from $P(p_t)$.\n",
    "        - Calculate the likelihood of the data under the previous parameter value(s) $p_{t-1}$: $L(p_{t-1})$\n",
    "        - Calculate the likelihood of the data under the proposed parameter value(s) $p_t$: $L(p_t)$\n",
    "        - Calculate acceptance ratio $r = \\frac{L(p_t)}{L(p_{t-1})}$.\n",
    "        - Generate a new random number on the unit interval: $s \\sim U(0, 1)$.\n",
    "        - Compare $s$ to $r$.\n",
    "            - If $s \\leq r$, accept $p_t$.\n",
    "            - If $s \\gt r$, reject $p_t$ and continue sampling again with $p_{t-1}$.\n",
    "\n",
    "[mh]: https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n",
    "\n",
    "In the algorithm described in the note above,\n",
    "our parameters $p$ are actually $(\\mu, \\sigma)$.\n",
    "This means that we have to propose two numbers\n",
    "and sample two numbers in each loop of the sampler.\n",
    "\n",
    "To make things simple for us, let's use the normal distribution\n",
    "centered on $0$ but with scale $0.1$\n",
    "to propose values for each.\n",
    "\n",
    "We can implement the algorithm in Python code:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = Normal(0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Normal' and 'float'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-6636e5545ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Compute joint log likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mLL_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mLL_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4344bac71c08>\u001b[0m in \u001b[0;36mmodel_log_prob\u001b[0;34m(mu, sigma, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# log-likelihood given priors and data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlikelihood_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Joint log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-25b8b6b966af>\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mcond0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mcond1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Normal' and 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal()\n",
    "sigma_prev = np.random.normal()\n",
    "\n",
    "# Keep a history of the parameter values and ratio.\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    sigma_t = np.random.normal(sigma_prev, 0.1)\n",
    "\n",
    "    # Compute joint log likelihood\n",
    "    LL_t = model_log_prob(mu_t, sigma_t, y)\n",
    "    LL_prev = model_log_prob(mu_prev, sigma_prev, y)\n",
    "\n",
    "    # Calculate the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        # We need to exponentiate to get the correct ratio,\n",
    "        # since all of our calculations were in log-space\n",
    "        ratio = np.exp(diff_log_like)\n",
    "\n",
    "    # Defensive programming check\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "\n",
    "    # Ratio comparison step\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "\n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "source": [
    "\n",
    "Because of a desire for convenience,\n",
    "we chose to use a single normal distribution to sample all values.\n",
    "However, that distribution choice is going to bite us during sampling,\n",
    "because the values that we could possibly sample for the $\\sigma$ parameter\n",
    "can take on negatives,\n",
    "but when a negative $\\sigma$ is passed\n",
    "into the normally-distributed likelihood,\n",
    "we are going to get computation errors!\n",
    "This is because the scale parameter of a normal distribution\n",
    "can only be positive, and cannot be negative or zero.\n",
    "(If it were zero, there would be no randomness.)\n",
    "\n",
    "### Transformations as a Hack\n",
    "\n",
    "The key problem here is that the support of the Exponential distribution\n",
    "is bound to be positive real numbers only.\n",
    "That said, we can get around this problem\n",
    "simply by sampling amongst the unbounded real number space $(-\\inf, +\\inf)$,\n",
    "and then transforming the number by a math function to be in the bounded space.\n",
    "\n",
    "One way we can transform numbers from an unbounded space\n",
    "to a positive-bounded space\n",
    "is to use the exponential transform:\n",
    "\n",
    "$$y = e^x$$\n",
    "\n",
    "For any given value $x$, $y$ will be guaranteed to be positive.\n",
    "\n",
    "Knowing this, we can modify our sampling code, specifically, what was before:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Normal' and 'float'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-381a17ae273d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Pass the transformed values into the log-likelihood calculation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mLL_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mLL_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4344bac71c08>\u001b[0m in \u001b[0;36mmodel_log_prob\u001b[0;34m(mu, sigma, y)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# log-likelihood given priors and data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlikelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mlikelihood_log_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Joint log-likelihood\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-25b8b6b966af>\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36mlogpdf\u001b[0;34m(self, x, *args, **kwds)\u001b[0m\n\u001b[1;32m   1780\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mdtyp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_common_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtyp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1783\u001b[0m         \u001b[0mcond0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_argcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m         \u001b[0mcond1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Normal' and 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Metropolis-Hastings Sampling\n",
    "mu_prev = np.random.normal()\n",
    "sigma_prev = np.random.normal()\n",
    "\n",
    "# Keep a history of the parameter values and ratio.\n",
    "mu_history = dict()\n",
    "sigma_history = dict()\n",
    "ratio_history = dict()\n",
    "\n",
    "# Initialize in unconstrained space\n",
    "sigma_prev_unbounded = np.random.normal(0, 1)\n",
    "\n",
    "for i in range(1000):\n",
    "    mu_history[i] = mu_prev\n",
    "    sigma_history[i] = sigma_prev\n",
    "    mu_t = np.random.normal(mu_prev, 0.1)\n",
    "    # sigma_t = np.random.normal(sigma_prev, 0.1)\n",
    "\n",
    "    # Propose in unconstrained space\n",
    "    sigma_t_unbounded = np.random.normal(sigma_prev_unbounded, 0.1)\n",
    "\n",
    "    # Transform the sampled values to the constrained space\n",
    "    sigma_prev = np.exp(sigma_prev_unbounded)\n",
    "    sigma_t = np.exp(sigma_t_unbounded)\n",
    "\n",
    "    # Pass the transformed values into the log-likelihood calculation\n",
    "    LL_t = model_log_prob(mu_t, sigma_t, y)\n",
    "    LL_prev = model_log_prob(mu_prev, sigma_prev, y)\n",
    "\n",
    "    # Calculate the difference in log-likelihoods\n",
    "    # (or a.k.a. ratio of likelihoods)\n",
    "    diff_log_like = LL_t - LL_prev\n",
    "    if diff_log_like > 0:\n",
    "        ratio = 1\n",
    "    else:\n",
    "        # We need to exponentiate to get the correct ratio,\n",
    "        # since all of our calculations were in log-space\n",
    "        ratio = np.exp(diff_log_like)\n",
    "\n",
    "    # Defensive programming check\n",
    "    if np.isinf(ratio) or np.isnan(ratio):\n",
    "        raise ValueError(f\"LL_t: {LL_t}, LL_prev: {LL_prev}\")\n",
    "\n",
    "    # Ratio comparison step\n",
    "    ratio_history[i] = ratio\n",
    "    p = np.random.uniform(0, 1)\n",
    "\n",
    "    if ratio >= p:\n",
    "        mu_prev = mu_t\n",
    "        sigma_prev = sigma_t"
   ]
  },
  {
   "source": [
    "\n",
    "And _voila_!\n",
    "If you notice, the key trick here was\n",
    "to **sample in unbounded space**,\n",
    "but **evalute log-likelihood in bounded space**.\n",
    "We call the \"unbounded\" space the _transformed_ space,\n",
    "while the \"bounded\" space is the _original_ or _untransformed_ space.\n",
    "We have implemented the necessary components\n",
    "to compute posterior distributions on parameters!\n",
    "\n",
    "### Samples from Posterior\n",
    "\n",
    "If we simulate 1000 data points from a $Normal(3, 1)$ distribution,\n",
    "and pass them into the model log probability function defined above,\n",
    "then after running the sampler,\n",
    "we get a chain of values that the sampler has picked out\n",
    "as maximizing the joint likelihood of the data and the model.\n",
    "This, by the way, is essentially the simplest version of\n",
    "Markov Chain Monte Carlo sampling that exists\n",
    "in modern computational Bayesian statistics.\n",
    "\n",
    "Let's examine the trace from one run:\n",
    "\n",
    "![](./comp-bayes-figures/mcmc-trace.png)\n",
    "\n",
    "Notice how it takes about 200 steps before the trace becomes **stationary**,\n",
    "that is it becomes a flat trend-line.\n",
    "If we prune the trace to just the values after the 200th iteration,\n",
    "we get the following trace:\n",
    "\n",
    "![](./comp-bayes-figures/mcmc-trace-burn-in.png)\n",
    "\n",
    "The samples drawn are an approximation to\n",
    "the expected values of $\\mu$ and $\\sigma$\n",
    "given the data and priors specified.\n",
    "\n",
    "???+ note \"Random Variables and Sampling\"\n",
    "\n",
    "    A piece of wisdom directly quoted from my friend [Colin Carroll][colin],\n",
    "    who is also a PyMC developer:\n",
    "\n",
    "    > Random variables are *measures*,\n",
    "    > and measures are only really defined under an integral sign.\n",
    "    > *Sampling* is usually defined as the act of generating data\n",
    "    > according to a certain measure.\n",
    "    > This is confusing, because we invert this relationship\n",
    "    > when we do computational statistics:\n",
    "    > we generate the data,\n",
    "    > and use that to approximate an integral or expectation.\n",
    "\n",
    "## Topics We Skipped Over\n",
    "\n",
    "We intentionally skipped over a number of topics.\n",
    "\n",
    "One of them was why we used a normal distribution with scale of 0.1\n",
    "to propose a different value, rather than a different scale.\n",
    "As it turns out the, scale parameter is a tunable hyperparameter,\n",
    "and in PyMC3 we do perform tuning as well.\n",
    "If you want to learn more about how tuning happens,\n",
    "[Colin][colin] has a [great essay][tuning] on that too.\n",
    "\n",
    "[tuning]: https://colcarroll.github.io/hmc_tuning_talk/\n",
    "\n",
    "We also skipped over API design,\n",
    "as that is a topic I will be exploring in a separate essay.\n",
    "It will also serve as a tour through the PyMC3 API\n",
    "as I understand it.\n",
    "\n",
    "## An Anchoring Thought Framework for Learning Computational Bayes\n",
    "\n",
    "Having gone through this exercise\n",
    "has been extremely helpful in deciphering\n",
    "what goes on behind-the-scenes in PyMC3\n",
    "(and the in-development PyMC4,\n",
    "which is built on top of TensorFlow probability).\n",
    "\n",
    "From digging through everything from scratch,\n",
    "my thought framework to think about Bayesian modelling\n",
    "has been updated (pun intended) to the following.\n",
    "\n",
    "Firstly, we can view a Bayesian model\n",
    "from the axis of **prior, likelihood, posterior**.\n",
    "Bayes' rule provides us the equation \"glue\"\n",
    "that links those three components together.\n",
    "\n",
    "Secondly, when doing _computational_ Bayesian statistics,\n",
    "we should be able to modularly separate **sampling**\n",
    "from **model definition**.\n",
    "**Sampling** is computing the posterior distribution of parameters\n",
    "given the model and data.\n",
    "**Model definition**, by contrast,\n",
    "is all about providing the model structure\n",
    "as well as a function that calculates the joint log likelihood\n",
    "of the model and data.\n",
    "\n",
    "In fact, based on the exercise above,\n",
    "any \"sampler\" is only concerned with the model log probability\n",
    "(though some also require the local gradient of the log probability\n",
    "w.r.t. the parameters to find where to climb next),\n",
    "and should only be required to accept a **model log probability** function\n",
    "and a proposed set of initial parameter values,\n",
    "and return a chain of sampled values.\n",
    "\n",
    "Finally, I hope the \"simplest complex example\"\n",
    "of estimating $\\mu$ and $\\sigma$ of a normal distribution\n",
    "helps further your understanding of the math behind Bayesian statistics.\n",
    "\n",
    "All in all, I hope this essay helps your learning, as writing it did for me!\n",
    "\n",
    "## Thank you for reading!\n",
    "\n",
    "If you enjoyed this essay and would like to receive early-bird access to more,\n",
    "[please support me on Patreon][patreon]!\n",
    "A coffee a month sent my way gets you _early_ access to my essays\n",
    "on a private URL exclusively for my supporters\n",
    "as well as shoutouts on every single essay that I put out.\n",
    "\n",
    "[patreon]: https://patreon.com/ericmjl\n",
    "\n",
    "Also, I have a free monthly newsletter that I use as an outlet\n",
    "to share programming-oriented data science tips and tools.\n",
    "If you'd like to receive it, sign up on [TinyLetter][tinyletter]!\n",
    "\n",
    "[tinyletter]: https://tinyletter.com/ericmjl\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}